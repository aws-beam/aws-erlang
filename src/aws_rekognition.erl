%% WARNING: DO NOT EDIT, AUTO-GENERATED CODE!
%% See https://github.com/aws-beam/aws-codegen for more details.

%% @doc This is the API Reference for Amazon Rekognition Image:
%% https://docs.aws.amazon.com/rekognition/latest/dg/images.html, Amazon
%% Rekognition Custom Labels:
%% https://docs.aws.amazon.com/rekognition/latest/customlabels-dg/what-is.html,
%% Amazon Rekognition Stored
%% Video: https://docs.aws.amazon.com/rekognition/latest/dg/video.html,
%% Amazon Rekognition Streaming Video:
%% https://docs.aws.amazon.com/rekognition/latest/dg/streaming-video.html.
%%
%% It provides descriptions of actions, data types, common
%% parameters, and common errors.
%%
%% Amazon Rekognition Image
%%
%% AssociateFaces:
%% https://docs.aws.amazon.com/rekognition/latest/APIReference/API_AssociateFaces.html
%%
%% CompareFaces:
%% https://docs.aws.amazon.com/rekognition/latest/APIReference/API_CompareFaces.html
%%
%% CreateCollection:
%% https://docs.aws.amazon.com/rekognition/latest/APIReference/API_CreateCollection.html
%%
%% CreateUser:
%% https://docs.aws.amazon.com/rekognition/latest/APIReference/API_CreateUser.html
%%
%% DeleteCollection:
%% https://docs.aws.amazon.com/rekognition/latest/APIReference/API_DeleteCollection.html
%%
%% DeleteFaces:
%% https://docs.aws.amazon.com/rekognition/latest/APIReference/API_DeleteFaces.html
%%
%% DeleteUser:
%% https://docs.aws.amazon.com/rekognition/latest/APIReference/API_DeleteUser.html
%%
%% DescribeCollection:
%% https://docs.aws.amazon.com/rekognition/latest/APIReference/API_DescribeCollection.html
%%
%% DetectFaces:
%% https://docs.aws.amazon.com/rekognition/latest/APIReference/API_DetectFaces.html
%%
%% DetectLabels:
%% https://docs.aws.amazon.com/rekognition/latest/APIReference/API_DetectLabels.html
%%
%% DetectModerationLabels:
%% https://docs.aws.amazon.com/rekognition/latest/APIReference/API_DetectModerationLabels.html
%%
%% DetectProtectiveEquipment:
%% https://docs.aws.amazon.com/rekognition/latest/APIReference/API_DetectProtectiveEquipment.html
%%
%% DetectText:
%% https://docs.aws.amazon.com/rekognition/latest/APIReference/API_DetectText.html
%%
%% DisassociateFaces:
%% https://docs.aws.amazon.com/rekognition/latest/APIReference/API_DisassociateFaces.html
%%
%% GetCelebrityInfo:
%% https://docs.aws.amazon.com/rekognition/latest/APIReference/API_GetCelebrityInfo.html
%%
%% GetMediaAnalysisJob:
%% https://docs.aws.amazon.com/rekognition/latest/APIReference/API_GetMediaAnalysisJob.html
%%
%% IndexFaces:
%% https://docs.aws.amazon.com/rekognition/latest/APIReference/API_IndexFaces.html
%%
%% ListCollections:
%% https://docs.aws.amazon.com/rekognition/latest/APIReference/API_ListCollections.html
%%
%% ListMediaAnalysisJob:
%% https://docs.aws.amazon.com/rekognition/latest/APIReference/API_ListMediaAnalysisJob.html
%%
%% ListFaces:
%% https://docs.aws.amazon.com/rekognition/latest/APIReference/API_ListFaces.html
%%
%% ListUsers:
%% https://docs.aws.amazon.com/rekognition/latest/APIReference/API_ListFaces.html
%%
%% RecognizeCelebrities:
%% https://docs.aws.amazon.com/rekognition/latest/APIReference/API_RecognizeCelebrities.html
%%
%% SearchFaces:
%% https://docs.aws.amazon.com/rekognition/latest/APIReference/API_SearchFaces.html
%%
%% SearchFacesByImage:
%% https://docs.aws.amazon.com/rekognition/latest/APIReference/API_SearchFacesByImage.html
%%
%% SearchUsers:
%% https://docs.aws.amazon.com/rekognition/latest/APIReference/API_SearchUsers.html
%%
%% SearchUsersByImage:
%% https://docs.aws.amazon.com/rekognition/latest/APIReference/API_SearchUsersByImage.html
%%
%% StartMediaAnalysisJob:
%% https://docs.aws.amazon.com/rekognition/latest/APIReference/API_StartMediaAnalysisJob.html
%%
%% Amazon Rekognition Custom Labels
%%
%% CopyProjectVersion:
%% https://docs.aws.amazon.com/rekognition/latest/APIReference/API_CopyProjectVersion.html
%%
%% CreateDataset:
%% https://docs.aws.amazon.com/rekognition/latest/APIReference/API_CreateDataset.html
%%
%% CreateProject:
%% https://docs.aws.amazon.com/rekognition/latest/APIReference/API_CreateProject.html
%%
%% CreateProjectVersion:
%% https://docs.aws.amazon.com/rekognition/latest/APIReference/API_CreateProjectVersion.html
%%
%% DeleteDataset:
%% https://docs.aws.amazon.com/rekognition/latest/APIReference/API_DeleteDataset.html
%%
%% DeleteProject:
%% https://docs.aws.amazon.com/rekognition/latest/APIReference/API_DeleteProject.html
%%
%% DeleteProjectPolicy:
%% https://docs.aws.amazon.com/rekognition/latest/APIReference/API_DeleteProjectPolicy.html
%%
%% DeleteProjectVersion:
%% https://docs.aws.amazon.com/rekognition/latest/APIReference/API_DeleteProjectVersion.html
%%
%% DescribeDataset:
%% https://docs.aws.amazon.com/rekognition/latest/APIReference/API_DescribeDataset.html
%%
%% DescribeProjects:
%% https://docs.aws.amazon.com/rekognition/latest/APIReference/API_DescribeProjects.html
%%
%% DescribeProjectVersions:
%% https://docs.aws.amazon.com/rekognition/latest/APIReference/API_DescribeProjectVersions.html
%%
%% DetectCustomLabels:
%% https://docs.aws.amazon.com/rekognition/latest/APIReference/API_DetectCustomLabels.html
%%
%% DistributeDatasetEntries:
%% https://docs.aws.amazon.com/rekognition/latest/APIReference/API_DistributeDatasetEntries.html
%%
%% ListDatasetEntries:
%% https://docs.aws.amazon.com/rekognition/latest/APIReference/API_ListDatasetEntries.html
%%
%% ListDatasetLabels:
%% https://docs.aws.amazon.com/rekognition/latest/APIReference/API_ListDatasetLabels.html
%%
%% ListProjectPolicies:
%% https://docs.aws.amazon.com/rekognition/latest/APIReference/API_ListProjectPolicies.html
%%
%% PutProjectPolicy:
%% https://docs.aws.amazon.com/rekognition/latest/APIReference/API_PutProjectPolicy.html
%%
%% StartProjectVersion:
%% https://docs.aws.amazon.com/rekognition/latest/APIReference/API_StartProjectVersion.html
%%
%% StopProjectVersion:
%% https://docs.aws.amazon.com/rekognition/latest/APIReference/API_StopProjectVersion.html
%%
%% UpdateDatasetEntries:
%% https://docs.aws.amazon.com/rekognition/latest/APIReference/API_UpdateDatasetEntries.html
%%
%% Amazon Rekognition Video Stored Video
%%
%% GetCelebrityRecognition:
%% https://docs.aws.amazon.com/rekognition/latest/APIReference/API_GetCelebrityRecognition.html
%%
%% GetContentModeration:
%% https://docs.aws.amazon.com/rekognition/latest/APIReference/API_GetContentModeration.html
%%
%% GetFaceDetection:
%% https://docs.aws.amazon.com/rekognition/latest/APIReference/API_GetFaceDetection.html
%%
%% GetFaceSearch:
%% https://docs.aws.amazon.com/rekognition/latest/APIReference/API_GetFaceSearch.html
%%
%% GetLabelDetection:
%% https://docs.aws.amazon.com/rekognition/latest/APIReference/API_GetLabelDetection.html
%%
%% GetPersonTracking:
%% https://docs.aws.amazon.com/rekognition/latest/APIReference/API_GetPersonTracking.html
%%
%% GetSegmentDetection:
%% https://docs.aws.amazon.com/rekognition/latest/APIReference/API_GetSegmentDetection.html
%%
%% GetTextDetection:
%% https://docs.aws.amazon.com/rekognition/latest/APIReference/API_GetTextDetection.html
%%
%% StartCelebrityRecognition:
%% https://docs.aws.amazon.com/rekognition/latest/APIReference/API_StartCelebrityRecognition.html
%%
%% StartContentModeration:
%% https://docs.aws.amazon.com/rekognition/latest/APIReference/API_StartContentModeration.html
%%
%% StartFaceDetection:
%% https://docs.aws.amazon.com/rekognition/latest/APIReference/API_StartFaceDetection.html
%%
%% StartFaceSearch:
%% https://docs.aws.amazon.com/rekognition/latest/APIReference/API_StartFaceSearch.html
%%
%% StartLabelDetection:
%% https://docs.aws.amazon.com/rekognition/latest/APIReference/API_StartLabelDetection.html
%%
%% StartPersonTracking:
%% https://docs.aws.amazon.com/rekognition/latest/APIReference/API_StartPersonTracking.html
%%
%% StartSegmentDetection:
%% https://docs.aws.amazon.com/rekognition/latest/APIReference/API_StartSegmentDetection.html
%%
%% StartTextDetection:
%% https://docs.aws.amazon.com/rekognition/latest/APIReference/API_StartTextDetection.html
%%
%% Amazon Rekognition Video Streaming Video
%%
%% CreateStreamProcessor:
%% https://docs.aws.amazon.com/rekognition/latest/APIReference/API_CreateStreamProcessor.html
%%
%% DeleteStreamProcessor:
%% https://docs.aws.amazon.com/rekognition/latest/APIReference/API_DeleteStreamProcessor.html
%%
%% DescribeStreamProcessor:
%% https://docs.aws.amazon.com/rekognition/latest/APIReference/API_DescribeStreamProcessor.html
%%
%% ListStreamProcessors:
%% https://docs.aws.amazon.com/rekognition/latest/APIReference/API_ListStreamProcessors.html
%%
%% StartStreamProcessor:
%% https://docs.aws.amazon.com/rekognition/latest/APIReference/API_StartStreamProcessor.html
%%
%% StopStreamProcessor:
%% https://docs.aws.amazon.com/rekognition/latest/APIReference/API_StopStreamProcessor.html
%%
%% UpdateStreamProcessor:
%% https://docs.aws.amazon.com/rekognition/latest/APIReference/API_UpdateStreamProcessor.html
-module(aws_rekognition).

-export([associate_faces/2,
         associate_faces/3,
         compare_faces/2,
         compare_faces/3,
         copy_project_version/2,
         copy_project_version/3,
         create_collection/2,
         create_collection/3,
         create_dataset/2,
         create_dataset/3,
         create_face_liveness_session/2,
         create_face_liveness_session/3,
         create_project/2,
         create_project/3,
         create_project_version/2,
         create_project_version/3,
         create_stream_processor/2,
         create_stream_processor/3,
         create_user/2,
         create_user/3,
         delete_collection/2,
         delete_collection/3,
         delete_dataset/2,
         delete_dataset/3,
         delete_faces/2,
         delete_faces/3,
         delete_project/2,
         delete_project/3,
         delete_project_policy/2,
         delete_project_policy/3,
         delete_project_version/2,
         delete_project_version/3,
         delete_stream_processor/2,
         delete_stream_processor/3,
         delete_user/2,
         delete_user/3,
         describe_collection/2,
         describe_collection/3,
         describe_dataset/2,
         describe_dataset/3,
         describe_project_versions/2,
         describe_project_versions/3,
         describe_projects/2,
         describe_projects/3,
         describe_stream_processor/2,
         describe_stream_processor/3,
         detect_custom_labels/2,
         detect_custom_labels/3,
         detect_faces/2,
         detect_faces/3,
         detect_labels/2,
         detect_labels/3,
         detect_moderation_labels/2,
         detect_moderation_labels/3,
         detect_protective_equipment/2,
         detect_protective_equipment/3,
         detect_text/2,
         detect_text/3,
         disassociate_faces/2,
         disassociate_faces/3,
         distribute_dataset_entries/2,
         distribute_dataset_entries/3,
         get_celebrity_info/2,
         get_celebrity_info/3,
         get_celebrity_recognition/2,
         get_celebrity_recognition/3,
         get_content_moderation/2,
         get_content_moderation/3,
         get_face_detection/2,
         get_face_detection/3,
         get_face_liveness_session_results/2,
         get_face_liveness_session_results/3,
         get_face_search/2,
         get_face_search/3,
         get_label_detection/2,
         get_label_detection/3,
         get_media_analysis_job/2,
         get_media_analysis_job/3,
         get_person_tracking/2,
         get_person_tracking/3,
         get_segment_detection/2,
         get_segment_detection/3,
         get_text_detection/2,
         get_text_detection/3,
         index_faces/2,
         index_faces/3,
         list_collections/2,
         list_collections/3,
         list_dataset_entries/2,
         list_dataset_entries/3,
         list_dataset_labels/2,
         list_dataset_labels/3,
         list_faces/2,
         list_faces/3,
         list_media_analysis_jobs/2,
         list_media_analysis_jobs/3,
         list_project_policies/2,
         list_project_policies/3,
         list_stream_processors/2,
         list_stream_processors/3,
         list_tags_for_resource/2,
         list_tags_for_resource/3,
         list_users/2,
         list_users/3,
         put_project_policy/2,
         put_project_policy/3,
         recognize_celebrities/2,
         recognize_celebrities/3,
         search_faces/2,
         search_faces/3,
         search_faces_by_image/2,
         search_faces_by_image/3,
         search_users/2,
         search_users/3,
         search_users_by_image/2,
         search_users_by_image/3,
         start_celebrity_recognition/2,
         start_celebrity_recognition/3,
         start_content_moderation/2,
         start_content_moderation/3,
         start_face_detection/2,
         start_face_detection/3,
         start_face_search/2,
         start_face_search/3,
         start_label_detection/2,
         start_label_detection/3,
         start_media_analysis_job/2,
         start_media_analysis_job/3,
         start_person_tracking/2,
         start_person_tracking/3,
         start_project_version/2,
         start_project_version/3,
         start_segment_detection/2,
         start_segment_detection/3,
         start_stream_processor/2,
         start_stream_processor/3,
         start_text_detection/2,
         start_text_detection/3,
         stop_project_version/2,
         stop_project_version/3,
         stop_stream_processor/2,
         stop_stream_processor/3,
         tag_resource/2,
         tag_resource/3,
         untag_resource/2,
         untag_resource/3,
         update_dataset_entries/2,
         update_dataset_entries/3,
         update_stream_processor/2,
         update_stream_processor/3]).

-include_lib("hackney/include/hackney_lib.hrl").


%% Example:
%% create_face_liveness_session_request_settings() :: #{
%%   <<"AuditImagesLimit">> => integer(),
%%   <<"ChallengePreferences">> => list(challenge_preference()),
%%   <<"OutputConfig">> => liveness_output_config()
%% }
-type create_face_liveness_session_request_settings() :: #{binary() => any()}.

%% Example:
%% video() :: #{
%%   <<"S3Object">> => s3_object()
%% }
-type video() :: #{binary() => any()}.

%% Example:
%% searched_user() :: #{
%%   <<"UserId">> => string()
%% }
-type searched_user() :: #{binary() => any()}.

%% Example:
%% update_dataset_entries_request() :: #{
%%   <<"Changes">> := dataset_changes(),
%%   <<"DatasetArn">> := string()
%% }
-type update_dataset_entries_request() :: #{binary() => any()}.

%% Example:
%% list_dataset_labels_request() :: #{
%%   <<"DatasetArn">> := string(),
%%   <<"MaxResults">> => integer(),
%%   <<"NextToken">> => string()
%% }
-type list_dataset_labels_request() :: #{binary() => any()}.

%% Example:
%% get_media_analysis_job_response() :: #{
%%   <<"CompletionTimestamp">> => non_neg_integer(),
%%   <<"CreationTimestamp">> => non_neg_integer(),
%%   <<"FailureDetails">> => media_analysis_job_failure_details(),
%%   <<"Input">> => media_analysis_input(),
%%   <<"JobId">> => string(),
%%   <<"JobName">> => string(),
%%   <<"KmsKeyId">> => string(),
%%   <<"ManifestSummary">> => media_analysis_manifest_summary(),
%%   <<"OperationsConfig">> => media_analysis_operations_config(),
%%   <<"OutputConfig">> => media_analysis_output_config(),
%%   <<"Results">> => media_analysis_results(),
%%   <<"Status">> => list(any())
%% }
-type get_media_analysis_job_response() :: #{binary() => any()}.

%% Example:
%% compared_source_image_face() :: #{
%%   <<"BoundingBox">> => bounding_box(),
%%   <<"Confidence">> => float()
%% }
-type compared_source_image_face() :: #{binary() => any()}.

%% Example:
%% segment_type_info() :: #{
%%   <<"ModelVersion">> => string(),
%%   <<"Type">> => list(any())
%% }
-type segment_type_info() :: #{binary() => any()}.

%% Example:
%% recognize_celebrities_request() :: #{
%%   <<"Image">> := image()
%% }
-type recognize_celebrities_request() :: #{binary() => any()}.

%% Example:
%% get_label_detection_request_metadata() :: #{
%%   <<"AggregateBy">> => list(any()),
%%   <<"SortBy">> => list(any())
%% }
-type get_label_detection_request_metadata() :: #{binary() => any()}.

%% Example:
%% detect_text_response() :: #{
%%   <<"TextDetections">> => list(text_detection()),
%%   <<"TextModelVersion">> => string()
%% }
-type detect_text_response() :: #{binary() => any()}.

%% Example:
%% detect_protective_equipment_response() :: #{
%%   <<"Persons">> => list(protective_equipment_person()),
%%   <<"ProtectiveEquipmentModelVersion">> => string(),
%%   <<"Summary">> => protective_equipment_summary()
%% }
-type detect_protective_equipment_response() :: #{binary() => any()}.

%% Example:
%% tag_resource_request() :: #{
%%   <<"ResourceArn">> := string(),
%%   <<"Tags">> := map()
%% }
-type tag_resource_request() :: #{binary() => any()}.

%% Example:
%% project_description() :: #{
%%   <<"AutoUpdate">> => list(any()),
%%   <<"CreationTimestamp">> => non_neg_integer(),
%%   <<"Datasets">> => list(dataset_metadata()),
%%   <<"Feature">> => list(any()),
%%   <<"ProjectArn">> => string(),
%%   <<"Status">> => list(any())
%% }
-type project_description() :: #{binary() => any()}.

%% Example:
%% update_dataset_entries_response() :: #{

%% }
-type update_dataset_entries_response() :: #{binary() => any()}.

%% Example:
%% start_shot_detection_filter() :: #{
%%   <<"MinSegmentConfidence">> => float()
%% }
-type start_shot_detection_filter() :: #{binary() => any()}.

%% Example:
%% delete_faces_request() :: #{
%%   <<"CollectionId">> := string(),
%%   <<"FaceIds">> := list(string())
%% }
-type delete_faces_request() :: #{binary() => any()}.

%% Example:
%% create_stream_processor_response() :: #{
%%   <<"StreamProcessorArn">> => string()
%% }
-type create_stream_processor_response() :: #{binary() => any()}.

%% Example:
%% start_face_search_response() :: #{
%%   <<"JobId">> => string()
%% }
-type start_face_search_response() :: #{binary() => any()}.

%% Example:
%% detect_labels_image_properties_settings() :: #{
%%   <<"MaxDominantColors">> => integer()
%% }
-type detect_labels_image_properties_settings() :: #{binary() => any()}.

%% Example:
%% customization_feature_content_moderation_config() :: #{
%%   <<"ConfidenceThreshold">> => float()
%% }
-type customization_feature_content_moderation_config() :: #{binary() => any()}.

%% Example:
%% search_users_response() :: #{
%%   <<"FaceModelVersion">> => string(),
%%   <<"SearchedFace">> => searched_face(),
%%   <<"SearchedUser">> => searched_user(),
%%   <<"UserMatches">> => list(user_match())
%% }
-type search_users_response() :: #{binary() => any()}.

%% Example:
%% matched_user() :: #{
%%   <<"UserId">> => string(),
%%   <<"UserStatus">> => list(any())
%% }
-type matched_user() :: #{binary() => any()}.

%% Example:
%% search_faces_response() :: #{
%%   <<"FaceMatches">> => list(face_match()),
%%   <<"FaceModelVersion">> => string(),
%%   <<"SearchedFaceId">> => string()
%% }
-type search_faces_response() :: #{binary() => any()}.

%% Example:
%% start_label_detection_request() :: #{
%%   <<"ClientRequestToken">> => string(),
%%   <<"Features">> => list(list(any())()),
%%   <<"JobTag">> => string(),
%%   <<"MinConfidence">> => float(),
%%   <<"NotificationChannel">> => notification_channel(),
%%   <<"Settings">> => label_detection_settings(),
%%   <<"Video">> := video()
%% }
-type start_label_detection_request() :: #{binary() => any()}.

%% Example:
%% delete_dataset_response() :: #{

%% }
-type delete_dataset_response() :: #{binary() => any()}.

%% Example:
%% list_dataset_labels_response() :: #{
%%   <<"DatasetLabelDescriptions">> => list(dataset_label_description()),
%%   <<"NextToken">> => string()
%% }
-type list_dataset_labels_response() :: #{binary() => any()}.

%% Example:
%% dataset_source() :: #{
%%   <<"DatasetArn">> => string(),
%%   <<"GroundTruthManifest">> => ground_truth_manifest()
%% }
-type dataset_source() :: #{binary() => any()}.

%% Example:
%% training_data_result() :: #{
%%   <<"Input">> => training_data(),
%%   <<"Output">> => training_data(),
%%   <<"Validation">> => validation_data()
%% }
-type training_data_result() :: #{binary() => any()}.

%% Example:
%% unindexed_face() :: #{
%%   <<"FaceDetail">> => face_detail(),
%%   <<"Reasons">> => list(list(any())())
%% }
-type unindexed_face() :: #{binary() => any()}.

%% Example:
%% geometry() :: #{
%%   <<"BoundingBox">> => bounding_box(),
%%   <<"Polygon">> => list(point())
%% }
-type geometry() :: #{binary() => any()}.

%% Example:
%% dataset_description() :: #{
%%   <<"CreationTimestamp">> => non_neg_integer(),
%%   <<"DatasetStats">> => dataset_stats(),
%%   <<"LastUpdatedTimestamp">> => non_neg_integer(),
%%   <<"Status">> => list(any()),
%%   <<"StatusMessage">> => string(),
%%   <<"StatusMessageCode">> => list(any())
%% }
-type dataset_description() :: #{binary() => any()}.

%% Example:
%% media_analysis_job_failure_details() :: #{
%%   <<"Code">> => list(any()),
%%   <<"Message">> => string()
%% }
-type media_analysis_job_failure_details() :: #{binary() => any()}.

%% Example:
%% untag_resource_response() :: #{

%% }
-type untag_resource_response() :: #{binary() => any()}.

%% Example:
%% stream_processor() :: #{
%%   <<"Name">> => string(),
%%   <<"Status">> => list(any())
%% }
-type stream_processor() :: #{binary() => any()}.

%% Example:
%% media_analysis_operations_config() :: #{
%%   <<"DetectModerationLabels">> => media_analysis_detect_moderation_labels_config()
%% }
-type media_analysis_operations_config() :: #{binary() => any()}.

%% Example:
%% start_media_analysis_job_request() :: #{
%%   <<"ClientRequestToken">> => string(),
%%   <<"Input">> := media_analysis_input(),
%%   <<"JobName">> => string(),
%%   <<"KmsKeyId">> => string(),
%%   <<"OperationsConfig">> := media_analysis_operations_config(),
%%   <<"OutputConfig">> := media_analysis_output_config()
%% }
-type start_media_analysis_job_request() :: #{binary() => any()}.

%% Example:
%% resource_in_use_exception() :: #{
%%   <<"Code">> => string(),
%%   <<"Logref">> => string(),
%%   <<"Message">> => string()
%% }
-type resource_in_use_exception() :: #{binary() => any()}.

%% Example:
%% index_faces_request() :: #{
%%   <<"CollectionId">> := string(),
%%   <<"DetectionAttributes">> => list(list(any())()),
%%   <<"ExternalImageId">> => string(),
%%   <<"Image">> := image(),
%%   <<"MaxFaces">> => integer(),
%%   <<"QualityFilter">> => list(any())
%% }
-type index_faces_request() :: #{binary() => any()}.

%% Example:
%% person_match() :: #{
%%   <<"FaceMatches">> => list(face_match()),
%%   <<"Person">> => person_detail(),
%%   <<"Timestamp">> => float()
%% }
-type person_match() :: #{binary() => any()}.

%% Example:
%% describe_projects_response() :: #{
%%   <<"NextToken">> => string(),
%%   <<"ProjectDescriptions">> => list(project_description())
%% }
-type describe_projects_response() :: #{binary() => any()}.

%% Example:
%% delete_faces_response() :: #{
%%   <<"DeletedFaces">> => list(string()),
%%   <<"UnsuccessfulFaceDeletions">> => list(unsuccessful_face_deletion())
%% }
-type delete_faces_response() :: #{binary() => any()}.

%% Example:
%% detect_faces_response() :: #{
%%   <<"FaceDetails">> => list(face_detail()),
%%   <<"OrientationCorrection">> => list(any())
%% }
-type detect_faces_response() :: #{binary() => any()}.

%% Example:
%% describe_stream_processor_response() :: #{
%%   <<"CreationTimestamp">> => non_neg_integer(),
%%   <<"DataSharingPreference">> => stream_processor_data_sharing_preference(),
%%   <<"Input">> => stream_processor_input(),
%%   <<"KmsKeyId">> => string(),
%%   <<"LastUpdateTimestamp">> => non_neg_integer(),
%%   <<"Name">> => string(),
%%   <<"NotificationChannel">> => stream_processor_notification_channel(),
%%   <<"Output">> => stream_processor_output(),
%%   <<"RegionsOfInterest">> => list(region_of_interest()),
%%   <<"RoleArn">> => string(),
%%   <<"Settings">> => stream_processor_settings(),
%%   <<"Status">> => list(any()),
%%   <<"StatusMessage">> => string(),
%%   <<"StreamProcessorArn">> => string()
%% }
-type describe_stream_processor_response() :: #{binary() => any()}.

%% Example:
%% stream_processing_start_selector() :: #{
%%   <<"KVSStreamStartSelector">> => kinesis_video_stream_start_selector()
%% }
-type stream_processing_start_selector() :: #{binary() => any()}.

%% Example:
%% versions() :: #{
%%   <<"Maximum">> => string(),
%%   <<"Minimum">> => string()
%% }
-type versions() :: #{binary() => any()}.

%% Example:
%% smile() :: #{
%%   <<"Confidence">> => float(),
%%   <<"Value">> => boolean()
%% }
-type smile() :: #{binary() => any()}.

%% Example:
%% connected_home_settings() :: #{
%%   <<"Labels">> => list(string()),
%%   <<"MinConfidence">> => float()
%% }
-type connected_home_settings() :: #{binary() => any()}.

%% Example:
%% get_celebrity_info_response() :: #{
%%   <<"KnownGender">> => known_gender(),
%%   <<"Name">> => string(),
%%   <<"Urls">> => list(string())
%% }
-type get_celebrity_info_response() :: #{binary() => any()}.

%% Example:
%% stop_project_version_request() :: #{
%%   <<"ProjectVersionArn">> := string()
%% }
-type stop_project_version_request() :: #{binary() => any()}.

%% Example:
%% media_analysis_output_config() :: #{
%%   <<"S3Bucket">> => string(),
%%   <<"S3KeyPrefix">> => string()
%% }
-type media_analysis_output_config() :: #{binary() => any()}.

%% Example:
%% person_detail() :: #{
%%   <<"BoundingBox">> => bounding_box(),
%%   <<"Face">> => face_detail(),
%%   <<"Index">> => float()
%% }
-type person_detail() :: #{binary() => any()}.

%% Example:
%% get_content_moderation_request_metadata() :: #{
%%   <<"AggregateBy">> => list(any()),
%%   <<"SortBy">> => list(any())
%% }
-type get_content_moderation_request_metadata() :: #{binary() => any()}.

%% Example:
%% get_face_search_request() :: #{
%%   <<"JobId">> := string(),
%%   <<"MaxResults">> => integer(),
%%   <<"NextToken">> => string(),
%%   <<"SortBy">> => list(any())
%% }
-type get_face_search_request() :: #{binary() => any()}.

%% Example:
%% start_stream_processor_response() :: #{
%%   <<"SessionId">> => string()
%% }
-type start_stream_processor_response() :: #{binary() => any()}.

%% Example:
%% associate_faces_response() :: #{
%%   <<"AssociatedFaces">> => list(associated_face()),
%%   <<"UnsuccessfulFaceAssociations">> => list(unsuccessful_face_association()),
%%   <<"UserStatus">> => list(any())
%% }
-type associate_faces_response() :: #{binary() => any()}.

%% Example:
%% unsuccessful_face_deletion() :: #{
%%   <<"FaceId">> => string(),
%%   <<"Reasons">> => list(list(any())()),
%%   <<"UserId">> => string()
%% }
-type unsuccessful_face_deletion() :: #{binary() => any()}.

%% Example:
%% start_face_detection_response() :: #{
%%   <<"JobId">> => string()
%% }
-type start_face_detection_response() :: #{binary() => any()}.

%% Example:
%% protective_equipment_person() :: #{
%%   <<"BodyParts">> => list(protective_equipment_body_part()),
%%   <<"BoundingBox">> => bounding_box(),
%%   <<"Confidence">> => float(),
%%   <<"Id">> => integer()
%% }
-type protective_equipment_person() :: #{binary() => any()}.

%% Example:
%% eye_direction() :: #{
%%   <<"Confidence">> => float(),
%%   <<"Pitch">> => float(),
%%   <<"Yaw">> => float()
%% }
-type eye_direction() :: #{binary() => any()}.

%% Example:
%% start_text_detection_request() :: #{
%%   <<"ClientRequestToken">> => string(),
%%   <<"Filters">> => start_text_detection_filters(),
%%   <<"JobTag">> => string(),
%%   <<"NotificationChannel">> => notification_channel(),
%%   <<"Video">> := video()
%% }
-type start_text_detection_request() :: #{binary() => any()}.

%% Example:
%% get_content_moderation_request() :: #{
%%   <<"AggregateBy">> => list(any()),
%%   <<"JobId">> := string(),
%%   <<"MaxResults">> => integer(),
%%   <<"NextToken">> => string(),
%%   <<"SortBy">> => list(any())
%% }
-type get_content_moderation_request() :: #{binary() => any()}.

%% Example:
%% s3_object() :: #{
%%   <<"Bucket">> => string(),
%%   <<"Name">> => string(),
%%   <<"Version">> => string()
%% }
-type s3_object() :: #{binary() => any()}.

%% Example:
%% connected_home_settings_for_update() :: #{
%%   <<"Labels">> => list(string()),
%%   <<"MinConfidence">> => float()
%% }
-type connected_home_settings_for_update() :: #{binary() => any()}.

%% Example:
%% media_analysis_manifest_summary() :: #{
%%   <<"S3Object">> => s3_object()
%% }
-type media_analysis_manifest_summary() :: #{binary() => any()}.

%% Example:
%% detect_moderation_labels_response() :: #{
%%   <<"ContentTypes">> => list(content_type()),
%%   <<"HumanLoopActivationOutput">> => human_loop_activation_output(),
%%   <<"ModerationLabels">> => list(moderation_label()),
%%   <<"ModerationModelVersion">> => string(),
%%   <<"ProjectVersion">> => string()
%% }
-type detect_moderation_labels_response() :: #{binary() => any()}.

%% Example:
%% image_too_large_exception() :: #{
%%   <<"Code">> => string(),
%%   <<"Logref">> => string(),
%%   <<"Message">> => string()
%% }
-type image_too_large_exception() :: #{binary() => any()}.

%% Example:
%% search_faces_request() :: #{
%%   <<"CollectionId">> := string(),
%%   <<"FaceId">> := string(),
%%   <<"FaceMatchThreshold">> => float(),
%%   <<"MaxFaces">> => integer()
%% }
-type search_faces_request() :: #{binary() => any()}.

%% Example:
%% start_project_version_response() :: #{
%%   <<"Status">> => list(any())
%% }
-type start_project_version_response() :: #{binary() => any()}.

%% Example:
%% disassociate_faces_response() :: #{
%%   <<"DisassociatedFaces">> => list(disassociated_face()),
%%   <<"UnsuccessfulFaceDisassociations">> => list(unsuccessful_face_disassociation()),
%%   <<"UserStatus">> => list(any())
%% }
-type disassociate_faces_response() :: #{binary() => any()}.

%% Example:
%% stream_processor_output() :: #{
%%   <<"KinesisDataStream">> => kinesis_data_stream(),
%%   <<"S3Destination">> => s3_destination()
%% }
-type stream_processor_output() :: #{binary() => any()}.

%% Example:
%% training_data() :: #{
%%   <<"Assets">> => list(asset())
%% }
-type training_data() :: #{binary() => any()}.

%% Example:
%% create_project_version_response() :: #{
%%   <<"ProjectVersionArn">> => string()
%% }
-type create_project_version_response() :: #{binary() => any()}.

%% Example:
%% detection_filter() :: #{
%%   <<"MinBoundingBoxHeight">> => float(),
%%   <<"MinBoundingBoxWidth">> => float(),
%%   <<"MinConfidence">> => float()
%% }
-type detection_filter() :: #{binary() => any()}.

%% Example:
%% untag_resource_request() :: #{
%%   <<"ResourceArn">> := string(),
%%   <<"TagKeys">> := list(string())
%% }
-type untag_resource_request() :: #{binary() => any()}.

%% Example:
%% dataset_label_description() :: #{
%%   <<"LabelName">> => string(),
%%   <<"LabelStats">> => dataset_label_stats()
%% }
-type dataset_label_description() :: #{binary() => any()}.

%% Example:
%% known_gender() :: #{
%%   <<"Type">> => list(any())
%% }
-type known_gender() :: #{binary() => any()}.

%% Example:
%% segment_detection() :: #{
%%   <<"DurationFrames">> => float(),
%%   <<"DurationMillis">> => float(),
%%   <<"DurationSMPTE">> => string(),
%%   <<"EndFrameNumber">> => float(),
%%   <<"EndTimecodeSMPTE">> => string(),
%%   <<"EndTimestampMillis">> => float(),
%%   <<"ShotSegment">> => shot_segment(),
%%   <<"StartFrameNumber">> => float(),
%%   <<"StartTimecodeSMPTE">> => string(),
%%   <<"StartTimestampMillis">> => float(),
%%   <<"TechnicalCueSegment">> => technical_cue_segment(),
%%   <<"Type">> => list(any())
%% }
-type segment_detection() :: #{binary() => any()}.

%% Example:
%% searched_face() :: #{
%%   <<"FaceId">> => string()
%% }
-type searched_face() :: #{binary() => any()}.

%% Example:
%% list_faces_request() :: #{
%%   <<"CollectionId">> := string(),
%%   <<"FaceIds">> => list(string()),
%%   <<"MaxResults">> => integer(),
%%   <<"NextToken">> => string(),
%%   <<"UserId">> => string()
%% }
-type list_faces_request() :: #{binary() => any()}.

%% Example:
%% detect_text_request() :: #{
%%   <<"Filters">> => detect_text_filters(),
%%   <<"Image">> := image()
%% }
-type detect_text_request() :: #{binary() => any()}.

%% Example:
%% get_label_detection_response() :: #{
%%   <<"GetRequestMetadata">> => get_label_detection_request_metadata(),
%%   <<"JobId">> => string(),
%%   <<"JobStatus">> => list(any()),
%%   <<"JobTag">> => string(),
%%   <<"LabelModelVersion">> => string(),
%%   <<"Labels">> => list(label_detection()),
%%   <<"NextToken">> => string(),
%%   <<"StatusMessage">> => string(),
%%   <<"Video">> => video(),
%%   <<"VideoMetadata">> => video_metadata()
%% }
-type get_label_detection_response() :: #{binary() => any()}.

%% Example:
%% moderation_label() :: #{
%%   <<"Confidence">> => float(),
%%   <<"Name">> => string(),
%%   <<"ParentName">> => string(),
%%   <<"TaxonomyLevel">> => integer()
%% }
-type moderation_label() :: #{binary() => any()}.

%% Example:
%% list_collections_response() :: #{
%%   <<"CollectionIds">> => list(string()),
%%   <<"FaceModelVersions">> => list(string()),
%%   <<"NextToken">> => string()
%% }
-type list_collections_response() :: #{binary() => any()}.

%% Example:
%% associated_face() :: #{
%%   <<"FaceId">> => string()
%% }
-type associated_face() :: #{binary() => any()}.

%% Example:
%% put_project_policy_response() :: #{
%%   <<"PolicyRevisionId">> => string()
%% }
-type put_project_policy_response() :: #{binary() => any()}.

%% Example:
%% compare_faces_request() :: #{
%%   <<"QualityFilter">> => list(any()),
%%   <<"SimilarityThreshold">> => float(),
%%   <<"SourceImage">> := image(),
%%   <<"TargetImage">> := image()
%% }
-type compare_faces_request() :: #{binary() => any()}.

%% Example:
%% region_of_interest() :: #{
%%   <<"BoundingBox">> => bounding_box(),
%%   <<"Polygon">> => list(point())
%% }
-type region_of_interest() :: #{binary() => any()}.

%% Example:
%% distribute_dataset_entries_request() :: #{
%%   <<"Datasets">> := list(distribute_dataset())
%% }
-type distribute_dataset_entries_request() :: #{binary() => any()}.

%% Example:
%% describe_projects_request() :: #{
%%   <<"Features">> => list(list(any())()),
%%   <<"MaxResults">> => integer(),
%%   <<"NextToken">> => string(),
%%   <<"ProjectNames">> => list(string())
%% }
-type describe_projects_request() :: #{binary() => any()}.

%% Example:
%% content_type() :: #{
%%   <<"Confidence">> => float(),
%%   <<"Name">> => string()
%% }
-type content_type() :: #{binary() => any()}.

%% Example:
%% emotion() :: #{
%%   <<"Confidence">> => float(),
%%   <<"Type">> => list(any())
%% }
-type emotion() :: #{binary() => any()}.

%% Example:
%% testing_data() :: #{
%%   <<"Assets">> => list(asset()),
%%   <<"AutoCreate">> => boolean()
%% }
-type testing_data() :: #{binary() => any()}.

%% Example:
%% get_face_liveness_session_results_request() :: #{
%%   <<"SessionId">> := string()
%% }
-type get_face_liveness_session_results_request() :: #{binary() => any()}.

%% Example:
%% notification_channel() :: #{
%%   <<"RoleArn">> => string(),
%%   <<"SNSTopicArn">> => string()
%% }
-type notification_channel() :: #{binary() => any()}.

%% Example:
%% get_celebrity_recognition_response() :: #{
%%   <<"Celebrities">> => list(celebrity_recognition()),
%%   <<"JobId">> => string(),
%%   <<"JobStatus">> => list(any()),
%%   <<"JobTag">> => string(),
%%   <<"NextToken">> => string(),
%%   <<"StatusMessage">> => string(),
%%   <<"Video">> => video(),
%%   <<"VideoMetadata">> => video_metadata()
%% }
-type get_celebrity_recognition_response() :: #{binary() => any()}.

%% Example:
%% point() :: #{
%%   <<"X">> => float(),
%%   <<"Y">> => float()
%% }
-type point() :: #{binary() => any()}.

%% Example:
%% create_dataset_response() :: #{
%%   <<"DatasetArn">> => string()
%% }
-type create_dataset_response() :: #{binary() => any()}.

%% Example:
%% update_stream_processor_response() :: #{

%% }
-type update_stream_processor_response() :: #{binary() => any()}.

%% Example:
%% human_loop_activation_output() :: #{
%%   <<"HumanLoopActivationConditionsEvaluationResults">> => string(),
%%   <<"HumanLoopActivationReasons">> => list(string()),
%%   <<"HumanLoopArn">> => string()
%% }
-type human_loop_activation_output() :: #{binary() => any()}.

%% Example:
%% eye_open() :: #{
%%   <<"Confidence">> => float(),
%%   <<"Value">> => boolean()
%% }
-type eye_open() :: #{binary() => any()}.

%% Example:
%% provisioned_throughput_exceeded_exception() :: #{
%%   <<"Code">> => string(),
%%   <<"Logref">> => string(),
%%   <<"Message">> => string()
%% }
-type provisioned_throughput_exceeded_exception() :: #{binary() => any()}.

%% Example:
%% invalid_policy_revision_id_exception() :: #{
%%   <<"Code">> => string(),
%%   <<"Logref">> => string(),
%%   <<"Message">> => string()
%% }
-type invalid_policy_revision_id_exception() :: #{binary() => any()}.

%% Example:
%% start_face_detection_request() :: #{
%%   <<"ClientRequestToken">> => string(),
%%   <<"FaceAttributes">> => list(any()),
%%   <<"JobTag">> => string(),
%%   <<"NotificationChannel">> => notification_channel(),
%%   <<"Video">> := video()
%% }
-type start_face_detection_request() :: #{binary() => any()}.

%% Example:
%% create_project_response() :: #{
%%   <<"ProjectArn">> => string()
%% }
-type create_project_response() :: #{binary() => any()}.

%% Example:
%% landmark() :: #{
%%   <<"Type">> => list(any()),
%%   <<"X">> => float(),
%%   <<"Y">> => float()
%% }
-type landmark() :: #{binary() => any()}.

%% Example:
%% stop_stream_processor_response() :: #{

%% }
-type stop_stream_processor_response() :: #{binary() => any()}.

%% Example:
%% conflict_exception() :: #{
%%   <<"Code">> => string(),
%%   <<"Logref">> => string(),
%%   <<"Message">> => string()
%% }
-type conflict_exception() :: #{binary() => any()}.

%% Example:
%% dominant_color() :: #{
%%   <<"Blue">> => integer(),
%%   <<"CSSColor">> => string(),
%%   <<"Green">> => integer(),
%%   <<"HexCode">> => string(),
%%   <<"PixelPercent">> => float(),
%%   <<"Red">> => integer(),
%%   <<"SimplifiedColor">> => string()
%% }
-type dominant_color() :: #{binary() => any()}.

%% Example:
%% resource_not_found_exception() :: #{
%%   <<"Code">> => string(),
%%   <<"Logref">> => string(),
%%   <<"Message">> => string()
%% }
-type resource_not_found_exception() :: #{binary() => any()}.

%% Example:
%% asset() :: #{
%%   <<"GroundTruthManifest">> => ground_truth_manifest()
%% }
-type asset() :: #{binary() => any()}.

%% Example:
%% dataset_changes() :: #{
%%   <<"GroundTruth">> => binary()
%% }
-type dataset_changes() :: #{binary() => any()}.

%% Example:
%% associate_faces_request() :: #{
%%   <<"ClientRequestToken">> => string(),
%%   <<"CollectionId">> := string(),
%%   <<"FaceIds">> := list(string()),
%%   <<"UserId">> := string(),
%%   <<"UserMatchThreshold">> => float()
%% }
-type associate_faces_request() :: #{binary() => any()}.

%% Example:
%% list_collections_request() :: #{
%%   <<"MaxResults">> => integer(),
%%   <<"NextToken">> => string()
%% }
-type list_collections_request() :: #{binary() => any()}.

%% Example:
%% delete_collection_request() :: #{
%%   <<"CollectionId">> := string()
%% }
-type delete_collection_request() :: #{binary() => any()}.

%% Example:
%% list_media_analysis_jobs_response() :: #{
%%   <<"MediaAnalysisJobs">> => list(media_analysis_job_description()),
%%   <<"NextToken">> => string()
%% }
-type list_media_analysis_jobs_response() :: #{binary() => any()}.

%% Example:
%% face_occluded() :: #{
%%   <<"Confidence">> => float(),
%%   <<"Value">> => boolean()
%% }
-type face_occluded() :: #{binary() => any()}.

%% Example:
%% stream_processor_settings_for_update() :: #{
%%   <<"ConnectedHomeForUpdate">> => connected_home_settings_for_update()
%% }
-type stream_processor_settings_for_update() :: #{binary() => any()}.

%% Example:
%% detect_labels_image_properties() :: #{
%%   <<"Background">> => detect_labels_image_background(),
%%   <<"DominantColors">> => list(dominant_color()),
%%   <<"Foreground">> => detect_labels_image_foreground(),
%%   <<"Quality">> => detect_labels_image_quality()
%% }
-type detect_labels_image_properties() :: #{binary() => any()}.

%% Example:
%% searched_face_details() :: #{
%%   <<"FaceDetail">> => face_detail()
%% }
-type searched_face_details() :: #{binary() => any()}.

%% Example:
%% ground_truth_manifest() :: #{
%%   <<"S3Object">> => s3_object()
%% }
-type ground_truth_manifest() :: #{binary() => any()}.

%% Example:
%% label_category() :: #{
%%   <<"Name">> => string()
%% }
-type label_category() :: #{binary() => any()}.

%% Example:
%% get_text_detection_request() :: #{
%%   <<"JobId">> := string(),
%%   <<"MaxResults">> => integer(),
%%   <<"NextToken">> => string()
%% }
-type get_text_detection_request() :: #{binary() => any()}.

%% Example:
%% detect_labels_request() :: #{
%%   <<"Features">> => list(list(any())()),
%%   <<"Image">> := image(),
%%   <<"MaxLabels">> => integer(),
%%   <<"MinConfidence">> => float(),
%%   <<"Settings">> => detect_labels_settings()
%% }
-type detect_labels_request() :: #{binary() => any()}.

%% Example:
%% list_project_policies_request() :: #{
%%   <<"MaxResults">> => integer(),
%%   <<"NextToken">> => string(),
%%   <<"ProjectArn">> := string()
%% }
-type list_project_policies_request() :: #{binary() => any()}.

%% Example:
%% list_stream_processors_response() :: #{
%%   <<"NextToken">> => string(),
%%   <<"StreamProcessors">> => list(stream_processor())
%% }
-type list_stream_processors_response() :: #{binary() => any()}.

%% Example:
%% service_quota_exceeded_exception() :: #{
%%   <<"Code">> => string(),
%%   <<"Logref">> => string(),
%%   <<"Message">> => string()
%% }
-type service_quota_exceeded_exception() :: #{binary() => any()}.

%% Example:
%% list_stream_processors_request() :: #{
%%   <<"MaxResults">> => integer(),
%%   <<"NextToken">> => string()
%% }
-type list_stream_processors_request() :: #{binary() => any()}.

%% Example:
%% mouth_open() :: #{
%%   <<"Confidence">> => float(),
%%   <<"Value">> => boolean()
%% }
-type mouth_open() :: #{binary() => any()}.

%% Example:
%% start_technical_cue_detection_filter() :: #{
%%   <<"BlackFrame">> => black_frame(),
%%   <<"MinSegmentConfidence">> => float()
%% }
-type start_technical_cue_detection_filter() :: #{binary() => any()}.

%% Example:
%% idempotent_parameter_mismatch_exception() :: #{
%%   <<"Code">> => string(),
%%   <<"Logref">> => string(),
%%   <<"Message">> => string()
%% }
-type idempotent_parameter_mismatch_exception() :: #{binary() => any()}.

%% Example:
%% delete_project_policy_response() :: #{

%% }
-type delete_project_policy_response() :: #{binary() => any()}.

%% Example:
%% start_segment_detection_filters() :: #{
%%   <<"ShotFilter">> => start_shot_detection_filter(),
%%   <<"TechnicalCueFilter">> => start_technical_cue_detection_filter()
%% }
-type start_segment_detection_filters() :: #{binary() => any()}.

%% Example:
%% detect_custom_labels_request() :: #{
%%   <<"Image">> := image(),
%%   <<"MaxResults">> => integer(),
%%   <<"MinConfidence">> => float(),
%%   <<"ProjectVersionArn">> := string()
%% }
-type detect_custom_labels_request() :: #{binary() => any()}.

%% Example:
%% list_users_response() :: #{
%%   <<"NextToken">> => string(),
%%   <<"Users">> => list(user())
%% }
-type list_users_response() :: #{binary() => any()}.

%% Example:
%% describe_collection_request() :: #{
%%   <<"CollectionId">> := string()
%% }
-type describe_collection_request() :: #{binary() => any()}.

%% Example:
%% get_face_detection_response() :: #{
%%   <<"Faces">> => list(face_detection()),
%%   <<"JobId">> => string(),
%%   <<"JobStatus">> => list(any()),
%%   <<"JobTag">> => string(),
%%   <<"NextToken">> => string(),
%%   <<"StatusMessage">> => string(),
%%   <<"Video">> => video(),
%%   <<"VideoMetadata">> => video_metadata()
%% }
-type get_face_detection_response() :: #{binary() => any()}.

%% Example:
%% dataset_stats() :: #{
%%   <<"ErrorEntries">> => integer(),
%%   <<"LabeledEntries">> => integer(),
%%   <<"TotalEntries">> => integer(),
%%   <<"TotalLabels">> => integer()
%% }
-type dataset_stats() :: #{binary() => any()}.

%% Example:
%% label_detection() :: #{
%%   <<"DurationMillis">> => float(),
%%   <<"EndTimestampMillis">> => float(),
%%   <<"Label">> => label(),
%%   <<"StartTimestampMillis">> => float(),
%%   <<"Timestamp">> => float()
%% }
-type label_detection() :: #{binary() => any()}.

%% Example:
%% detect_text_filters() :: #{
%%   <<"RegionsOfInterest">> => list(region_of_interest()),
%%   <<"WordFilter">> => detection_filter()
%% }
-type detect_text_filters() :: #{binary() => any()}.

%% Example:
%% list_dataset_entries_request() :: #{
%%   <<"ContainsLabels">> => list(string()),
%%   <<"DatasetArn">> := string(),
%%   <<"HasErrors">> => boolean(),
%%   <<"Labeled">> => boolean(),
%%   <<"MaxResults">> => integer(),
%%   <<"NextToken">> => string(),
%%   <<"SourceRefContains">> => string()
%% }
-type list_dataset_entries_request() :: #{binary() => any()}.

%% Example:
%% detect_custom_labels_response() :: #{
%%   <<"CustomLabels">> => list(custom_label())
%% }
-type detect_custom_labels_response() :: #{binary() => any()}.

%% Example:
%% search_faces_by_image_request() :: #{
%%   <<"CollectionId">> := string(),
%%   <<"FaceMatchThreshold">> => float(),
%%   <<"Image">> := image(),
%%   <<"MaxFaces">> => integer(),
%%   <<"QualityFilter">> => list(any())
%% }
-type search_faces_by_image_request() :: #{binary() => any()}.

%% Example:
%% search_users_by_image_request() :: #{
%%   <<"CollectionId">> := string(),
%%   <<"Image">> := image(),
%%   <<"MaxUsers">> => integer(),
%%   <<"QualityFilter">> => list(any()),
%%   <<"UserMatchThreshold">> => float()
%% }
-type search_users_by_image_request() :: #{binary() => any()}.

%% Example:
%% start_celebrity_recognition_request() :: #{
%%   <<"ClientRequestToken">> => string(),
%%   <<"JobTag">> => string(),
%%   <<"NotificationChannel">> => notification_channel(),
%%   <<"Video">> := video()
%% }
-type start_celebrity_recognition_request() :: #{binary() => any()}.

%% Example:
%% create_collection_request() :: #{
%%   <<"CollectionId">> := string(),
%%   <<"Tags">> => map()
%% }
-type create_collection_request() :: #{binary() => any()}.

%% Example:
%% compare_faces_match() :: #{
%%   <<"Face">> => compared_face(),
%%   <<"Similarity">> => float()
%% }
-type compare_faces_match() :: #{binary() => any()}.

%% Example:
%% kinesis_video_stream_start_selector() :: #{
%%   <<"FragmentNumber">> => string(),
%%   <<"ProducerTimestamp">> => float()
%% }
-type kinesis_video_stream_start_selector() :: #{binary() => any()}.

%% Example:
%% project_version_description() :: #{
%%   <<"BaseModelVersion">> => string(),
%%   <<"BillableTrainingTimeInSeconds">> => float(),
%%   <<"CreationTimestamp">> => non_neg_integer(),
%%   <<"EvaluationResult">> => evaluation_result(),
%%   <<"Feature">> => list(any()),
%%   <<"FeatureConfig">> => customization_feature_config(),
%%   <<"KmsKeyId">> => string(),
%%   <<"ManifestSummary">> => ground_truth_manifest(),
%%   <<"MaxInferenceUnits">> => integer(),
%%   <<"MinInferenceUnits">> => integer(),
%%   <<"OutputConfig">> => output_config(),
%%   <<"ProjectVersionArn">> => string(),
%%   <<"SourceProjectVersionArn">> => string(),
%%   <<"Status">> => list(any()),
%%   <<"StatusMessage">> => string(),
%%   <<"TestingDataResult">> => testing_data_result(),
%%   <<"TrainingDataResult">> => training_data_result(),
%%   <<"TrainingEndTimestamp">> => non_neg_integer(),
%%   <<"VersionDescription">> => string()
%% }
-type project_version_description() :: #{binary() => any()}.

%% Example:
%% start_person_tracking_response() :: #{
%%   <<"JobId">> => string()
%% }
-type start_person_tracking_response() :: #{binary() => any()}.

%% Example:
%% list_tags_for_resource_response() :: #{
%%   <<"Tags">> => map()
%% }
-type list_tags_for_resource_response() :: #{binary() => any()}.

%% Example:
%% dataset_label_stats() :: #{
%%   <<"BoundingBoxCount">> => integer(),
%%   <<"EntryCount">> => integer()
%% }
-type dataset_label_stats() :: #{binary() => any()}.

%% Example:
%% content_moderation_detection() :: #{
%%   <<"ContentTypes">> => list(content_type()),
%%   <<"DurationMillis">> => float(),
%%   <<"EndTimestampMillis">> => float(),
%%   <<"ModerationLabel">> => moderation_label(),
%%   <<"StartTimestampMillis">> => float(),
%%   <<"Timestamp">> => float()
%% }
-type content_moderation_detection() :: #{binary() => any()}.

%% Example:
%% media_analysis_results() :: #{
%%   <<"ModelVersions">> => media_analysis_model_versions(),
%%   <<"S3Object">> => s3_object()
%% }
-type media_analysis_results() :: #{binary() => any()}.

%% Example:
%% get_label_detection_request() :: #{
%%   <<"AggregateBy">> => list(any()),
%%   <<"JobId">> := string(),
%%   <<"MaxResults">> => integer(),
%%   <<"NextToken">> => string(),
%%   <<"SortBy">> => list(any())
%% }
-type get_label_detection_request() :: #{binary() => any()}.

%% Example:
%% detect_protective_equipment_request() :: #{
%%   <<"Image">> := image(),
%%   <<"SummarizationAttributes">> => protective_equipment_summarization_attributes()
%% }
-type detect_protective_equipment_request() :: #{binary() => any()}.

%% Example:
%% search_users_request() :: #{
%%   <<"CollectionId">> := string(),
%%   <<"FaceId">> => string(),
%%   <<"MaxUsers">> => integer(),
%%   <<"UserId">> => string(),
%%   <<"UserMatchThreshold">> => float()
%% }
-type search_users_request() :: #{binary() => any()}.

%% Example:
%% search_users_by_image_response() :: #{
%%   <<"FaceModelVersion">> => string(),
%%   <<"SearchedFace">> => searched_face_details(),
%%   <<"UnsearchedFaces">> => list(unsearched_face()),
%%   <<"UserMatches">> => list(user_match())
%% }
-type search_users_by_image_response() :: #{binary() => any()}.

%% Example:
%% face_detail() :: #{
%%   <<"AgeRange">> => age_range(),
%%   <<"Beard">> => beard(),
%%   <<"BoundingBox">> => bounding_box(),
%%   <<"Confidence">> => float(),
%%   <<"Emotions">> => list(emotion()),
%%   <<"EyeDirection">> => eye_direction(),
%%   <<"Eyeglasses">> => eyeglasses(),
%%   <<"EyesOpen">> => eye_open(),
%%   <<"FaceOccluded">> => face_occluded(),
%%   <<"Gender">> => gender(),
%%   <<"Landmarks">> => list(landmark()),
%%   <<"MouthOpen">> => mouth_open(),
%%   <<"Mustache">> => mustache(),
%%   <<"Pose">> => pose(),
%%   <<"Quality">> => image_quality(),
%%   <<"Smile">> => smile(),
%%   <<"Sunglasses">> => sunglasses()
%% }
-type face_detail() :: #{binary() => any()}.

%% Example:
%% black_frame() :: #{
%%   <<"MaxPixelThreshold">> => float(),
%%   <<"MinCoveragePercentage">> => float()
%% }
-type black_frame() :: #{binary() => any()}.

%% Example:
%% detect_labels_response() :: #{
%%   <<"ImageProperties">> => detect_labels_image_properties(),
%%   <<"LabelModelVersion">> => string(),
%%   <<"Labels">> => list(label()),
%%   <<"OrientationCorrection">> => list(any())
%% }
-type detect_labels_response() :: #{binary() => any()}.

%% Example:
%% testing_data_result() :: #{
%%   <<"Input">> => testing_data(),
%%   <<"Output">> => testing_data(),
%%   <<"Validation">> => validation_data()
%% }
-type testing_data_result() :: #{binary() => any()}.

%% Example:
%% unsearched_face() :: #{
%%   <<"FaceDetails">> => face_detail(),
%%   <<"Reasons">> => list(list(any())())
%% }
-type unsearched_face() :: #{binary() => any()}.

%% Example:
%% face() :: #{
%%   <<"BoundingBox">> => bounding_box(),
%%   <<"Confidence">> => float(),
%%   <<"ExternalImageId">> => string(),
%%   <<"FaceId">> => string(),
%%   <<"ImageId">> => string(),
%%   <<"IndexFacesModelVersion">> => string(),
%%   <<"UserId">> => string()
%% }
-type face() :: #{binary() => any()}.

%% Example:
%% list_project_policies_response() :: #{
%%   <<"NextToken">> => string(),
%%   <<"ProjectPolicies">> => list(project_policy())
%% }
-type list_project_policies_response() :: #{binary() => any()}.

%% Example:
%% create_project_version_request() :: #{
%%   <<"FeatureConfig">> => customization_feature_config(),
%%   <<"KmsKeyId">> => string(),
%%   <<"OutputConfig">> := output_config(),
%%   <<"ProjectArn">> := string(),
%%   <<"Tags">> => map(),
%%   <<"TestingData">> => testing_data(),
%%   <<"TrainingData">> => training_data(),
%%   <<"VersionDescription">> => string(),
%%   <<"VersionName">> := string()
%% }
-type create_project_version_request() :: #{binary() => any()}.

%% Example:
%% beard() :: #{
%%   <<"Confidence">> => float(),
%%   <<"Value">> => boolean()
%% }
-type beard() :: #{binary() => any()}.

%% Example:
%% distribute_dataset_entries_response() :: #{

%% }
-type distribute_dataset_entries_response() :: #{binary() => any()}.

%% Example:
%% protective_equipment_summarization_attributes() :: #{
%%   <<"MinConfidence">> => float(),
%%   <<"RequiredEquipmentTypes">> => list(list(any())())
%% }
-type protective_equipment_summarization_attributes() :: #{binary() => any()}.

%% Example:
%% delete_user_request() :: #{
%%   <<"ClientRequestToken">> => string(),
%%   <<"CollectionId">> := string(),
%%   <<"UserId">> := string()
%% }
-type delete_user_request() :: #{binary() => any()}.

%% Example:
%% s3_destination() :: #{
%%   <<"Bucket">> => string(),
%%   <<"KeyPrefix">> => string()
%% }
-type s3_destination() :: #{binary() => any()}.

%% Example:
%% get_segment_detection_response() :: #{
%%   <<"AudioMetadata">> => list(audio_metadata()),
%%   <<"JobId">> => string(),
%%   <<"JobStatus">> => list(any()),
%%   <<"JobTag">> => string(),
%%   <<"NextToken">> => string(),
%%   <<"Segments">> => list(segment_detection()),
%%   <<"SelectedSegmentTypes">> => list(segment_type_info()),
%%   <<"StatusMessage">> => string(),
%%   <<"Video">> => video(),
%%   <<"VideoMetadata">> => list(video_metadata())
%% }
-type get_segment_detection_response() :: #{binary() => any()}.

%% Example:
%% custom_label() :: #{
%%   <<"Confidence">> => float(),
%%   <<"Geometry">> => geometry(),
%%   <<"Name">> => string()
%% }
-type custom_label() :: #{binary() => any()}.

%% Example:
%% celebrity() :: #{
%%   <<"Face">> => compared_face(),
%%   <<"Id">> => string(),
%%   <<"KnownGender">> => known_gender(),
%%   <<"MatchConfidence">> => float(),
%%   <<"Name">> => string(),
%%   <<"Urls">> => list(string())
%% }
-type celebrity() :: #{binary() => any()}.

%% Example:
%% delete_user_response() :: #{

%% }
-type delete_user_response() :: #{binary() => any()}.

%% Example:
%% get_celebrity_recognition_request() :: #{
%%   <<"JobId">> := string(),
%%   <<"MaxResults">> => integer(),
%%   <<"NextToken">> => string(),
%%   <<"SortBy">> => list(any())
%% }
-type get_celebrity_recognition_request() :: #{binary() => any()}.

%% Example:
%% challenge_preference() :: #{
%%   <<"Type">> => list(any()),
%%   <<"Versions">> => versions()
%% }
-type challenge_preference() :: #{binary() => any()}.

%% Example:
%% celebrity_recognition() :: #{
%%   <<"Celebrity">> => celebrity_detail(),
%%   <<"Timestamp">> => float()
%% }
-type celebrity_recognition() :: #{binary() => any()}.

%% Example:
%% recognize_celebrities_response() :: #{
%%   <<"CelebrityFaces">> => list(celebrity()),
%%   <<"OrientationCorrection">> => list(any()),
%%   <<"UnrecognizedFaces">> => list(compared_face())
%% }
-type recognize_celebrities_response() :: #{binary() => any()}.

%% Example:
%% challenge() :: #{
%%   <<"Type">> => list(any()),
%%   <<"Version">> => string()
%% }
-type challenge() :: #{binary() => any()}.

%% Example:
%% face_search_settings() :: #{
%%   <<"CollectionId">> => string(),
%%   <<"FaceMatchThreshold">> => float()
%% }
-type face_search_settings() :: #{binary() => any()}.

%% Example:
%% create_face_liveness_session_response() :: #{
%%   <<"SessionId">> => string()
%% }
-type create_face_liveness_session_response() :: #{binary() => any()}.

%% Example:
%% stream_processor_data_sharing_preference() :: #{
%%   <<"OptIn">> => boolean()
%% }
-type stream_processor_data_sharing_preference() :: #{binary() => any()}.

%% Example:
%% describe_collection_response() :: #{
%%   <<"CollectionARN">> => string(),
%%   <<"CreationTimestamp">> => non_neg_integer(),
%%   <<"FaceCount">> => float(),
%%   <<"FaceModelVersion">> => string(),
%%   <<"UserCount">> => float()
%% }
-type describe_collection_response() :: #{binary() => any()}.

%% Example:
%% stream_processor_settings() :: #{
%%   <<"ConnectedHome">> => connected_home_settings(),
%%   <<"FaceSearch">> => face_search_settings()
%% }
-type stream_processor_settings() :: #{binary() => any()}.

%% Example:
%% create_collection_response() :: #{
%%   <<"CollectionArn">> => string(),
%%   <<"FaceModelVersion">> => string(),
%%   <<"StatusCode">> => integer()
%% }
-type create_collection_response() :: #{binary() => any()}.

%% Example:
%% age_range() :: #{
%%   <<"High">> => integer(),
%%   <<"Low">> => integer()
%% }
-type age_range() :: #{binary() => any()}.

%% Example:
%% human_loop_data_attributes() :: #{
%%   <<"ContentClassifiers">> => list(list(any())())
%% }
-type human_loop_data_attributes() :: #{binary() => any()}.

%% Example:
%% protective_equipment_body_part() :: #{
%%   <<"Confidence">> => float(),
%%   <<"EquipmentDetections">> => list(equipment_detection()),
%%   <<"Name">> => list(any())
%% }
-type protective_equipment_body_part() :: #{binary() => any()}.

%% Example:
%% delete_project_response() :: #{
%%   <<"Status">> => list(any())
%% }
-type delete_project_response() :: #{binary() => any()}.

%% Example:
%% put_project_policy_request() :: #{
%%   <<"PolicyDocument">> := string(),
%%   <<"PolicyName">> := string(),
%%   <<"PolicyRevisionId">> => string(),
%%   <<"ProjectArn">> := string()
%% }
-type put_project_policy_request() :: #{binary() => any()}.

%% Example:
%% bounding_box() :: #{
%%   <<"Height">> => float(),
%%   <<"Left">> => float(),
%%   <<"Top">> => float(),
%%   <<"Width">> => float()
%% }
-type bounding_box() :: #{binary() => any()}.

%% Example:
%% delete_stream_processor_request() :: #{
%%   <<"Name">> := string()
%% }
-type delete_stream_processor_request() :: #{binary() => any()}.

%% Example:
%% unsuccessful_face_disassociation() :: #{
%%   <<"FaceId">> => string(),
%%   <<"Reasons">> => list(list(any())()),
%%   <<"UserId">> => string()
%% }
-type unsuccessful_face_disassociation() :: #{binary() => any()}.

%% Example:
%% stop_stream_processor_request() :: #{
%%   <<"Name">> := string()
%% }
-type stop_stream_processor_request() :: #{binary() => any()}.

%% Example:
%% face_match() :: #{
%%   <<"Face">> => face(),
%%   <<"Similarity">> => float()
%% }
-type face_match() :: #{binary() => any()}.

%% Example:
%% media_analysis_job_description() :: #{
%%   <<"CompletionTimestamp">> => non_neg_integer(),
%%   <<"CreationTimestamp">> => non_neg_integer(),
%%   <<"FailureDetails">> => media_analysis_job_failure_details(),
%%   <<"Input">> => media_analysis_input(),
%%   <<"JobId">> => string(),
%%   <<"JobName">> => string(),
%%   <<"KmsKeyId">> => string(),
%%   <<"ManifestSummary">> => media_analysis_manifest_summary(),
%%   <<"OperationsConfig">> => media_analysis_operations_config(),
%%   <<"OutputConfig">> => media_analysis_output_config(),
%%   <<"Results">> => media_analysis_results(),
%%   <<"Status">> => list(any())
%% }
-type media_analysis_job_description() :: #{binary() => any()}.

%% Example:
%% start_text_detection_response() :: #{
%%   <<"JobId">> => string()
%% }
-type start_text_detection_response() :: #{binary() => any()}.

%% Example:
%% label_detection_settings() :: #{
%%   <<"GeneralLabels">> => general_labels_settings()
%% }
-type label_detection_settings() :: #{binary() => any()}.

%% Example:
%% stream_processor_input() :: #{
%%   <<"KinesisVideoStream">> => kinesis_video_stream()
%% }
-type stream_processor_input() :: #{binary() => any()}.

%% Example:
%% get_person_tracking_response() :: #{
%%   <<"JobId">> => string(),
%%   <<"JobStatus">> => list(any()),
%%   <<"JobTag">> => string(),
%%   <<"NextToken">> => string(),
%%   <<"Persons">> => list(person_detection()),
%%   <<"StatusMessage">> => string(),
%%   <<"Video">> => video(),
%%   <<"VideoMetadata">> => video_metadata()
%% }
-type get_person_tracking_response() :: #{binary() => any()}.

%% Example:
%% copy_project_version_response() :: #{
%%   <<"ProjectVersionArn">> => string()
%% }
-type copy_project_version_response() :: #{binary() => any()}.

%% Example:
%% image() :: #{
%%   <<"Bytes">> => binary(),
%%   <<"S3Object">> => s3_object()
%% }
-type image() :: #{binary() => any()}.

%% Example:
%% list_media_analysis_jobs_request() :: #{
%%   <<"MaxResults">> => integer(),
%%   <<"NextToken">> => string()
%% }
-type list_media_analysis_jobs_request() :: #{binary() => any()}.

%% Example:
%% start_content_moderation_request() :: #{
%%   <<"ClientRequestToken">> => string(),
%%   <<"JobTag">> => string(),
%%   <<"MinConfidence">> => float(),
%%   <<"NotificationChannel">> => notification_channel(),
%%   <<"Video">> := video()
%% }
-type start_content_moderation_request() :: #{binary() => any()}.

%% Example:
%% covers_body_part() :: #{
%%   <<"Confidence">> => float(),
%%   <<"Value">> => boolean()
%% }
-type covers_body_part() :: #{binary() => any()}.

%% Example:
%% internal_server_error() :: #{
%%   <<"Code">> => string(),
%%   <<"Logref">> => string(),
%%   <<"Message">> => string()
%% }
-type internal_server_error() :: #{binary() => any()}.

%% Example:
%% instance() :: #{
%%   <<"BoundingBox">> => bounding_box(),
%%   <<"Confidence">> => float(),
%%   <<"DominantColors">> => list(dominant_color())
%% }
-type instance() :: #{binary() => any()}.

%% Example:
%% image_quality() :: #{
%%   <<"Brightness">> => float(),
%%   <<"Sharpness">> => float()
%% }
-type image_quality() :: #{binary() => any()}.

%% Example:
%% list_faces_response() :: #{
%%   <<"FaceModelVersion">> => string(),
%%   <<"Faces">> => list(face()),
%%   <<"NextToken">> => string()
%% }
-type list_faces_response() :: #{binary() => any()}.

%% Example:
%% access_denied_exception() :: #{
%%   <<"Code">> => string(),
%%   <<"Logref">> => string(),
%%   <<"Message">> => string()
%% }
-type access_denied_exception() :: #{binary() => any()}.

%% Example:
%% delete_project_policy_request() :: #{
%%   <<"PolicyName">> := string(),
%%   <<"PolicyRevisionId">> => string(),
%%   <<"ProjectArn">> := string()
%% }
-type delete_project_policy_request() :: #{binary() => any()}.

%% Example:
%% invalid_parameter_exception() :: #{
%%   <<"Code">> => string(),
%%   <<"Logref">> => string(),
%%   <<"Message">> => string()
%% }
-type invalid_parameter_exception() :: #{binary() => any()}.

%% Example:
%% create_stream_processor_request() :: #{
%%   <<"DataSharingPreference">> => stream_processor_data_sharing_preference(),
%%   <<"Input">> := stream_processor_input(),
%%   <<"KmsKeyId">> => string(),
%%   <<"Name">> := string(),
%%   <<"NotificationChannel">> => stream_processor_notification_channel(),
%%   <<"Output">> := stream_processor_output(),
%%   <<"RegionsOfInterest">> => list(region_of_interest()),
%%   <<"RoleArn">> := string(),
%%   <<"Settings">> := stream_processor_settings(),
%%   <<"Tags">> => map()
%% }
-type create_stream_processor_request() :: #{binary() => any()}.

%% Example:
%% index_faces_response() :: #{
%%   <<"FaceModelVersion">> => string(),
%%   <<"FaceRecords">> => list(face_record()),
%%   <<"OrientationCorrection">> => list(any()),
%%   <<"UnindexedFaces">> => list(unindexed_face())
%% }
-type index_faces_response() :: #{binary() => any()}.

%% Example:
%% tag_resource_response() :: #{

%% }
-type tag_resource_response() :: #{binary() => any()}.

%% Example:
%% get_media_analysis_job_request() :: #{
%%   <<"JobId">> := string()
%% }
-type get_media_analysis_job_request() :: #{binary() => any()}.

%% Example:
%% technical_cue_segment() :: #{
%%   <<"Confidence">> => float(),
%%   <<"Type">> => list(any())
%% }
-type technical_cue_segment() :: #{binary() => any()}.

%% Example:
%% output_config() :: #{
%%   <<"S3Bucket">> => string(),
%%   <<"S3KeyPrefix">> => string()
%% }
-type output_config() :: #{binary() => any()}.

%% Example:
%% video_metadata() :: #{
%%   <<"Codec">> => string(),
%%   <<"ColorRange">> => list(any()),
%%   <<"DurationMillis">> => float(),
%%   <<"Format">> => string(),
%%   <<"FrameHeight">> => float(),
%%   <<"FrameRate">> => float(),
%%   <<"FrameWidth">> => float()
%% }
-type video_metadata() :: #{binary() => any()}.

%% Example:
%% detect_moderation_labels_request() :: #{
%%   <<"HumanLoopConfig">> => human_loop_config(),
%%   <<"Image">> := image(),
%%   <<"MinConfidence">> => float(),
%%   <<"ProjectVersion">> => string()
%% }
-type detect_moderation_labels_request() :: #{binary() => any()}.

%% Example:
%% eyeglasses() :: #{
%%   <<"Confidence">> => float(),
%%   <<"Value">> => boolean()
%% }
-type eyeglasses() :: #{binary() => any()}.

%% Example:
%% start_content_moderation_response() :: #{
%%   <<"JobId">> => string()
%% }
-type start_content_moderation_response() :: #{binary() => any()}.

%% Example:
%% start_segment_detection_response() :: #{
%%   <<"JobId">> => string()
%% }
-type start_segment_detection_response() :: #{binary() => any()}.

%% Example:
%% unsuccessful_face_association() :: #{
%%   <<"Confidence">> => float(),
%%   <<"FaceId">> => string(),
%%   <<"Reasons">> => list(list(any())()),
%%   <<"UserId">> => string()
%% }
-type unsuccessful_face_association() :: #{binary() => any()}.

%% Example:
%% detect_faces_request() :: #{
%%   <<"Attributes">> => list(list(any())()),
%%   <<"Image">> := image()
%% }
-type detect_faces_request() :: #{binary() => any()}.

%% Example:
%% describe_dataset_response() :: #{
%%   <<"DatasetDescription">> => dataset_description()
%% }
-type describe_dataset_response() :: #{binary() => any()}.

%% Example:
%% person_detection() :: #{
%%   <<"Person">> => person_detail(),
%%   <<"Timestamp">> => float()
%% }
-type person_detection() :: #{binary() => any()}.

%% Example:
%% start_person_tracking_request() :: #{
%%   <<"ClientRequestToken">> => string(),
%%   <<"JobTag">> => string(),
%%   <<"NotificationChannel">> => notification_channel(),
%%   <<"Video">> := video()
%% }
-type start_person_tracking_request() :: #{binary() => any()}.

%% Example:
%% distribute_dataset() :: #{
%%   <<"Arn">> => string()
%% }
-type distribute_dataset() :: #{binary() => any()}.

%% Example:
%% list_tags_for_resource_request() :: #{
%%   <<"ResourceArn">> := string()
%% }
-type list_tags_for_resource_request() :: #{binary() => any()}.

%% Example:
%% invalid_image_format_exception() :: #{
%%   <<"Code">> => string(),
%%   <<"Logref">> => string(),
%%   <<"Message">> => string()
%% }
-type invalid_image_format_exception() :: #{binary() => any()}.

%% Example:
%% create_dataset_request() :: #{
%%   <<"DatasetSource">> => dataset_source(),
%%   <<"DatasetType">> := list(any()),
%%   <<"ProjectArn">> := string(),
%%   <<"Tags">> => map()
%% }
-type create_dataset_request() :: #{binary() => any()}.

%% Example:
%% face_detection() :: #{
%%   <<"Face">> => face_detail(),
%%   <<"Timestamp">> => float()
%% }
-type face_detection() :: #{binary() => any()}.

%% Example:
%% audit_image() :: #{
%%   <<"BoundingBox">> => bounding_box(),
%%   <<"Bytes">> => binary(),
%%   <<"S3Object">> => s3_object()
%% }
-type audit_image() :: #{binary() => any()}.

%% Example:
%% start_celebrity_recognition_response() :: #{
%%   <<"JobId">> => string()
%% }
-type start_celebrity_recognition_response() :: #{binary() => any()}.

%% Example:
%% get_segment_detection_request() :: #{
%%   <<"JobId">> := string(),
%%   <<"MaxResults">> => integer(),
%%   <<"NextToken">> => string()
%% }
-type get_segment_detection_request() :: #{binary() => any()}.

%% Example:
%% shot_segment() :: #{
%%   <<"Confidence">> => float(),
%%   <<"Index">> => float()
%% }
-type shot_segment() :: #{binary() => any()}.

%% Example:
%% get_face_liveness_session_results_response() :: #{
%%   <<"AuditImages">> => list(audit_image()),
%%   <<"Challenge">> => challenge(),
%%   <<"Confidence">> => float(),
%%   <<"ReferenceImage">> => audit_image(),
%%   <<"SessionId">> => string(),
%%   <<"Status">> => list(any())
%% }
-type get_face_liveness_session_results_response() :: #{binary() => any()}.

%% Example:
%% throttling_exception() :: #{
%%   <<"Code">> => string(),
%%   <<"Logref">> => string(),
%%   <<"Message">> => string()
%% }
-type throttling_exception() :: #{binary() => any()}.

%% Example:
%% delete_stream_processor_response() :: #{

%% }
-type delete_stream_processor_response() :: #{binary() => any()}.

%% Example:
%% disassociated_face() :: #{
%%   <<"FaceId">> => string()
%% }
-type disassociated_face() :: #{binary() => any()}.

%% Example:
%% user() :: #{
%%   <<"UserId">> => string(),
%%   <<"UserStatus">> => list(any())
%% }
-type user() :: #{binary() => any()}.

%% Example:
%% text_detection() :: #{
%%   <<"Confidence">> => float(),
%%   <<"DetectedText">> => string(),
%%   <<"Geometry">> => geometry(),
%%   <<"Id">> => integer(),
%%   <<"ParentId">> => integer(),
%%   <<"Type">> => list(any())
%% }
-type text_detection() :: #{binary() => any()}.

%% Example:
%% text_detection_result() :: #{
%%   <<"TextDetection">> => text_detection(),
%%   <<"Timestamp">> => float()
%% }
-type text_detection_result() :: #{binary() => any()}.

%% Example:
%% create_project_request() :: #{
%%   <<"AutoUpdate">> => list(any()),
%%   <<"Feature">> => list(any()),
%%   <<"ProjectName">> := string(),
%%   <<"Tags">> => map()
%% }
-type create_project_request() :: #{binary() => any()}.

%% Example:
%% malformed_policy_document_exception() :: #{
%%   <<"Code">> => string(),
%%   <<"Logref">> => string(),
%%   <<"Message">> => string()
%% }
-type malformed_policy_document_exception() :: #{binary() => any()}.

%% Example:
%% session_not_found_exception() :: #{
%%   <<"Code">> => string(),
%%   <<"Logref">> => string(),
%%   <<"Message">> => string()
%% }
-type session_not_found_exception() :: #{binary() => any()}.

%% Example:
%% customization_feature_config() :: #{
%%   <<"ContentModeration">> => customization_feature_content_moderation_config()
%% }
-type customization_feature_config() :: #{binary() => any()}.

%% Example:
%% describe_stream_processor_request() :: #{
%%   <<"Name">> := string()
%% }
-type describe_stream_processor_request() :: #{binary() => any()}.

%% Example:
%% pose() :: #{
%%   <<"Pitch">> => float(),
%%   <<"Roll">> => float(),
%%   <<"Yaw">> => float()
%% }
-type pose() :: #{binary() => any()}.

%% Example:
%% update_stream_processor_request() :: #{
%%   <<"DataSharingPreferenceForUpdate">> => stream_processor_data_sharing_preference(),
%%   <<"Name">> := string(),
%%   <<"ParametersToDelete">> => list(list(any())()),
%%   <<"RegionsOfInterestForUpdate">> => list(region_of_interest()),
%%   <<"SettingsForUpdate">> => stream_processor_settings_for_update()
%% }
-type update_stream_processor_request() :: #{binary() => any()}.

%% Example:
%% start_stream_processor_request() :: #{
%%   <<"Name">> := string(),
%%   <<"StartSelector">> => stream_processing_start_selector(),
%%   <<"StopSelector">> => stream_processing_stop_selector()
%% }
-type start_stream_processor_request() :: #{binary() => any()}.

%% Example:
%% copy_project_version_request() :: #{
%%   <<"DestinationProjectArn">> := string(),
%%   <<"KmsKeyId">> => string(),
%%   <<"OutputConfig">> := output_config(),
%%   <<"SourceProjectArn">> := string(),
%%   <<"SourceProjectVersionArn">> := string(),
%%   <<"Tags">> => map(),
%%   <<"VersionName">> := string()
%% }
-type copy_project_version_request() :: #{binary() => any()}.

%% Example:
%% equipment_detection() :: #{
%%   <<"BoundingBox">> => bounding_box(),
%%   <<"Confidence">> => float(),
%%   <<"CoversBodyPart">> => covers_body_part(),
%%   <<"Type">> => list(any())
%% }
-type equipment_detection() :: #{binary() => any()}.

%% Example:
%% list_users_request() :: #{
%%   <<"CollectionId">> := string(),
%%   <<"MaxResults">> => integer(),
%%   <<"NextToken">> => string()
%% }
-type list_users_request() :: #{binary() => any()}.

%% Example:
%% general_labels_settings() :: #{
%%   <<"LabelCategoryExclusionFilters">> => list(string()),
%%   <<"LabelCategoryInclusionFilters">> => list(string()),
%%   <<"LabelExclusionFilters">> => list(string()),
%%   <<"LabelInclusionFilters">> => list(string())
%% }
-type general_labels_settings() :: #{binary() => any()}.

%% Example:
%% media_analysis_input() :: #{
%%   <<"S3Object">> => s3_object()
%% }
-type media_analysis_input() :: #{binary() => any()}.

%% Example:
%% stream_processor_notification_channel() :: #{
%%   <<"SNSTopicArn">> => string()
%% }
-type stream_processor_notification_channel() :: #{binary() => any()}.

%% Example:
%% limit_exceeded_exception() :: #{
%%   <<"Code">> => string(),
%%   <<"Logref">> => string(),
%%   <<"Message">> => string()
%% }
-type limit_exceeded_exception() :: #{binary() => any()}.

%% Example:
%% start_media_analysis_job_response() :: #{
%%   <<"JobId">> => string()
%% }
-type start_media_analysis_job_response() :: #{binary() => any()}.

%% Example:
%% media_analysis_detect_moderation_labels_config() :: #{
%%   <<"MinConfidence">> => float(),
%%   <<"ProjectVersion">> => string()
%% }
-type media_analysis_detect_moderation_labels_config() :: #{binary() => any()}.

%% Example:
%% invalid_manifest_exception() :: #{
%%   <<"Code">> => string(),
%%   <<"Logref">> => string(),
%%   <<"Message">> => string()
%% }
-type invalid_manifest_exception() :: #{binary() => any()}.

%% Example:
%% video_too_large_exception() :: #{
%%   <<"Code">> => string(),
%%   <<"Logref">> => string(),
%%   <<"Message">> => string()
%% }
-type video_too_large_exception() :: #{binary() => any()}.

%% Example:
%% human_loop_quota_exceeded_exception() :: #{
%%   <<"Code">> => string(),
%%   <<"Logref">> => string(),
%%   <<"Message">> => string(),
%%   <<"QuotaCode">> => string(),
%%   <<"ResourceType">> => string(),
%%   <<"ServiceCode">> => string()
%% }
-type human_loop_quota_exceeded_exception() :: #{binary() => any()}.

%% Example:
%% disassociate_faces_request() :: #{
%%   <<"ClientRequestToken">> => string(),
%%   <<"CollectionId">> := string(),
%%   <<"FaceIds">> := list(string()),
%%   <<"UserId">> := string()
%% }
-type disassociate_faces_request() :: #{binary() => any()}.

%% Example:
%% get_text_detection_response() :: #{
%%   <<"JobId">> => string(),
%%   <<"JobStatus">> => list(any()),
%%   <<"JobTag">> => string(),
%%   <<"NextToken">> => string(),
%%   <<"StatusMessage">> => string(),
%%   <<"TextDetections">> => list(text_detection_result()),
%%   <<"TextModelVersion">> => string(),
%%   <<"Video">> => video(),
%%   <<"VideoMetadata">> => video_metadata()
%% }
-type get_text_detection_response() :: #{binary() => any()}.

%% Example:
%% get_celebrity_info_request() :: #{
%%   <<"Id">> := string()
%% }
-type get_celebrity_info_request() :: #{binary() => any()}.

%% Example:
%% protective_equipment_summary() :: #{
%%   <<"PersonsIndeterminate">> => list(integer()),
%%   <<"PersonsWithRequiredEquipment">> => list(integer()),
%%   <<"PersonsWithoutRequiredEquipment">> => list(integer())
%% }
-type protective_equipment_summary() :: #{binary() => any()}.

%% Example:
%% start_text_detection_filters() :: #{
%%   <<"RegionsOfInterest">> => list(region_of_interest()),
%%   <<"WordFilter">> => detection_filter()
%% }
-type start_text_detection_filters() :: #{binary() => any()}.

%% Example:
%% start_segment_detection_request() :: #{
%%   <<"ClientRequestToken">> => string(),
%%   <<"Filters">> => start_segment_detection_filters(),
%%   <<"JobTag">> => string(),
%%   <<"NotificationChannel">> => notification_channel(),
%%   <<"SegmentTypes">> := list(list(any())()),
%%   <<"Video">> := video()
%% }
-type start_segment_detection_request() :: #{binary() => any()}.

%% Example:
%% evaluation_result() :: #{
%%   <<"F1Score">> => float(),
%%   <<"Summary">> => summary()
%% }
-type evaluation_result() :: #{binary() => any()}.

%% Example:
%% user_match() :: #{
%%   <<"Similarity">> => float(),
%%   <<"User">> => matched_user()
%% }
-type user_match() :: #{binary() => any()}.

%% Example:
%% describe_dataset_request() :: #{
%%   <<"DatasetArn">> := string()
%% }
-type describe_dataset_request() :: #{binary() => any()}.

%% Example:
%% delete_dataset_request() :: #{
%%   <<"DatasetArn">> := string()
%% }
-type delete_dataset_request() :: #{binary() => any()}.

%% Example:
%% detect_labels_image_quality() :: #{
%%   <<"Brightness">> => float(),
%%   <<"Contrast">> => float(),
%%   <<"Sharpness">> => float()
%% }
-type detect_labels_image_quality() :: #{binary() => any()}.

%% Example:
%% describe_project_versions_response() :: #{
%%   <<"NextToken">> => string(),
%%   <<"ProjectVersionDescriptions">> => list(project_version_description())
%% }
-type describe_project_versions_response() :: #{binary() => any()}.

%% Example:
%% detect_labels_settings() :: #{
%%   <<"GeneralLabels">> => general_labels_settings(),
%%   <<"ImageProperties">> => detect_labels_image_properties_settings()
%% }
-type detect_labels_settings() :: #{binary() => any()}.

%% Example:
%% liveness_output_config() :: #{
%%   <<"S3Bucket">> => string(),
%%   <<"S3KeyPrefix">> => string()
%% }
-type liveness_output_config() :: #{binary() => any()}.

%% Example:
%% face_record() :: #{
%%   <<"Face">> => face(),
%%   <<"FaceDetail">> => face_detail()
%% }
-type face_record() :: #{binary() => any()}.

%% Example:
%% start_label_detection_response() :: #{
%%   <<"JobId">> => string()
%% }
-type start_label_detection_response() :: #{binary() => any()}.

%% Example:
%% parent() :: #{
%%   <<"Name">> => string()
%% }
-type parent() :: #{binary() => any()}.

%% Example:
%% resource_already_exists_exception() :: #{
%%   <<"Code">> => string(),
%%   <<"Logref">> => string(),
%%   <<"Message">> => string()
%% }
-type resource_already_exists_exception() :: #{binary() => any()}.

%% Example:
%% validation_data() :: #{
%%   <<"Assets">> => list(asset())
%% }
-type validation_data() :: #{binary() => any()}.

%% Example:
%% describe_project_versions_request() :: #{
%%   <<"MaxResults">> => integer(),
%%   <<"NextToken">> => string(),
%%   <<"ProjectArn">> := string(),
%%   <<"VersionNames">> => list(string())
%% }
-type describe_project_versions_request() :: #{binary() => any()}.

%% Example:
%% detect_labels_image_foreground() :: #{
%%   <<"DominantColors">> => list(dominant_color()),
%%   <<"Quality">> => detect_labels_image_quality()
%% }
-type detect_labels_image_foreground() :: #{binary() => any()}.

%% Example:
%% invalid_s3_object_exception() :: #{
%%   <<"Code">> => string(),
%%   <<"Logref">> => string(),
%%   <<"Message">> => string()
%% }
-type invalid_s3_object_exception() :: #{binary() => any()}.

%% Example:
%% media_analysis_model_versions() :: #{
%%   <<"Moderation">> => string()
%% }
-type media_analysis_model_versions() :: #{binary() => any()}.

%% Example:
%% delete_project_request() :: #{
%%   <<"ProjectArn">> := string()
%% }
-type delete_project_request() :: #{binary() => any()}.

%% Example:
%% kinesis_data_stream() :: #{
%%   <<"Arn">> => string()
%% }
-type kinesis_data_stream() :: #{binary() => any()}.

%% Example:
%% label_alias() :: #{
%%   <<"Name">> => string()
%% }
-type label_alias() :: #{binary() => any()}.

%% Example:
%% mustache() :: #{
%%   <<"Confidence">> => float(),
%%   <<"Value">> => boolean()
%% }
-type mustache() :: #{binary() => any()}.

%% Example:
%% sunglasses() :: #{
%%   <<"Confidence">> => float(),
%%   <<"Value">> => boolean()
%% }
-type sunglasses() :: #{binary() => any()}.

%% Example:
%% human_loop_config() :: #{
%%   <<"DataAttributes">> => human_loop_data_attributes(),
%%   <<"FlowDefinitionArn">> => string(),
%%   <<"HumanLoopName">> => string()
%% }
-type human_loop_config() :: #{binary() => any()}.

%% Example:
%% start_face_search_request() :: #{
%%   <<"ClientRequestToken">> => string(),
%%   <<"CollectionId">> := string(),
%%   <<"FaceMatchThreshold">> => float(),
%%   <<"JobTag">> => string(),
%%   <<"NotificationChannel">> => notification_channel(),
%%   <<"Video">> := video()
%% }
-type start_face_search_request() :: #{binary() => any()}.

%% Example:
%% resource_not_ready_exception() :: #{
%%   <<"Code">> => string(),
%%   <<"Logref">> => string(),
%%   <<"Message">> => string()
%% }
-type resource_not_ready_exception() :: #{binary() => any()}.

%% Example:
%% delete_collection_response() :: #{
%%   <<"StatusCode">> => integer()
%% }
-type delete_collection_response() :: #{binary() => any()}.

%% Example:
%% get_content_moderation_response() :: #{
%%   <<"GetRequestMetadata">> => get_content_moderation_request_metadata(),
%%   <<"JobId">> => string(),
%%   <<"JobStatus">> => list(any()),
%%   <<"JobTag">> => string(),
%%   <<"ModerationLabels">> => list(content_moderation_detection()),
%%   <<"ModerationModelVersion">> => string(),
%%   <<"NextToken">> => string(),
%%   <<"StatusMessage">> => string(),
%%   <<"Video">> => video(),
%%   <<"VideoMetadata">> => video_metadata()
%% }
-type get_content_moderation_response() :: #{binary() => any()}.

%% Example:
%% create_user_response() :: #{

%% }
-type create_user_response() :: #{binary() => any()}.

%% Example:
%% compare_faces_response() :: #{
%%   <<"FaceMatches">> => list(compare_faces_match()),
%%   <<"SourceImageFace">> => compared_source_image_face(),
%%   <<"SourceImageOrientationCorrection">> => list(any()),
%%   <<"TargetImageOrientationCorrection">> => list(any()),
%%   <<"UnmatchedFaces">> => list(compared_face())
%% }
-type compare_faces_response() :: #{binary() => any()}.

%% Example:
%% project_policy() :: #{
%%   <<"CreationTimestamp">> => non_neg_integer(),
%%   <<"LastUpdatedTimestamp">> => non_neg_integer(),
%%   <<"PolicyDocument">> => string(),
%%   <<"PolicyName">> => string(),
%%   <<"PolicyRevisionId">> => string(),
%%   <<"ProjectArn">> => string()
%% }
-type project_policy() :: #{binary() => any()}.

%% Example:
%% search_faces_by_image_response() :: #{
%%   <<"FaceMatches">> => list(face_match()),
%%   <<"FaceModelVersion">> => string(),
%%   <<"SearchedFaceBoundingBox">> => bounding_box(),
%%   <<"SearchedFaceConfidence">> => float()
%% }
-type search_faces_by_image_response() :: #{binary() => any()}.

%% Example:
%% list_dataset_entries_response() :: #{
%%   <<"DatasetEntries">> => list(string()),
%%   <<"NextToken">> => string()
%% }
-type list_dataset_entries_response() :: #{binary() => any()}.

%% Example:
%% summary() :: #{
%%   <<"S3Object">> => s3_object()
%% }
-type summary() :: #{binary() => any()}.

%% Example:
%% kinesis_video_stream() :: #{
%%   <<"Arn">> => string()
%% }
-type kinesis_video_stream() :: #{binary() => any()}.

%% Example:
%% dataset_metadata() :: #{
%%   <<"CreationTimestamp">> => non_neg_integer(),
%%   <<"DatasetArn">> => string(),
%%   <<"DatasetType">> => list(any()),
%%   <<"Status">> => list(any()),
%%   <<"StatusMessage">> => string(),
%%   <<"StatusMessageCode">> => list(any())
%% }
-type dataset_metadata() :: #{binary() => any()}.

%% Example:
%% detect_labels_image_background() :: #{
%%   <<"DominantColors">> => list(dominant_color()),
%%   <<"Quality">> => detect_labels_image_quality()
%% }
-type detect_labels_image_background() :: #{binary() => any()}.

%% Example:
%% celebrity_detail() :: #{
%%   <<"BoundingBox">> => bounding_box(),
%%   <<"Confidence">> => float(),
%%   <<"Face">> => face_detail(),
%%   <<"Id">> => string(),
%%   <<"KnownGender">> => known_gender(),
%%   <<"Name">> => string(),
%%   <<"Urls">> => list(string())
%% }
-type celebrity_detail() :: #{binary() => any()}.

%% Example:
%% stream_processing_stop_selector() :: #{
%%   <<"MaxDurationInSeconds">> => float()
%% }
-type stream_processing_stop_selector() :: #{binary() => any()}.

%% Example:
%% audio_metadata() :: #{
%%   <<"Codec">> => string(),
%%   <<"DurationMillis">> => float(),
%%   <<"NumberOfChannels">> => float(),
%%   <<"SampleRate">> => float()
%% }
-type audio_metadata() :: #{binary() => any()}.

%% Example:
%% gender() :: #{
%%   <<"Confidence">> => float(),
%%   <<"Value">> => list(any())
%% }
-type gender() :: #{binary() => any()}.

%% Example:
%% label() :: #{
%%   <<"Aliases">> => list(label_alias()),
%%   <<"Categories">> => list(label_category()),
%%   <<"Confidence">> => float(),
%%   <<"Instances">> => list(instance()),
%%   <<"Name">> => string(),
%%   <<"Parents">> => list(parent())
%% }
-type label() :: #{binary() => any()}.

%% Example:
%% get_face_search_response() :: #{
%%   <<"JobId">> => string(),
%%   <<"JobStatus">> => list(any()),
%%   <<"JobTag">> => string(),
%%   <<"NextToken">> => string(),
%%   <<"Persons">> => list(person_match()),
%%   <<"StatusMessage">> => string(),
%%   <<"Video">> => video(),
%%   <<"VideoMetadata">> => video_metadata()
%% }
-type get_face_search_response() :: #{binary() => any()}.

%% Example:
%% start_project_version_request() :: #{
%%   <<"MaxInferenceUnits">> => integer(),
%%   <<"MinInferenceUnits">> := integer(),
%%   <<"ProjectVersionArn">> := string()
%% }
-type start_project_version_request() :: #{binary() => any()}.

%% Example:
%% compared_face() :: #{
%%   <<"BoundingBox">> => bounding_box(),
%%   <<"Confidence">> => float(),
%%   <<"Emotions">> => list(emotion()),
%%   <<"Landmarks">> => list(landmark()),
%%   <<"Pose">> => pose(),
%%   <<"Quality">> => image_quality(),
%%   <<"Smile">> => smile()
%% }
-type compared_face() :: #{binary() => any()}.

%% Example:
%% get_face_detection_request() :: #{
%%   <<"JobId">> := string(),
%%   <<"MaxResults">> => integer(),
%%   <<"NextToken">> => string()
%% }
-type get_face_detection_request() :: #{binary() => any()}.

%% Example:
%% delete_project_version_request() :: #{
%%   <<"ProjectVersionArn">> := string()
%% }
-type delete_project_version_request() :: #{binary() => any()}.

%% Example:
%% stop_project_version_response() :: #{
%%   <<"Status">> => list(any())
%% }
-type stop_project_version_response() :: #{binary() => any()}.

%% Example:
%% create_user_request() :: #{
%%   <<"ClientRequestToken">> => string(),
%%   <<"CollectionId">> := string(),
%%   <<"UserId">> := string()
%% }
-type create_user_request() :: #{binary() => any()}.

%% Example:
%% create_face_liveness_session_request() :: #{
%%   <<"ClientRequestToken">> => string(),
%%   <<"KmsKeyId">> => string(),
%%   <<"Settings">> => create_face_liveness_session_request_settings()
%% }
-type create_face_liveness_session_request() :: #{binary() => any()}.

%% Example:
%% invalid_pagination_token_exception() :: #{
%%   <<"Code">> => string(),
%%   <<"Logref">> => string(),
%%   <<"Message">> => string()
%% }
-type invalid_pagination_token_exception() :: #{binary() => any()}.

%% Example:
%% delete_project_version_response() :: #{
%%   <<"Status">> => list(any())
%% }
-type delete_project_version_response() :: #{binary() => any()}.

%% Example:
%% get_person_tracking_request() :: #{
%%   <<"JobId">> := string(),
%%   <<"MaxResults">> => integer(),
%%   <<"NextToken">> => string(),
%%   <<"SortBy">> => list(any())
%% }
-type get_person_tracking_request() :: #{binary() => any()}.

-type associate_faces_errors() ::
    throttling_exception() | 
    invalid_parameter_exception() | 
    access_denied_exception() | 
    internal_server_error() | 
    idempotent_parameter_mismatch_exception() | 
    service_quota_exceeded_exception() | 
    resource_not_found_exception() | 
    conflict_exception() | 
    provisioned_throughput_exceeded_exception().

-type compare_faces_errors() ::
    invalid_s3_object_exception() | 
    throttling_exception() | 
    invalid_image_format_exception() | 
    invalid_parameter_exception() | 
    access_denied_exception() | 
    internal_server_error() | 
    provisioned_throughput_exceeded_exception() | 
    image_too_large_exception().

-type copy_project_version_errors() ::
    limit_exceeded_exception() | 
    throttling_exception() | 
    invalid_parameter_exception() | 
    access_denied_exception() | 
    internal_server_error() | 
    service_quota_exceeded_exception() | 
    resource_not_found_exception() | 
    provisioned_throughput_exceeded_exception() | 
    resource_in_use_exception().

-type create_collection_errors() ::
    resource_already_exists_exception() | 
    throttling_exception() | 
    invalid_parameter_exception() | 
    access_denied_exception() | 
    internal_server_error() | 
    service_quota_exceeded_exception() | 
    provisioned_throughput_exceeded_exception().

-type create_dataset_errors() ::
    invalid_s3_object_exception() | 
    resource_already_exists_exception() | 
    limit_exceeded_exception() | 
    throttling_exception() | 
    invalid_parameter_exception() | 
    access_denied_exception() | 
    internal_server_error() | 
    resource_not_found_exception() | 
    provisioned_throughput_exceeded_exception().

-type create_face_liveness_session_errors() ::
    throttling_exception() | 
    invalid_parameter_exception() | 
    access_denied_exception() | 
    internal_server_error() | 
    provisioned_throughput_exceeded_exception().

-type create_project_errors() ::
    limit_exceeded_exception() | 
    throttling_exception() | 
    invalid_parameter_exception() | 
    access_denied_exception() | 
    internal_server_error() | 
    provisioned_throughput_exceeded_exception() | 
    resource_in_use_exception().

-type create_project_version_errors() ::
    limit_exceeded_exception() | 
    throttling_exception() | 
    invalid_parameter_exception() | 
    access_denied_exception() | 
    internal_server_error() | 
    service_quota_exceeded_exception() | 
    resource_not_found_exception() | 
    provisioned_throughput_exceeded_exception() | 
    resource_in_use_exception().

-type create_stream_processor_errors() ::
    limit_exceeded_exception() | 
    throttling_exception() | 
    invalid_parameter_exception() | 
    access_denied_exception() | 
    internal_server_error() | 
    service_quota_exceeded_exception() | 
    provisioned_throughput_exceeded_exception() | 
    resource_in_use_exception().

-type create_user_errors() ::
    throttling_exception() | 
    invalid_parameter_exception() | 
    access_denied_exception() | 
    internal_server_error() | 
    idempotent_parameter_mismatch_exception() | 
    service_quota_exceeded_exception() | 
    resource_not_found_exception() | 
    conflict_exception() | 
    provisioned_throughput_exceeded_exception().

-type delete_collection_errors() ::
    throttling_exception() | 
    invalid_parameter_exception() | 
    access_denied_exception() | 
    internal_server_error() | 
    resource_not_found_exception() | 
    provisioned_throughput_exceeded_exception().

-type delete_dataset_errors() ::
    limit_exceeded_exception() | 
    throttling_exception() | 
    invalid_parameter_exception() | 
    access_denied_exception() | 
    internal_server_error() | 
    resource_not_found_exception() | 
    provisioned_throughput_exceeded_exception() | 
    resource_in_use_exception().

-type delete_faces_errors() ::
    throttling_exception() | 
    invalid_parameter_exception() | 
    access_denied_exception() | 
    internal_server_error() | 
    resource_not_found_exception() | 
    provisioned_throughput_exceeded_exception().

-type delete_project_errors() ::
    throttling_exception() | 
    invalid_parameter_exception() | 
    access_denied_exception() | 
    internal_server_error() | 
    resource_not_found_exception() | 
    provisioned_throughput_exceeded_exception() | 
    resource_in_use_exception().

-type delete_project_policy_errors() ::
    throttling_exception() | 
    invalid_parameter_exception() | 
    access_denied_exception() | 
    internal_server_error() | 
    resource_not_found_exception() | 
    invalid_policy_revision_id_exception() | 
    provisioned_throughput_exceeded_exception().

-type delete_project_version_errors() ::
    throttling_exception() | 
    invalid_parameter_exception() | 
    access_denied_exception() | 
    internal_server_error() | 
    resource_not_found_exception() | 
    provisioned_throughput_exceeded_exception() | 
    resource_in_use_exception().

-type delete_stream_processor_errors() ::
    throttling_exception() | 
    invalid_parameter_exception() | 
    access_denied_exception() | 
    internal_server_error() | 
    resource_not_found_exception() | 
    provisioned_throughput_exceeded_exception() | 
    resource_in_use_exception().

-type delete_user_errors() ::
    throttling_exception() | 
    invalid_parameter_exception() | 
    access_denied_exception() | 
    internal_server_error() | 
    idempotent_parameter_mismatch_exception() | 
    resource_not_found_exception() | 
    conflict_exception() | 
    provisioned_throughput_exceeded_exception().

-type describe_collection_errors() ::
    throttling_exception() | 
    invalid_parameter_exception() | 
    access_denied_exception() | 
    internal_server_error() | 
    resource_not_found_exception() | 
    provisioned_throughput_exceeded_exception().

-type describe_dataset_errors() ::
    throttling_exception() | 
    invalid_parameter_exception() | 
    access_denied_exception() | 
    internal_server_error() | 
    resource_not_found_exception() | 
    provisioned_throughput_exceeded_exception().

-type describe_project_versions_errors() ::
    invalid_pagination_token_exception() | 
    throttling_exception() | 
    invalid_parameter_exception() | 
    access_denied_exception() | 
    internal_server_error() | 
    resource_not_found_exception() | 
    provisioned_throughput_exceeded_exception().

-type describe_projects_errors() ::
    invalid_pagination_token_exception() | 
    throttling_exception() | 
    invalid_parameter_exception() | 
    access_denied_exception() | 
    internal_server_error() | 
    provisioned_throughput_exceeded_exception().

-type describe_stream_processor_errors() ::
    throttling_exception() | 
    invalid_parameter_exception() | 
    access_denied_exception() | 
    internal_server_error() | 
    resource_not_found_exception() | 
    provisioned_throughput_exceeded_exception().

-type detect_custom_labels_errors() ::
    resource_not_ready_exception() | 
    invalid_s3_object_exception() | 
    limit_exceeded_exception() | 
    throttling_exception() | 
    invalid_image_format_exception() | 
    invalid_parameter_exception() | 
    access_denied_exception() | 
    internal_server_error() | 
    resource_not_found_exception() | 
    provisioned_throughput_exceeded_exception() | 
    image_too_large_exception().

-type detect_faces_errors() ::
    invalid_s3_object_exception() | 
    throttling_exception() | 
    invalid_image_format_exception() | 
    invalid_parameter_exception() | 
    access_denied_exception() | 
    internal_server_error() | 
    provisioned_throughput_exceeded_exception() | 
    image_too_large_exception().

-type detect_labels_errors() ::
    invalid_s3_object_exception() | 
    throttling_exception() | 
    invalid_image_format_exception() | 
    invalid_parameter_exception() | 
    access_denied_exception() | 
    internal_server_error() | 
    provisioned_throughput_exceeded_exception() | 
    image_too_large_exception().

-type detect_moderation_labels_errors() ::
    resource_not_ready_exception() | 
    invalid_s3_object_exception() | 
    human_loop_quota_exceeded_exception() | 
    throttling_exception() | 
    invalid_image_format_exception() | 
    invalid_parameter_exception() | 
    access_denied_exception() | 
    internal_server_error() | 
    resource_not_found_exception() | 
    provisioned_throughput_exceeded_exception() | 
    image_too_large_exception().

-type detect_protective_equipment_errors() ::
    invalid_s3_object_exception() | 
    throttling_exception() | 
    invalid_image_format_exception() | 
    invalid_parameter_exception() | 
    access_denied_exception() | 
    internal_server_error() | 
    provisioned_throughput_exceeded_exception() | 
    image_too_large_exception().

-type detect_text_errors() ::
    invalid_s3_object_exception() | 
    throttling_exception() | 
    invalid_image_format_exception() | 
    invalid_parameter_exception() | 
    access_denied_exception() | 
    internal_server_error() | 
    provisioned_throughput_exceeded_exception() | 
    image_too_large_exception().

-type disassociate_faces_errors() ::
    throttling_exception() | 
    invalid_parameter_exception() | 
    access_denied_exception() | 
    internal_server_error() | 
    idempotent_parameter_mismatch_exception() | 
    resource_not_found_exception() | 
    conflict_exception() | 
    provisioned_throughput_exceeded_exception().

-type distribute_dataset_entries_errors() ::
    resource_not_ready_exception() | 
    throttling_exception() | 
    invalid_parameter_exception() | 
    access_denied_exception() | 
    internal_server_error() | 
    resource_not_found_exception() | 
    provisioned_throughput_exceeded_exception().

-type get_celebrity_info_errors() ::
    throttling_exception() | 
    invalid_parameter_exception() | 
    access_denied_exception() | 
    internal_server_error() | 
    resource_not_found_exception() | 
    provisioned_throughput_exceeded_exception().

-type get_celebrity_recognition_errors() ::
    invalid_pagination_token_exception() | 
    throttling_exception() | 
    invalid_parameter_exception() | 
    access_denied_exception() | 
    internal_server_error() | 
    resource_not_found_exception() | 
    provisioned_throughput_exceeded_exception().

-type get_content_moderation_errors() ::
    invalid_pagination_token_exception() | 
    throttling_exception() | 
    invalid_parameter_exception() | 
    access_denied_exception() | 
    internal_server_error() | 
    resource_not_found_exception() | 
    provisioned_throughput_exceeded_exception().

-type get_face_detection_errors() ::
    invalid_pagination_token_exception() | 
    throttling_exception() | 
    invalid_parameter_exception() | 
    access_denied_exception() | 
    internal_server_error() | 
    resource_not_found_exception() | 
    provisioned_throughput_exceeded_exception().

-type get_face_liveness_session_results_errors() ::
    session_not_found_exception() | 
    throttling_exception() | 
    invalid_parameter_exception() | 
    access_denied_exception() | 
    internal_server_error() | 
    provisioned_throughput_exceeded_exception().

-type get_face_search_errors() ::
    invalid_pagination_token_exception() | 
    throttling_exception() | 
    invalid_parameter_exception() | 
    access_denied_exception() | 
    internal_server_error() | 
    resource_not_found_exception() | 
    provisioned_throughput_exceeded_exception().

-type get_label_detection_errors() ::
    invalid_pagination_token_exception() | 
    throttling_exception() | 
    invalid_parameter_exception() | 
    access_denied_exception() | 
    internal_server_error() | 
    resource_not_found_exception() | 
    provisioned_throughput_exceeded_exception().

-type get_media_analysis_job_errors() ::
    throttling_exception() | 
    invalid_parameter_exception() | 
    access_denied_exception() | 
    internal_server_error() | 
    resource_not_found_exception() | 
    provisioned_throughput_exceeded_exception().

-type get_person_tracking_errors() ::
    invalid_pagination_token_exception() | 
    throttling_exception() | 
    invalid_parameter_exception() | 
    access_denied_exception() | 
    internal_server_error() | 
    resource_not_found_exception() | 
    provisioned_throughput_exceeded_exception().

-type get_segment_detection_errors() ::
    invalid_pagination_token_exception() | 
    throttling_exception() | 
    invalid_parameter_exception() | 
    access_denied_exception() | 
    internal_server_error() | 
    resource_not_found_exception() | 
    provisioned_throughput_exceeded_exception().

-type get_text_detection_errors() ::
    invalid_pagination_token_exception() | 
    throttling_exception() | 
    invalid_parameter_exception() | 
    access_denied_exception() | 
    internal_server_error() | 
    resource_not_found_exception() | 
    provisioned_throughput_exceeded_exception().

-type index_faces_errors() ::
    invalid_s3_object_exception() | 
    throttling_exception() | 
    invalid_image_format_exception() | 
    invalid_parameter_exception() | 
    access_denied_exception() | 
    internal_server_error() | 
    service_quota_exceeded_exception() | 
    resource_not_found_exception() | 
    provisioned_throughput_exceeded_exception() | 
    image_too_large_exception().

-type list_collections_errors() ::
    invalid_pagination_token_exception() | 
    throttling_exception() | 
    invalid_parameter_exception() | 
    access_denied_exception() | 
    internal_server_error() | 
    resource_not_found_exception() | 
    provisioned_throughput_exceeded_exception().

-type list_dataset_entries_errors() ::
    invalid_pagination_token_exception() | 
    resource_not_ready_exception() | 
    throttling_exception() | 
    invalid_parameter_exception() | 
    access_denied_exception() | 
    internal_server_error() | 
    resource_not_found_exception() | 
    provisioned_throughput_exceeded_exception() | 
    resource_in_use_exception().

-type list_dataset_labels_errors() ::
    invalid_pagination_token_exception() | 
    resource_not_ready_exception() | 
    throttling_exception() | 
    invalid_parameter_exception() | 
    access_denied_exception() | 
    internal_server_error() | 
    resource_not_found_exception() | 
    provisioned_throughput_exceeded_exception() | 
    resource_in_use_exception().

-type list_faces_errors() ::
    invalid_pagination_token_exception() | 
    throttling_exception() | 
    invalid_parameter_exception() | 
    access_denied_exception() | 
    internal_server_error() | 
    resource_not_found_exception() | 
    provisioned_throughput_exceeded_exception().

-type list_media_analysis_jobs_errors() ::
    invalid_pagination_token_exception() | 
    throttling_exception() | 
    invalid_parameter_exception() | 
    access_denied_exception() | 
    internal_server_error() | 
    provisioned_throughput_exceeded_exception().

-type list_project_policies_errors() ::
    invalid_pagination_token_exception() | 
    throttling_exception() | 
    invalid_parameter_exception() | 
    access_denied_exception() | 
    internal_server_error() | 
    resource_not_found_exception() | 
    provisioned_throughput_exceeded_exception().

-type list_stream_processors_errors() ::
    invalid_pagination_token_exception() | 
    throttling_exception() | 
    invalid_parameter_exception() | 
    access_denied_exception() | 
    internal_server_error() | 
    provisioned_throughput_exceeded_exception().

-type list_tags_for_resource_errors() ::
    throttling_exception() | 
    invalid_parameter_exception() | 
    access_denied_exception() | 
    internal_server_error() | 
    resource_not_found_exception() | 
    provisioned_throughput_exceeded_exception().

-type list_users_errors() ::
    invalid_pagination_token_exception() | 
    throttling_exception() | 
    invalid_parameter_exception() | 
    access_denied_exception() | 
    internal_server_error() | 
    resource_not_found_exception() | 
    provisioned_throughput_exceeded_exception().

-type put_project_policy_errors() ::
    resource_already_exists_exception() | 
    limit_exceeded_exception() | 
    malformed_policy_document_exception() | 
    throttling_exception() | 
    invalid_parameter_exception() | 
    access_denied_exception() | 
    internal_server_error() | 
    service_quota_exceeded_exception() | 
    resource_not_found_exception() | 
    invalid_policy_revision_id_exception() | 
    provisioned_throughput_exceeded_exception().

-type recognize_celebrities_errors() ::
    invalid_s3_object_exception() | 
    throttling_exception() | 
    invalid_image_format_exception() | 
    invalid_parameter_exception() | 
    access_denied_exception() | 
    internal_server_error() | 
    provisioned_throughput_exceeded_exception() | 
    image_too_large_exception().

-type search_faces_errors() ::
    throttling_exception() | 
    invalid_parameter_exception() | 
    access_denied_exception() | 
    internal_server_error() | 
    resource_not_found_exception() | 
    provisioned_throughput_exceeded_exception().

-type search_faces_by_image_errors() ::
    invalid_s3_object_exception() | 
    throttling_exception() | 
    invalid_image_format_exception() | 
    invalid_parameter_exception() | 
    access_denied_exception() | 
    internal_server_error() | 
    resource_not_found_exception() | 
    provisioned_throughput_exceeded_exception() | 
    image_too_large_exception().

-type search_users_errors() ::
    throttling_exception() | 
    invalid_parameter_exception() | 
    access_denied_exception() | 
    internal_server_error() | 
    resource_not_found_exception() | 
    provisioned_throughput_exceeded_exception().

-type search_users_by_image_errors() ::
    invalid_s3_object_exception() | 
    throttling_exception() | 
    invalid_image_format_exception() | 
    invalid_parameter_exception() | 
    access_denied_exception() | 
    internal_server_error() | 
    resource_not_found_exception() | 
    provisioned_throughput_exceeded_exception() | 
    image_too_large_exception().

-type start_celebrity_recognition_errors() ::
    invalid_s3_object_exception() | 
    video_too_large_exception() | 
    limit_exceeded_exception() | 
    throttling_exception() | 
    invalid_parameter_exception() | 
    access_denied_exception() | 
    internal_server_error() | 
    idempotent_parameter_mismatch_exception() | 
    provisioned_throughput_exceeded_exception().

-type start_content_moderation_errors() ::
    invalid_s3_object_exception() | 
    video_too_large_exception() | 
    limit_exceeded_exception() | 
    throttling_exception() | 
    invalid_parameter_exception() | 
    access_denied_exception() | 
    internal_server_error() | 
    idempotent_parameter_mismatch_exception() | 
    provisioned_throughput_exceeded_exception().

-type start_face_detection_errors() ::
    invalid_s3_object_exception() | 
    video_too_large_exception() | 
    limit_exceeded_exception() | 
    throttling_exception() | 
    invalid_parameter_exception() | 
    access_denied_exception() | 
    internal_server_error() | 
    idempotent_parameter_mismatch_exception() | 
    provisioned_throughput_exceeded_exception().

-type start_face_search_errors() ::
    invalid_s3_object_exception() | 
    video_too_large_exception() | 
    limit_exceeded_exception() | 
    throttling_exception() | 
    invalid_parameter_exception() | 
    access_denied_exception() | 
    internal_server_error() | 
    idempotent_parameter_mismatch_exception() | 
    resource_not_found_exception() | 
    provisioned_throughput_exceeded_exception().

-type start_label_detection_errors() ::
    invalid_s3_object_exception() | 
    video_too_large_exception() | 
    limit_exceeded_exception() | 
    throttling_exception() | 
    invalid_parameter_exception() | 
    access_denied_exception() | 
    internal_server_error() | 
    idempotent_parameter_mismatch_exception() | 
    provisioned_throughput_exceeded_exception().

-type start_media_analysis_job_errors() ::
    resource_not_ready_exception() | 
    invalid_s3_object_exception() | 
    invalid_manifest_exception() | 
    limit_exceeded_exception() | 
    throttling_exception() | 
    invalid_parameter_exception() | 
    access_denied_exception() | 
    internal_server_error() | 
    idempotent_parameter_mismatch_exception() | 
    resource_not_found_exception() | 
    provisioned_throughput_exceeded_exception().

-type start_person_tracking_errors() ::
    invalid_s3_object_exception() | 
    video_too_large_exception() | 
    limit_exceeded_exception() | 
    throttling_exception() | 
    invalid_parameter_exception() | 
    access_denied_exception() | 
    internal_server_error() | 
    idempotent_parameter_mismatch_exception() | 
    provisioned_throughput_exceeded_exception().

-type start_project_version_errors() ::
    limit_exceeded_exception() | 
    throttling_exception() | 
    invalid_parameter_exception() | 
    access_denied_exception() | 
    internal_server_error() | 
    resource_not_found_exception() | 
    provisioned_throughput_exceeded_exception() | 
    resource_in_use_exception().

-type start_segment_detection_errors() ::
    invalid_s3_object_exception() | 
    video_too_large_exception() | 
    limit_exceeded_exception() | 
    throttling_exception() | 
    invalid_parameter_exception() | 
    access_denied_exception() | 
    internal_server_error() | 
    idempotent_parameter_mismatch_exception() | 
    provisioned_throughput_exceeded_exception().

-type start_stream_processor_errors() ::
    throttling_exception() | 
    invalid_parameter_exception() | 
    access_denied_exception() | 
    internal_server_error() | 
    resource_not_found_exception() | 
    provisioned_throughput_exceeded_exception() | 
    resource_in_use_exception().

-type start_text_detection_errors() ::
    invalid_s3_object_exception() | 
    video_too_large_exception() | 
    limit_exceeded_exception() | 
    throttling_exception() | 
    invalid_parameter_exception() | 
    access_denied_exception() | 
    internal_server_error() | 
    idempotent_parameter_mismatch_exception() | 
    provisioned_throughput_exceeded_exception().

-type stop_project_version_errors() ::
    throttling_exception() | 
    invalid_parameter_exception() | 
    access_denied_exception() | 
    internal_server_error() | 
    resource_not_found_exception() | 
    provisioned_throughput_exceeded_exception() | 
    resource_in_use_exception().

-type stop_stream_processor_errors() ::
    throttling_exception() | 
    invalid_parameter_exception() | 
    access_denied_exception() | 
    internal_server_error() | 
    resource_not_found_exception() | 
    provisioned_throughput_exceeded_exception() | 
    resource_in_use_exception().

-type tag_resource_errors() ::
    throttling_exception() | 
    invalid_parameter_exception() | 
    access_denied_exception() | 
    internal_server_error() | 
    service_quota_exceeded_exception() | 
    resource_not_found_exception() | 
    provisioned_throughput_exceeded_exception().

-type untag_resource_errors() ::
    throttling_exception() | 
    invalid_parameter_exception() | 
    access_denied_exception() | 
    internal_server_error() | 
    resource_not_found_exception() | 
    provisioned_throughput_exceeded_exception().

-type update_dataset_entries_errors() ::
    limit_exceeded_exception() | 
    throttling_exception() | 
    invalid_parameter_exception() | 
    access_denied_exception() | 
    internal_server_error() | 
    resource_not_found_exception() | 
    provisioned_throughput_exceeded_exception() | 
    resource_in_use_exception().

-type update_stream_processor_errors() ::
    throttling_exception() | 
    invalid_parameter_exception() | 
    access_denied_exception() | 
    internal_server_error() | 
    resource_not_found_exception() | 
    provisioned_throughput_exceeded_exception() | 
    resource_in_use_exception().

%%====================================================================
%% API
%%====================================================================

%% @doc Associates one or more faces with an existing UserID.
%%
%% Takes an array of
%% `FaceIds'. Each `FaceId' that are present in the `FaceIds'
%% list is associated with the provided UserID. The number of FaceIds that
%% can be used as input
%% in a single request is limited to 100.
%%
%% Note that the total number of faces that can be associated with a single
%% `UserID' is also limited to 100. Once a `UserID' has 100 faces
%% associated with it, no additional faces can be added. If more API calls
%% are made after the
%% limit is reached, a `ServiceQuotaExceededException' will result.
%%
%% The `UserMatchThreshold' parameter specifies the minimum user match
%% confidence
%% required for the face to be associated with a UserID that has at least one
%% `FaceID'
%% already associated. This ensures that the `FaceIds' are associated
%% with the right
%% UserID. The value ranges from 0-100 and default value is 75.
%%
%% If successful, an array of `AssociatedFace' objects containing the
%% associated
%% `FaceIds' is returned. If a given face is already associated with the
%% given
%% `UserID', it will be ignored and will not be returned in the response.
%% If a given
%% face is already associated to a different `UserID', isn't found in
%% the collection,
%% doesnt meet the `UserMatchThreshold', or there are already 100 faces
%% associated
%% with the `UserID', it will be returned as part of an array of
%% `UnsuccessfulFaceAssociations.'
%%
%% The `UserStatus' reflects the status of an operation which updates a
%% UserID
%% representation with a list of given faces. The `UserStatus' can be:
%%
%% ACTIVE - All associations or disassociations of FaceID(s) for a UserID are
%% complete.
%%
%% CREATED - A UserID has been created, but has no FaceID(s) associated with
%% it.
%%
%% UPDATING - A UserID is being updated and there are current associations or
%% disassociations of FaceID(s) taking place.
-spec associate_faces(aws_client:aws_client(), associate_faces_request()) ->
    {ok, associate_faces_response(), tuple()} |
    {error, any()} |
    {error, associate_faces_errors(), tuple()}.
associate_faces(Client, Input)
  when is_map(Client), is_map(Input) ->
    associate_faces(Client, Input, []).

-spec associate_faces(aws_client:aws_client(), associate_faces_request(), proplists:proplist()) ->
    {ok, associate_faces_response(), tuple()} |
    {error, any()} |
    {error, associate_faces_errors(), tuple()}.
associate_faces(Client, Input, Options)
  when is_map(Client), is_map(Input), is_list(Options) ->
    request(Client, <<"AssociateFaces">>, Input, Options).

%% @doc Compares a face in the source input image with each of the 100
%% largest faces detected in the target input image.
%%
%% If the source image contains multiple faces, the service detects the
%% largest face and
%% compares it with each face detected in the target image.
%%
%% CompareFaces uses machine learning algorithms, which are probabilistic. A
%% false negative
%% is an incorrect prediction that a face in the target image has a low
%% similarity confidence
%% score when compared to the face in the source image. To reduce the
%% probability of false
%% negatives, we recommend that you compare the target image against multiple
%% source images. If
%% you plan to use `CompareFaces' to make a decision that impacts an
%% individual's
%% rights, privacy, or access to services, we recommend that you pass the
%% result to a human for
%% review and further validation before taking action.
%%
%% You pass the input and target images either as base64-encoded image bytes
%% or as
%% references to images in an Amazon S3 bucket. If you use the
%% AWS
%% CLI to call Amazon Rekognition operations, passing image bytes isn't
%% supported. The image must be formatted as a PNG or JPEG file.
%%
%% In response, the operation returns an array of face matches ordered by
%% similarity score
%% in descending order. For each face match, the response provides a bounding
%% box of the face,
%% facial landmarks, pose details (pitch, roll, and yaw), quality (brightness
%% and sharpness), and
%% confidence value (indicating the level of confidence that the bounding box
%% contains a face).
%% The response also provides a similarity score, which indicates how closely
%% the faces match.
%%
%% By default, only faces with a similarity score of greater than or equal to
%% 80% are
%% returned in the response. You can change this value by specifying the
%% `SimilarityThreshold' parameter.
%%
%% `CompareFaces' also returns an array of faces that don't match the
%% source
%% image. For each face, it returns a bounding box, confidence value,
%% landmarks, pose details,
%% and quality. The response also returns information about the face in the
%% source image,
%% including the bounding box of the face and confidence value.
%%
%% The `QualityFilter' input parameter allows you to filter out detected
%% faces
%% that dont meet a required quality bar. The quality bar is based on a
%% variety of common use
%% cases. Use `QualityFilter' to set the quality bar by specifying
%% `LOW',
%% `MEDIUM', or `HIGH'. If you do not want to filter detected faces,
%% specify `NONE'. The default value is `NONE'.
%%
%% If the image doesn't contain Exif metadata, `CompareFaces' returns
%% orientation information for the source and target images. Use these values
%% to display the
%% images with the correct image orientation.
%%
%% If no faces are detected in the source or target images,
%% `CompareFaces'
%% returns an `InvalidParameterException' error.
%%
%% This is a stateless API operation. That is, data returned by this
%% operation doesn't
%% persist.
%%
%% For an example, see Comparing Faces in Images in the Amazon Rekognition
%% Developer
%% Guide.
%%
%% This operation requires permissions to perform the
%% `rekognition:CompareFaces' action.
-spec compare_faces(aws_client:aws_client(), compare_faces_request()) ->
    {ok, compare_faces_response(), tuple()} |
    {error, any()} |
    {error, compare_faces_errors(), tuple()}.
compare_faces(Client, Input)
  when is_map(Client), is_map(Input) ->
    compare_faces(Client, Input, []).

-spec compare_faces(aws_client:aws_client(), compare_faces_request(), proplists:proplist()) ->
    {ok, compare_faces_response(), tuple()} |
    {error, any()} |
    {error, compare_faces_errors(), tuple()}.
compare_faces(Client, Input, Options)
  when is_map(Client), is_map(Input), is_list(Options) ->
    request(Client, <<"CompareFaces">>, Input, Options).

%% @doc
%% This operation applies only to Amazon Rekognition Custom Labels.
%%
%% Copies a version of an Amazon Rekognition Custom Labels model from a
%% source project to a destination project. The source and
%% destination projects can be in different AWS accounts but must be in the
%% same AWS Region.
%% You can't copy a model to another AWS service.
%%
%% To copy a model version to a different AWS account, you need to create a
%% resource-based policy known as a
%% project policy. You attach the project policy to the
%% source project by calling `PutProjectPolicy'. The project policy
%% gives permission to copy the model version from a trusting AWS account to
%% a trusted account.
%%
%% For more information creating and attaching a project policy, see
%% Attaching a project policy (SDK)
%% in the Amazon Rekognition Custom Labels Developer Guide.
%%
%% If you are copying a model version to a project in the same AWS account,
%% you don't need to create a project policy.
%%
%% Copying project versions is supported only for Custom Labels models.
%%
%% To copy a model, the destination project, source project, and source model
%% version
%% must already exist.
%%
%% Copying a model version takes a while to complete. To get the current
%% status, call `DescribeProjectVersions' and check the value of
%% `Status' in the
%% `ProjectVersionDescription' object. The copy operation has finished
%% when
%% the value of `Status' is `COPYING_COMPLETED'.
%%
%% This operation requires permissions to perform the
%% `rekognition:CopyProjectVersion' action.
-spec copy_project_version(aws_client:aws_client(), copy_project_version_request()) ->
    {ok, copy_project_version_response(), tuple()} |
    {error, any()} |
    {error, copy_project_version_errors(), tuple()}.
copy_project_version(Client, Input)
  when is_map(Client), is_map(Input) ->
    copy_project_version(Client, Input, []).

-spec copy_project_version(aws_client:aws_client(), copy_project_version_request(), proplists:proplist()) ->
    {ok, copy_project_version_response(), tuple()} |
    {error, any()} |
    {error, copy_project_version_errors(), tuple()}.
copy_project_version(Client, Input, Options)
  when is_map(Client), is_map(Input), is_list(Options) ->
    request(Client, <<"CopyProjectVersion">>, Input, Options).

%% @doc Creates a collection in an AWS Region.
%%
%% You can add faces to the collection using the
%% `IndexFaces' operation.
%%
%% For example, you might create collections, one for each of your
%% application users. A
%% user can then index faces using the `IndexFaces' operation and persist
%% results in a
%% specific collection. Then, a user can search the collection for faces in
%% the user-specific
%% container.
%%
%% When you create a collection, it is associated with the latest version of
%% the face model
%% version.
%%
%% Collection names are case-sensitive.
%%
%% This operation requires permissions to perform the
%% `rekognition:CreateCollection' action. If you want to tag your
%% collection, you
%% also require permission to perform the `rekognition:TagResource'
%% operation.
-spec create_collection(aws_client:aws_client(), create_collection_request()) ->
    {ok, create_collection_response(), tuple()} |
    {error, any()} |
    {error, create_collection_errors(), tuple()}.
create_collection(Client, Input)
  when is_map(Client), is_map(Input) ->
    create_collection(Client, Input, []).

-spec create_collection(aws_client:aws_client(), create_collection_request(), proplists:proplist()) ->
    {ok, create_collection_response(), tuple()} |
    {error, any()} |
    {error, create_collection_errors(), tuple()}.
create_collection(Client, Input, Options)
  when is_map(Client), is_map(Input), is_list(Options) ->
    request(Client, <<"CreateCollection">>, Input, Options).

%% @doc
%% This operation applies only to Amazon Rekognition Custom Labels.
%%
%% Creates a new Amazon Rekognition Custom Labels dataset. You can create a
%% dataset by using
%% an Amazon Sagemaker format manifest file or by copying an existing Amazon
%% Rekognition Custom Labels dataset.
%%
%% To create a training dataset for a project, specify `TRAIN' for the
%% value of
%% `DatasetType'. To create the test dataset for a project,
%% specify `TEST' for the value of `DatasetType'.
%%
%% The response from `CreateDataset' is the Amazon Resource Name (ARN)
%% for the dataset.
%% Creating a dataset takes a while to complete. Use `DescribeDataset' to
%% check the
%% current status. The dataset created successfully if the value of
%% `Status' is
%% `CREATE_COMPLETE'.
%%
%% To check if any non-terminal errors occurred, call
%% `ListDatasetEntries'
%% and check for the presence of `errors' lists in the JSON Lines.
%%
%% Dataset creation fails if a terminal error occurs (`Status' =
%% `CREATE_FAILED').
%% Currently, you can't access the terminal error information.
%%
%% For more information, see Creating dataset in the Amazon Rekognition
%% Custom Labels Developer Guide.
%%
%% This operation requires permissions to perform the
%% `rekognition:CreateDataset' action.
%% If you want to copy an existing dataset, you also require permission to
%% perform the `rekognition:ListDatasetEntries' action.
-spec create_dataset(aws_client:aws_client(), create_dataset_request()) ->
    {ok, create_dataset_response(), tuple()} |
    {error, any()} |
    {error, create_dataset_errors(), tuple()}.
create_dataset(Client, Input)
  when is_map(Client), is_map(Input) ->
    create_dataset(Client, Input, []).

-spec create_dataset(aws_client:aws_client(), create_dataset_request(), proplists:proplist()) ->
    {ok, create_dataset_response(), tuple()} |
    {error, any()} |
    {error, create_dataset_errors(), tuple()}.
create_dataset(Client, Input, Options)
  when is_map(Client), is_map(Input), is_list(Options) ->
    request(Client, <<"CreateDataset">>, Input, Options).

%% @doc This API operation initiates a Face Liveness session.
%%
%% It returns a `SessionId',
%% which you can use to start streaming Face Liveness video and get the
%% results for a Face
%% Liveness session.
%%
%% You can use the `OutputConfig' option in the Settings parameter to
%% provide an
%% Amazon S3 bucket location. The Amazon S3 bucket stores reference images
%% and audit images. If no Amazon S3
%% bucket is defined, raw bytes are sent instead.
%%
%% You can use `AuditImagesLimit' to limit the number of audit images
%% returned
%% when `GetFaceLivenessSessionResults' is called. This number is between
%% 0 and 4. By
%% default, it is set to 0. The limit is best effort and based on the
%% duration of the
%% selfie-video.
-spec create_face_liveness_session(aws_client:aws_client(), create_face_liveness_session_request()) ->
    {ok, create_face_liveness_session_response(), tuple()} |
    {error, any()} |
    {error, create_face_liveness_session_errors(), tuple()}.
create_face_liveness_session(Client, Input)
  when is_map(Client), is_map(Input) ->
    create_face_liveness_session(Client, Input, []).

-spec create_face_liveness_session(aws_client:aws_client(), create_face_liveness_session_request(), proplists:proplist()) ->
    {ok, create_face_liveness_session_response(), tuple()} |
    {error, any()} |
    {error, create_face_liveness_session_errors(), tuple()}.
create_face_liveness_session(Client, Input, Options)
  when is_map(Client), is_map(Input), is_list(Options) ->
    request(Client, <<"CreateFaceLivenessSession">>, Input, Options).

%% @doc Creates a new Amazon Rekognition project.
%%
%% A project is a group of resources (datasets, model
%% versions) that you use to create and manage a Amazon Rekognition Custom
%% Labels Model or custom adapter. You can
%% specify a feature to create the project with, if no feature is specified
%% then Custom Labels
%% is used by default. For adapters, you can also choose whether or not to
%% have the project
%% auto update by using the AutoUpdate argument. This operation requires
%% permissions to
%% perform the `rekognition:CreateProject' action.
-spec create_project(aws_client:aws_client(), create_project_request()) ->
    {ok, create_project_response(), tuple()} |
    {error, any()} |
    {error, create_project_errors(), tuple()}.
create_project(Client, Input)
  when is_map(Client), is_map(Input) ->
    create_project(Client, Input, []).

-spec create_project(aws_client:aws_client(), create_project_request(), proplists:proplist()) ->
    {ok, create_project_response(), tuple()} |
    {error, any()} |
    {error, create_project_errors(), tuple()}.
create_project(Client, Input, Options)
  when is_map(Client), is_map(Input), is_list(Options) ->
    request(Client, <<"CreateProject">>, Input, Options).

%% @doc Creates a new version of Amazon Rekognition project (like a Custom
%% Labels model or a custom adapter)
%% and begins training.
%%
%% Models and adapters are managed as part of a Rekognition project. The
%% response from `CreateProjectVersion' is an Amazon Resource Name (ARN)
%% for the
%% project version.
%%
%% The FeatureConfig operation argument allows you to configure specific
%% model or adapter
%% settings. You can provide a description to the project version by using
%% the
%% VersionDescription argment. Training can take a while to complete. You can
%% get the current
%% status by calling `DescribeProjectVersions'. Training completed
%% successfully if the value of the `Status' field is
%% `TRAINING_COMPLETED'. Once training has successfully completed, call
%% `DescribeProjectVersions' to get the training results and evaluate the
%% model.
%%
%% This operation requires permissions to perform the
%% `rekognition:CreateProjectVersion' action.
%%
%% The following applies only to projects with Amazon Rekognition Custom
%% Labels as the chosen
%% feature:
%%
%% You can train a model in a project that doesn't have associated
%% datasets by specifying manifest files in the
%% `TrainingData' and `TestingData' fields.
%%
%% If you open the console after training a model with manifest files, Amazon
%% Rekognition Custom Labels creates
%% the datasets for you using the most recent manifest files. You can no
%% longer train
%% a model version for the project by specifying manifest files.
%%
%% Instead of training with a project without associated datasets,
%% we recommend that you use the manifest
%% files to create training and test datasets for the project.
-spec create_project_version(aws_client:aws_client(), create_project_version_request()) ->
    {ok, create_project_version_response(), tuple()} |
    {error, any()} |
    {error, create_project_version_errors(), tuple()}.
create_project_version(Client, Input)
  when is_map(Client), is_map(Input) ->
    create_project_version(Client, Input, []).

-spec create_project_version(aws_client:aws_client(), create_project_version_request(), proplists:proplist()) ->
    {ok, create_project_version_response(), tuple()} |
    {error, any()} |
    {error, create_project_version_errors(), tuple()}.
create_project_version(Client, Input, Options)
  when is_map(Client), is_map(Input), is_list(Options) ->
    request(Client, <<"CreateProjectVersion">>, Input, Options).

%% @doc Creates an Amazon Rekognition stream processor that you can use to
%% detect and recognize faces or to detect labels in a streaming video.
%%
%% Amazon Rekognition Video is a consumer of live video from Amazon Kinesis
%% Video Streams. There are two different settings for stream processors in
%% Amazon Rekognition: detecting faces and detecting labels.
%%
%% If you are creating a stream processor for detecting faces, you provide as
%% input a Kinesis video stream
%% (`Input') and a Kinesis data stream (`Output') stream for
%% receiving
%% the output. You must use the `FaceSearch' option in
%% `Settings', specifying the collection that contains the faces you
%% want to recognize. After you have finished analyzing a streaming video,
%% use
%% `StopStreamProcessor' to stop processing.
%%
%% If you are creating a stream processor to detect labels, you provide as
%% input a Kinesis video stream
%% (`Input'), Amazon S3 bucket information (`Output'), and an
%% Amazon SNS topic ARN (`NotificationChannel'). You can also provide a
%% KMS
%% key ID to encrypt the data sent to your Amazon S3 bucket. You specify what
%% you want
%% to detect by using the `ConnectedHome' option in settings, and
%% selecting one of the following: `PERSON', `PET',
%% `PACKAGE', `ALL' You can also specify where in the
%% frame you want Amazon Rekognition to monitor with `RegionsOfInterest'.
%% When
%% you run the `StartStreamProcessor' operation on a label
%% detection stream processor, you input start and stop information to
%% determine
%% the length of the processing time.
%%
%% Use `Name' to assign an identifier for the stream processor. You use
%% `Name'
%% to manage the stream processor. For example, you can start processing the
%% source video by calling `StartStreamProcessor' with
%% the `Name' field.
%%
%% This operation requires permissions to perform the
%% `rekognition:CreateStreamProcessor' action. If you want to tag your
%% stream processor, you also require permission to perform the
%% `rekognition:TagResource' operation.
-spec create_stream_processor(aws_client:aws_client(), create_stream_processor_request()) ->
    {ok, create_stream_processor_response(), tuple()} |
    {error, any()} |
    {error, create_stream_processor_errors(), tuple()}.
create_stream_processor(Client, Input)
  when is_map(Client), is_map(Input) ->
    create_stream_processor(Client, Input, []).

-spec create_stream_processor(aws_client:aws_client(), create_stream_processor_request(), proplists:proplist()) ->
    {ok, create_stream_processor_response(), tuple()} |
    {error, any()} |
    {error, create_stream_processor_errors(), tuple()}.
create_stream_processor(Client, Input, Options)
  when is_map(Client), is_map(Input), is_list(Options) ->
    request(Client, <<"CreateStreamProcessor">>, Input, Options).

%% @doc Creates a new User within a collection specified by
%% `CollectionId'.
%%
%% Takes
%% `UserId' as a parameter, which is a user provided ID which should be
%% unique
%% within the collection. The provided `UserId' will alias the system
%% generated UUID
%% to make the `UserId' more user friendly.
%%
%% Uses a `ClientToken', an idempotency token that ensures a call to
%% `CreateUser' completes only once. If the value is not supplied, the
%% AWS SDK
%% generates an idempotency token for the requests. This prevents retries
%% after a network error
%% results from making multiple `CreateUser' calls.
-spec create_user(aws_client:aws_client(), create_user_request()) ->
    {ok, create_user_response(), tuple()} |
    {error, any()} |
    {error, create_user_errors(), tuple()}.
create_user(Client, Input)
  when is_map(Client), is_map(Input) ->
    create_user(Client, Input, []).

-spec create_user(aws_client:aws_client(), create_user_request(), proplists:proplist()) ->
    {ok, create_user_response(), tuple()} |
    {error, any()} |
    {error, create_user_errors(), tuple()}.
create_user(Client, Input, Options)
  when is_map(Client), is_map(Input), is_list(Options) ->
    request(Client, <<"CreateUser">>, Input, Options).

%% @doc Deletes the specified collection.
%%
%% Note that this operation removes all faces in the
%% collection. For an example, see Deleting a
%% collection:
%% https://docs.aws.amazon.com/rekognition/latest/dg/delete-collection-procedure.html.
%%
%% This operation requires permissions to perform the
%% `rekognition:DeleteCollection' action.
-spec delete_collection(aws_client:aws_client(), delete_collection_request()) ->
    {ok, delete_collection_response(), tuple()} |
    {error, any()} |
    {error, delete_collection_errors(), tuple()}.
delete_collection(Client, Input)
  when is_map(Client), is_map(Input) ->
    delete_collection(Client, Input, []).

-spec delete_collection(aws_client:aws_client(), delete_collection_request(), proplists:proplist()) ->
    {ok, delete_collection_response(), tuple()} |
    {error, any()} |
    {error, delete_collection_errors(), tuple()}.
delete_collection(Client, Input, Options)
  when is_map(Client), is_map(Input), is_list(Options) ->
    request(Client, <<"DeleteCollection">>, Input, Options).

%% @doc
%% This operation applies only to Amazon Rekognition Custom Labels.
%%
%% Deletes an existing Amazon Rekognition Custom Labels dataset.
%% Deleting a dataset might take while. Use `DescribeDataset' to check
%% the
%% current status. The dataset is still deleting if the value of `Status'
%% is
%% `DELETE_IN_PROGRESS'. If you try to access the dataset after it is
%% deleted, you get
%% a `ResourceNotFoundException' exception.
%%
%% You can't delete a dataset while it is creating (`Status' =
%% `CREATE_IN_PROGRESS')
%% or if the dataset is updating (`Status' = `UPDATE_IN_PROGRESS').
%%
%% This operation requires permissions to perform the
%% `rekognition:DeleteDataset' action.
-spec delete_dataset(aws_client:aws_client(), delete_dataset_request()) ->
    {ok, delete_dataset_response(), tuple()} |
    {error, any()} |
    {error, delete_dataset_errors(), tuple()}.
delete_dataset(Client, Input)
  when is_map(Client), is_map(Input) ->
    delete_dataset(Client, Input, []).

-spec delete_dataset(aws_client:aws_client(), delete_dataset_request(), proplists:proplist()) ->
    {ok, delete_dataset_response(), tuple()} |
    {error, any()} |
    {error, delete_dataset_errors(), tuple()}.
delete_dataset(Client, Input, Options)
  when is_map(Client), is_map(Input), is_list(Options) ->
    request(Client, <<"DeleteDataset">>, Input, Options).

%% @doc Deletes faces from a collection.
%%
%% You specify a collection ID and an array of face IDs
%% to remove from the collection.
%%
%% This operation requires permissions to perform the
%% `rekognition:DeleteFaces'
%% action.
-spec delete_faces(aws_client:aws_client(), delete_faces_request()) ->
    {ok, delete_faces_response(), tuple()} |
    {error, any()} |
    {error, delete_faces_errors(), tuple()}.
delete_faces(Client, Input)
  when is_map(Client), is_map(Input) ->
    delete_faces(Client, Input, []).

-spec delete_faces(aws_client:aws_client(), delete_faces_request(), proplists:proplist()) ->
    {ok, delete_faces_response(), tuple()} |
    {error, any()} |
    {error, delete_faces_errors(), tuple()}.
delete_faces(Client, Input, Options)
  when is_map(Client), is_map(Input), is_list(Options) ->
    request(Client, <<"DeleteFaces">>, Input, Options).

%% @doc Deletes a Amazon Rekognition project.
%%
%% To delete a project you must first delete all models or
%% adapters associated with the project. To delete a model or adapter, see
%% `DeleteProjectVersion'.
%%
%% `DeleteProject' is an asynchronous operation. To check if the project
%% is
%% deleted, call `DescribeProjects'. The project is deleted when the
%% project
%% no longer appears in the response. Be aware that deleting a given project
%% will also delete
%% any `ProjectPolicies' associated with that project.
%%
%% This operation requires permissions to perform the
%% `rekognition:DeleteProject' action.
-spec delete_project(aws_client:aws_client(), delete_project_request()) ->
    {ok, delete_project_response(), tuple()} |
    {error, any()} |
    {error, delete_project_errors(), tuple()}.
delete_project(Client, Input)
  when is_map(Client), is_map(Input) ->
    delete_project(Client, Input, []).

-spec delete_project(aws_client:aws_client(), delete_project_request(), proplists:proplist()) ->
    {ok, delete_project_response(), tuple()} |
    {error, any()} |
    {error, delete_project_errors(), tuple()}.
delete_project(Client, Input, Options)
  when is_map(Client), is_map(Input), is_list(Options) ->
    request(Client, <<"DeleteProject">>, Input, Options).

%% @doc
%% This operation applies only to Amazon Rekognition Custom Labels.
%%
%% Deletes an existing project policy.
%%
%% To get a list of project policies attached to a project, call
%% `ListProjectPolicies'. To attach a project policy to a project, call
%% `PutProjectPolicy'.
%%
%% This operation requires permissions to perform the
%% `rekognition:DeleteProjectPolicy' action.
-spec delete_project_policy(aws_client:aws_client(), delete_project_policy_request()) ->
    {ok, delete_project_policy_response(), tuple()} |
    {error, any()} |
    {error, delete_project_policy_errors(), tuple()}.
delete_project_policy(Client, Input)
  when is_map(Client), is_map(Input) ->
    delete_project_policy(Client, Input, []).

-spec delete_project_policy(aws_client:aws_client(), delete_project_policy_request(), proplists:proplist()) ->
    {ok, delete_project_policy_response(), tuple()} |
    {error, any()} |
    {error, delete_project_policy_errors(), tuple()}.
delete_project_policy(Client, Input, Options)
  when is_map(Client), is_map(Input), is_list(Options) ->
    request(Client, <<"DeleteProjectPolicy">>, Input, Options).

%% @doc Deletes a Rekognition project model or project version, like a Amazon
%% Rekognition Custom Labels model or a custom
%% adapter.
%%
%% You can't delete a project version if it is running or if it is
%% training. To check
%% the status of a project version, use the Status field returned from
%% `DescribeProjectVersions'. To stop a project version call
%% `StopProjectVersion'. If the project version is training, wait until
%% it
%% finishes.
%%
%% This operation requires permissions to perform the
%% `rekognition:DeleteProjectVersion' action.
-spec delete_project_version(aws_client:aws_client(), delete_project_version_request()) ->
    {ok, delete_project_version_response(), tuple()} |
    {error, any()} |
    {error, delete_project_version_errors(), tuple()}.
delete_project_version(Client, Input)
  when is_map(Client), is_map(Input) ->
    delete_project_version(Client, Input, []).

-spec delete_project_version(aws_client:aws_client(), delete_project_version_request(), proplists:proplist()) ->
    {ok, delete_project_version_response(), tuple()} |
    {error, any()} |
    {error, delete_project_version_errors(), tuple()}.
delete_project_version(Client, Input, Options)
  when is_map(Client), is_map(Input), is_list(Options) ->
    request(Client, <<"DeleteProjectVersion">>, Input, Options).

%% @doc Deletes the stream processor identified by `Name'.
%%
%% You assign the value for `Name' when you create the stream processor
%% with
%% `CreateStreamProcessor'. You might not be able to use the same name
%% for a stream processor for a few seconds after calling
%% `DeleteStreamProcessor'.
-spec delete_stream_processor(aws_client:aws_client(), delete_stream_processor_request()) ->
    {ok, delete_stream_processor_response(), tuple()} |
    {error, any()} |
    {error, delete_stream_processor_errors(), tuple()}.
delete_stream_processor(Client, Input)
  when is_map(Client), is_map(Input) ->
    delete_stream_processor(Client, Input, []).

-spec delete_stream_processor(aws_client:aws_client(), delete_stream_processor_request(), proplists:proplist()) ->
    {ok, delete_stream_processor_response(), tuple()} |
    {error, any()} |
    {error, delete_stream_processor_errors(), tuple()}.
delete_stream_processor(Client, Input, Options)
  when is_map(Client), is_map(Input), is_list(Options) ->
    request(Client, <<"DeleteStreamProcessor">>, Input, Options).

%% @doc Deletes the specified UserID within the collection.
%%
%% Faces that are associated with the
%% UserID are disassociated from the UserID before deleting the specified
%% UserID. If the
%% specified `Collection' or `UserID' is already deleted or not
%% found, a
%% `ResourceNotFoundException' will be thrown. If the action is
%% successful with a
%% 200 response, an empty HTTP body is returned.
-spec delete_user(aws_client:aws_client(), delete_user_request()) ->
    {ok, delete_user_response(), tuple()} |
    {error, any()} |
    {error, delete_user_errors(), tuple()}.
delete_user(Client, Input)
  when is_map(Client), is_map(Input) ->
    delete_user(Client, Input, []).

-spec delete_user(aws_client:aws_client(), delete_user_request(), proplists:proplist()) ->
    {ok, delete_user_response(), tuple()} |
    {error, any()} |
    {error, delete_user_errors(), tuple()}.
delete_user(Client, Input, Options)
  when is_map(Client), is_map(Input), is_list(Options) ->
    request(Client, <<"DeleteUser">>, Input, Options).

%% @doc Describes the specified collection.
%%
%% You can use `DescribeCollection' to get
%% information, such as the number of faces indexed into a collection and the
%% version of the
%% model used by the collection for face detection.
%%
%% For more information, see Describing a Collection in the
%% Amazon Rekognition Developer Guide.
-spec describe_collection(aws_client:aws_client(), describe_collection_request()) ->
    {ok, describe_collection_response(), tuple()} |
    {error, any()} |
    {error, describe_collection_errors(), tuple()}.
describe_collection(Client, Input)
  when is_map(Client), is_map(Input) ->
    describe_collection(Client, Input, []).

-spec describe_collection(aws_client:aws_client(), describe_collection_request(), proplists:proplist()) ->
    {ok, describe_collection_response(), tuple()} |
    {error, any()} |
    {error, describe_collection_errors(), tuple()}.
describe_collection(Client, Input, Options)
  when is_map(Client), is_map(Input), is_list(Options) ->
    request(Client, <<"DescribeCollection">>, Input, Options).

%% @doc
%% This operation applies only to Amazon Rekognition Custom Labels.
%%
%% Describes an Amazon Rekognition Custom Labels dataset. You can get
%% information such as the current status of a dataset and
%% statistics about the images and labels in a dataset.
%%
%% This operation requires permissions to perform the
%% `rekognition:DescribeDataset' action.
-spec describe_dataset(aws_client:aws_client(), describe_dataset_request()) ->
    {ok, describe_dataset_response(), tuple()} |
    {error, any()} |
    {error, describe_dataset_errors(), tuple()}.
describe_dataset(Client, Input)
  when is_map(Client), is_map(Input) ->
    describe_dataset(Client, Input, []).

-spec describe_dataset(aws_client:aws_client(), describe_dataset_request(), proplists:proplist()) ->
    {ok, describe_dataset_response(), tuple()} |
    {error, any()} |
    {error, describe_dataset_errors(), tuple()}.
describe_dataset(Client, Input, Options)
  when is_map(Client), is_map(Input), is_list(Options) ->
    request(Client, <<"DescribeDataset">>, Input, Options).

%% @doc Lists and describes the versions of an Amazon Rekognition project.
%%
%% You can specify up to 10 model or
%% adapter versions in `ProjectVersionArns'. If you don't specify a
%% value,
%% descriptions for all model/adapter versions in the project are returned.
%%
%% This operation requires permissions to perform the
%% `rekognition:DescribeProjectVersions'
%% action.
-spec describe_project_versions(aws_client:aws_client(), describe_project_versions_request()) ->
    {ok, describe_project_versions_response(), tuple()} |
    {error, any()} |
    {error, describe_project_versions_errors(), tuple()}.
describe_project_versions(Client, Input)
  when is_map(Client), is_map(Input) ->
    describe_project_versions(Client, Input, []).

-spec describe_project_versions(aws_client:aws_client(), describe_project_versions_request(), proplists:proplist()) ->
    {ok, describe_project_versions_response(), tuple()} |
    {error, any()} |
    {error, describe_project_versions_errors(), tuple()}.
describe_project_versions(Client, Input, Options)
  when is_map(Client), is_map(Input), is_list(Options) ->
    request(Client, <<"DescribeProjectVersions">>, Input, Options).

%% @doc Gets information about your Rekognition projects.
%%
%% This operation requires permissions to perform the
%% `rekognition:DescribeProjects' action.
-spec describe_projects(aws_client:aws_client(), describe_projects_request()) ->
    {ok, describe_projects_response(), tuple()} |
    {error, any()} |
    {error, describe_projects_errors(), tuple()}.
describe_projects(Client, Input)
  when is_map(Client), is_map(Input) ->
    describe_projects(Client, Input, []).

-spec describe_projects(aws_client:aws_client(), describe_projects_request(), proplists:proplist()) ->
    {ok, describe_projects_response(), tuple()} |
    {error, any()} |
    {error, describe_projects_errors(), tuple()}.
describe_projects(Client, Input, Options)
  when is_map(Client), is_map(Input), is_list(Options) ->
    request(Client, <<"DescribeProjects">>, Input, Options).

%% @doc Provides information about a stream processor created by
%% `CreateStreamProcessor'.
%%
%% You can get information about the input and output streams, the input
%% parameters for the face recognition being performed,
%% and the current status of the stream processor.
-spec describe_stream_processor(aws_client:aws_client(), describe_stream_processor_request()) ->
    {ok, describe_stream_processor_response(), tuple()} |
    {error, any()} |
    {error, describe_stream_processor_errors(), tuple()}.
describe_stream_processor(Client, Input)
  when is_map(Client), is_map(Input) ->
    describe_stream_processor(Client, Input, []).

-spec describe_stream_processor(aws_client:aws_client(), describe_stream_processor_request(), proplists:proplist()) ->
    {ok, describe_stream_processor_response(), tuple()} |
    {error, any()} |
    {error, describe_stream_processor_errors(), tuple()}.
describe_stream_processor(Client, Input, Options)
  when is_map(Client), is_map(Input), is_list(Options) ->
    request(Client, <<"DescribeStreamProcessor">>, Input, Options).

%% @doc
%% This operation applies only to Amazon Rekognition Custom Labels.
%%
%% Detects custom labels in a supplied image by using an Amazon Rekognition
%% Custom Labels model.
%%
%% You specify which version of a model version to use by using the
%% `ProjectVersionArn' input
%% parameter.
%%
%% You pass the input image as base64-encoded image bytes or as a reference
%% to an image in
%% an Amazon S3 bucket. If you use the AWS CLI to call Amazon Rekognition
%% operations, passing
%% image bytes is not supported. The image must be either a PNG or JPEG
%% formatted file.
%%
%% For each object that the model version detects on an image, the API
%% returns a
%% (`CustomLabel') object in an array (`CustomLabels').
%% Each `CustomLabel' object provides the label name (`Name'), the
%% level
%% of confidence that the image contains the object (`Confidence'), and
%% object location information, if it exists, for the label on the image
%% (`Geometry').
%%
%% To filter labels that are returned, specify a value for
%% `MinConfidence'.
%% `DetectCustomLabelsLabels' only returns labels with a confidence
%% that's higher than
%% the specified value.
%%
%% The value of `MinConfidence' maps to the assumed threshold values
%% created during training. For more information, see Assumed threshold
%% in the Amazon Rekognition Custom Labels Developer Guide.
%% Amazon Rekognition Custom Labels metrics expresses an assumed threshold as
%% a floating point value between 0-1. The range of
%% `MinConfidence' normalizes the threshold value to a percentage value
%% (0-100). Confidence
%% responses from `DetectCustomLabels' are also returned as a percentage.
%% You can use `MinConfidence' to change the precision and recall or your
%% model.
%% For more information, see
%% Analyzing an image in the Amazon Rekognition Custom Labels Developer
%% Guide.
%%
%% If you don't specify a value for `MinConfidence',
%% `DetectCustomLabels'
%% returns labels based on the assumed threshold of each label.
%%
%% This is a stateless API operation. That is, the operation does not persist
%% any
%% data.
%%
%% This operation requires permissions to perform the
%% `rekognition:DetectCustomLabels' action.
%%
%% For more information, see
%% Analyzing an image in the Amazon Rekognition Custom Labels Developer
%% Guide.
-spec detect_custom_labels(aws_client:aws_client(), detect_custom_labels_request()) ->
    {ok, detect_custom_labels_response(), tuple()} |
    {error, any()} |
    {error, detect_custom_labels_errors(), tuple()}.
detect_custom_labels(Client, Input)
  when is_map(Client), is_map(Input) ->
    detect_custom_labels(Client, Input, []).

-spec detect_custom_labels(aws_client:aws_client(), detect_custom_labels_request(), proplists:proplist()) ->
    {ok, detect_custom_labels_response(), tuple()} |
    {error, any()} |
    {error, detect_custom_labels_errors(), tuple()}.
detect_custom_labels(Client, Input, Options)
  when is_map(Client), is_map(Input), is_list(Options) ->
    request(Client, <<"DetectCustomLabels">>, Input, Options).

%% @doc Detects faces within an image that is provided as input.
%%
%% `DetectFaces' detects the 100 largest faces in the image. For each
%% face
%% detected, the operation returns face details. These details include a
%% bounding box of the
%% face, a confidence value (that the bounding box contains a face), and a
%% fixed set of
%% attributes such as facial landmarks (for example, coordinates of eye and
%% mouth), pose,
%% presence of facial occlusion, and so on.
%%
%% The face-detection algorithm is most effective on frontal faces. For
%% non-frontal or
%% obscured faces, the algorithm might not detect the faces or might detect
%% faces with lower
%% confidence.
%%
%% You pass the input image either as base64-encoded image bytes or as a
%% reference to an
%% image in an Amazon S3 bucket. If you use the AWS CLI to call Amazon
%% Rekognition operations,
%% passing image bytes is not supported. The image must be either a PNG or
%% JPEG formatted file.
%%
%% This is a stateless API operation. That is, the operation does not persist
%% any
%% data.
%%
%% This operation requires permissions to perform the
%% `rekognition:DetectFaces'
%% action.
-spec detect_faces(aws_client:aws_client(), detect_faces_request()) ->
    {ok, detect_faces_response(), tuple()} |
    {error, any()} |
    {error, detect_faces_errors(), tuple()}.
detect_faces(Client, Input)
  when is_map(Client), is_map(Input) ->
    detect_faces(Client, Input, []).

-spec detect_faces(aws_client:aws_client(), detect_faces_request(), proplists:proplist()) ->
    {ok, detect_faces_response(), tuple()} |
    {error, any()} |
    {error, detect_faces_errors(), tuple()}.
detect_faces(Client, Input, Options)
  when is_map(Client), is_map(Input), is_list(Options) ->
    request(Client, <<"DetectFaces">>, Input, Options).

%% @doc Detects instances of real-world entities within an image (JPEG or
%% PNG) provided as
%% input.
%%
%% This includes objects like flower, tree, and table; events like wedding,
%% graduation,
%% and birthday party; and concepts like landscape, evening, and nature.
%%
%% For an example, see Analyzing images stored in an Amazon S3 bucket in the
%% Amazon Rekognition Developer Guide.
%%
%% You pass the input image as base64-encoded image bytes or as a reference
%% to an image in
%% an Amazon S3 bucket. If you use the
%% AWS
%% CLI to call Amazon Rekognition operations, passing image bytes is not
%% supported. The image must be either a PNG or JPEG formatted file.
%%
%% Optional Parameters
%%
%% You can specify one or both of the `GENERAL_LABELS' and
%% `IMAGE_PROPERTIES' feature types when calling the DetectLabels API.
%% Including
%% `GENERAL_LABELS' will ensure the response includes the labels detected
%% in the
%% input image, while including `IMAGE_PROPERTIES 'will ensure the
%% response includes
%% information about the image quality and color.
%%
%% When using `GENERAL_LABELS' and/or `IMAGE_PROPERTIES' you can
%% provide filtering criteria to the Settings parameter. You can filter with
%% sets of individual
%% labels or with label categories. You can specify inclusive filters,
%% exclusive filters, or a
%% combination of inclusive and exclusive filters. For more information on
%% filtering see Detecting
%% Labels in an Image:
%% https://docs.aws.amazon.com/rekognition/latest/dg/labels-detect-labels-image.html.
%%
%% When getting labels, you can specify `MinConfidence' to control the
%% confidence threshold for the labels returned. The default is 55%. You can
%% also add the
%% `MaxLabels' parameter to limit the number of labels returned. The
%% default and
%% upper limit is 1000 labels. These arguments are only valid when supplying
%% GENERAL_LABELS as a
%% feature type.
%%
%% Response Elements
%%
%% For each object, scene, and concept the API returns one or more labels.
%% The API
%% returns the following types of information about labels:
%%
%% Name - The name of the detected label.
%%
%% Confidence - The level of confidence in the label assigned to a detected
%% object.
%%
%% Parents - The ancestor labels for a detected label. DetectLabels returns a
%% hierarchical taxonomy of detected labels. For example, a detected car
%% might be assigned
%% the label car. The label car has two parent labels: Vehicle (its parent)
%% and
%% Transportation (its grandparent). The response includes the all ancestors
%% for a label,
%% where every ancestor is a unique label. In the previous example, Car,
%% Vehicle, and
%% Transportation are returned as unique labels in the response.
%%
%% Aliases - Possible Aliases for the label.
%%
%% Categories - The label categories that the detected label belongs to.
%%
%% BoundingBox  Bounding boxes are described for all instances of detected
%% common
%% object labels, returned in an array of Instance objects. An Instance
%% object contains a
%% BoundingBox object, describing the location of the label on the input
%% image. It also
%% includes the confidence for the accuracy of the detected bounding box.
%%
%% The API returns the following information regarding the image, as part of
%% the
%% ImageProperties structure:
%%
%% Quality - Information about the Sharpness, Brightness, and Contrast of the
%% input
%% image, scored between 0 to 100. Image quality is returned for the entire
%% image, as well as
%% the background and the foreground.
%%
%% Dominant Color - An array of the dominant colors in the image.
%%
%% Foreground - Information about the sharpness, brightness, and dominant
%% colors of the
%% input images foreground.
%%
%% Background - Information about the sharpness, brightness, and dominant
%% colors of the
%% input images background.
%%
%% The list of returned labels will include at least one label for every
%% detected object,
%% along with information about that label. In the following example, suppose
%% the input image has
%% a lighthouse, the sea, and a rock. The response includes all three labels,
%% one for each
%% object, as well as the confidence in the label:
%%
%% `{Name: lighthouse, Confidence: 98.4629}'
%%
%% `{Name: rock,Confidence: 79.2097}'
%%
%% ` {Name: sea,Confidence: 75.061}'
%%
%% The list of labels can include multiple labels for the same object. For
%% example, if the
%% input image shows a flower (for example, a tulip), the operation might
%% return the following
%% three labels.
%%
%% `{Name: flower,Confidence: 99.0562}'
%%
%% `{Name: plant,Confidence: 99.0562}'
%%
%% `{Name: tulip,Confidence: 99.0562}'
%%
%% In this example, the detection algorithm more precisely identifies the
%% flower as a
%% tulip.
%%
%% If the object detected is a person, the operation doesn't provide the
%% same facial
%% details that the `DetectFaces' operation provides.
%%
%% This is a stateless API operation that doesn't return any data.
%%
%% This operation requires permissions to perform the
%% `rekognition:DetectLabels' action.
-spec detect_labels(aws_client:aws_client(), detect_labels_request()) ->
    {ok, detect_labels_response(), tuple()} |
    {error, any()} |
    {error, detect_labels_errors(), tuple()}.
detect_labels(Client, Input)
  when is_map(Client), is_map(Input) ->
    detect_labels(Client, Input, []).

-spec detect_labels(aws_client:aws_client(), detect_labels_request(), proplists:proplist()) ->
    {ok, detect_labels_response(), tuple()} |
    {error, any()} |
    {error, detect_labels_errors(), tuple()}.
detect_labels(Client, Input, Options)
  when is_map(Client), is_map(Input), is_list(Options) ->
    request(Client, <<"DetectLabels">>, Input, Options).

%% @doc Detects unsafe content in a specified JPEG or PNG format image.
%%
%% Use
%% `DetectModerationLabels' to moderate images depending on your
%% requirements. For
%% example, you might want to filter images that contain nudity, but not
%% images containing
%% suggestive content.
%%
%% To filter images, use the labels returned by `DetectModerationLabels'
%% to
%% determine which types of content are appropriate.
%%
%% For information about moderation labels, see Detecting Unsafe Content in
%% the
%% Amazon Rekognition Developer Guide.
%%
%% You pass the input image either as base64-encoded image bytes or as a
%% reference to an
%% image in an Amazon S3 bucket. If you use the
%% AWS
%% CLI to call Amazon Rekognition operations, passing image bytes is not
%% supported. The image must be either a PNG or JPEG formatted file.
%%
%% You can specify an adapter to use when retrieving label predictions by
%% providing a
%% `ProjectVersionArn' to the `ProjectVersion' argument.
-spec detect_moderation_labels(aws_client:aws_client(), detect_moderation_labels_request()) ->
    {ok, detect_moderation_labels_response(), tuple()} |
    {error, any()} |
    {error, detect_moderation_labels_errors(), tuple()}.
detect_moderation_labels(Client, Input)
  when is_map(Client), is_map(Input) ->
    detect_moderation_labels(Client, Input, []).

-spec detect_moderation_labels(aws_client:aws_client(), detect_moderation_labels_request(), proplists:proplist()) ->
    {ok, detect_moderation_labels_response(), tuple()} |
    {error, any()} |
    {error, detect_moderation_labels_errors(), tuple()}.
detect_moderation_labels(Client, Input, Options)
  when is_map(Client), is_map(Input), is_list(Options) ->
    request(Client, <<"DetectModerationLabels">>, Input, Options).

%% @doc Detects Personal Protective Equipment (PPE) worn by people detected
%% in an image.
%%
%% Amazon Rekognition can detect the
%% following types of PPE.
%%
%% Face cover
%%
%% Hand cover
%%
%% Head cover
%%
%% You pass the input image as base64-encoded image bytes or as a reference
%% to an image in an Amazon S3 bucket.
%% The image must be either a PNG or JPG formatted file.
%%
%% `DetectProtectiveEquipment' detects PPE worn by up to 15 persons
%% detected in an image.
%%
%% For each person detected in the image the API returns an array of body
%% parts (face, head, left-hand, right-hand).
%% For each body part, an array of detected items of PPE is returned,
%% including an indicator of whether or not the PPE
%% covers the body part. The API returns the confidence it has in each
%% detection
%% (person, PPE, body part and body part coverage). It also returns a
%% bounding box (`BoundingBox') for each detected
%% person and each detected item of PPE.
%%
%% You can optionally request a summary of detected PPE items with the
%% `SummarizationAttributes' input parameter.
%% The summary provides the following information.
%%
%% The persons detected as wearing all of the types of PPE that you specify.
%%
%% The persons detected as not wearing all of the types PPE that you specify.
%%
%% The persons detected where PPE adornment could not be determined.
%%
%% This is a stateless API operation. That is, the operation does not persist
%% any data.
%%
%% This operation requires permissions to perform the
%% `rekognition:DetectProtectiveEquipment' action.
-spec detect_protective_equipment(aws_client:aws_client(), detect_protective_equipment_request()) ->
    {ok, detect_protective_equipment_response(), tuple()} |
    {error, any()} |
    {error, detect_protective_equipment_errors(), tuple()}.
detect_protective_equipment(Client, Input)
  when is_map(Client), is_map(Input) ->
    detect_protective_equipment(Client, Input, []).

-spec detect_protective_equipment(aws_client:aws_client(), detect_protective_equipment_request(), proplists:proplist()) ->
    {ok, detect_protective_equipment_response(), tuple()} |
    {error, any()} |
    {error, detect_protective_equipment_errors(), tuple()}.
detect_protective_equipment(Client, Input, Options)
  when is_map(Client), is_map(Input), is_list(Options) ->
    request(Client, <<"DetectProtectiveEquipment">>, Input, Options).

%% @doc Detects text in the input image and converts it into machine-readable
%% text.
%%
%% Pass the input image as base64-encoded image bytes or as a reference to an
%% image in an
%% Amazon S3 bucket. If you use the AWS CLI to call Amazon Rekognition
%% operations, you must pass it as a
%% reference to an image in an Amazon S3 bucket. For the AWS CLI, passing
%% image bytes is not
%% supported. The image must be either a .png or .jpeg formatted file.
%%
%% The `DetectText' operation returns text in an array of
%% `TextDetection' elements, `TextDetections'. Each
%% `TextDetection' element provides information about a single word or
%% line of text
%% that was detected in the image.
%%
%% A word is one or more script characters that are not separated by spaces.
%% `DetectText' can detect up to 100 words in an image.
%%
%% A line is a string of equally spaced words. A line isn't necessarily a
%% complete
%% sentence. For example, a driver's license number is detected as a
%% line. A line ends when there
%% is no aligned text after it. Also, a line ends when there is a large gap
%% between words,
%% relative to the length of the words. This means, depending on the gap
%% between words, Amazon Rekognition
%% may detect multiple lines in text aligned in the same direction. Periods
%% don't represent the
%% end of a line. If a sentence spans multiple lines, the `DetectText'
%% operation
%% returns multiple lines.
%%
%% To determine whether a `TextDetection' element is a line of text or a
%% word,
%% use the `TextDetection' object `Type' field.
%%
%% To be detected, text must be within +/- 90 degrees orientation of the
%% horizontal
%% axis.
%%
%% For more information, see Detecting text in the Amazon Rekognition
%% Developer
%% Guide.
-spec detect_text(aws_client:aws_client(), detect_text_request()) ->
    {ok, detect_text_response(), tuple()} |
    {error, any()} |
    {error, detect_text_errors(), tuple()}.
detect_text(Client, Input)
  when is_map(Client), is_map(Input) ->
    detect_text(Client, Input, []).

-spec detect_text(aws_client:aws_client(), detect_text_request(), proplists:proplist()) ->
    {ok, detect_text_response(), tuple()} |
    {error, any()} |
    {error, detect_text_errors(), tuple()}.
detect_text(Client, Input, Options)
  when is_map(Client), is_map(Input), is_list(Options) ->
    request(Client, <<"DetectText">>, Input, Options).

%% @doc Removes the association between a `Face' supplied in an array of
%% `FaceIds' and the User.
%%
%% If the User is not present already, then a
%% `ResourceNotFound' exception is thrown. If successful, an array of
%% faces that are
%% disassociated from the User is returned. If a given face is already
%% disassociated from the
%% given UserID, it will be ignored and not be returned in the response. If a
%% given face is
%% already associated with a different User or not found in the collection it
%% will be returned as
%% part of `UnsuccessfulDisassociations'. You can remove 1 - 100 face IDs
%% from a user
%% at one time.
-spec disassociate_faces(aws_client:aws_client(), disassociate_faces_request()) ->
    {ok, disassociate_faces_response(), tuple()} |
    {error, any()} |
    {error, disassociate_faces_errors(), tuple()}.
disassociate_faces(Client, Input)
  when is_map(Client), is_map(Input) ->
    disassociate_faces(Client, Input, []).

-spec disassociate_faces(aws_client:aws_client(), disassociate_faces_request(), proplists:proplist()) ->
    {ok, disassociate_faces_response(), tuple()} |
    {error, any()} |
    {error, disassociate_faces_errors(), tuple()}.
disassociate_faces(Client, Input, Options)
  when is_map(Client), is_map(Input), is_list(Options) ->
    request(Client, <<"DisassociateFaces">>, Input, Options).

%% @doc
%% This operation applies only to Amazon Rekognition Custom Labels.
%%
%% Distributes the entries (images) in a training dataset across the training
%% dataset and the test dataset for a project.
%% `DistributeDatasetEntries' moves 20% of the training dataset images to
%% the test dataset.
%% An entry is a JSON Line that describes an image.
%%
%% You supply the Amazon Resource Names (ARN) of a project's training
%% dataset and test dataset.
%% The training dataset must contain the images that you want to split. The
%% test dataset
%% must be empty. The datasets must belong to the same project. To create
%% training and test datasets for a project, call `CreateDataset'.
%%
%% Distributing a dataset takes a while to complete. To check the status call
%% `DescribeDataset'. The operation
%% is complete when the `Status' field for the training dataset and the
%% test dataset is `UPDATE_COMPLETE'.
%% If the dataset split fails, the value of `Status' is
%% `UPDATE_FAILED'.
%%
%% This operation requires permissions to perform the
%% `rekognition:DistributeDatasetEntries' action.
-spec distribute_dataset_entries(aws_client:aws_client(), distribute_dataset_entries_request()) ->
    {ok, distribute_dataset_entries_response(), tuple()} |
    {error, any()} |
    {error, distribute_dataset_entries_errors(), tuple()}.
distribute_dataset_entries(Client, Input)
  when is_map(Client), is_map(Input) ->
    distribute_dataset_entries(Client, Input, []).

-spec distribute_dataset_entries(aws_client:aws_client(), distribute_dataset_entries_request(), proplists:proplist()) ->
    {ok, distribute_dataset_entries_response(), tuple()} |
    {error, any()} |
    {error, distribute_dataset_entries_errors(), tuple()}.
distribute_dataset_entries(Client, Input, Options)
  when is_map(Client), is_map(Input), is_list(Options) ->
    request(Client, <<"DistributeDatasetEntries">>, Input, Options).

%% @doc Gets the name and additional information about a celebrity based on
%% their Amazon Rekognition ID.
%%
%% The additional information is returned as an array of URLs. If there is no
%% additional
%% information about the celebrity, this list is empty.
%%
%% For more information, see Getting information about a celebrity in the
%% Amazon Rekognition Developer Guide.
%%
%% This operation requires permissions to perform the
%% `rekognition:GetCelebrityInfo' action.
-spec get_celebrity_info(aws_client:aws_client(), get_celebrity_info_request()) ->
    {ok, get_celebrity_info_response(), tuple()} |
    {error, any()} |
    {error, get_celebrity_info_errors(), tuple()}.
get_celebrity_info(Client, Input)
  when is_map(Client), is_map(Input) ->
    get_celebrity_info(Client, Input, []).

-spec get_celebrity_info(aws_client:aws_client(), get_celebrity_info_request(), proplists:proplist()) ->
    {ok, get_celebrity_info_response(), tuple()} |
    {error, any()} |
    {error, get_celebrity_info_errors(), tuple()}.
get_celebrity_info(Client, Input, Options)
  when is_map(Client), is_map(Input), is_list(Options) ->
    request(Client, <<"GetCelebrityInfo">>, Input, Options).

%% @doc Gets the celebrity recognition results for a Amazon Rekognition Video
%% analysis started by
%% `StartCelebrityRecognition'.
%%
%% Celebrity recognition in a video is an asynchronous operation. Analysis is
%% started by a
%% call to `StartCelebrityRecognition' which returns a job identifier
%% (`JobId').
%%
%% When the celebrity recognition operation finishes, Amazon Rekognition
%% Video publishes a completion
%% status to the Amazon Simple Notification Service topic registered in the
%% initial call to
%% `StartCelebrityRecognition'. To get the results of the celebrity
%% recognition
%% analysis, first check that the status value published to the Amazon SNS
%% topic is
%% `SUCCEEDED'. If so, call `GetCelebrityDetection' and pass the job
%% identifier (`JobId') from the initial call to
%% `StartCelebrityDetection'.
%%
%% For more information, see Working With Stored Videos in the Amazon
%% Rekognition Developer Guide.
%%
%% `GetCelebrityRecognition' returns detected celebrities and the time(s)
%% they
%% are detected in an array (`Celebrities') of `CelebrityRecognition'
%% objects. Each `CelebrityRecognition'
%% contains information about the celebrity in a `CelebrityDetail' object
%% and the
%% time, `Timestamp', the celebrity was detected. This
%% `CelebrityDetail' object stores information about the detected
%% celebrity's face
%% attributes, a face bounding box, known gender, the celebrity's name,
%% and a confidence
%% estimate.
%%
%% `GetCelebrityRecognition' only returns the default facial
%% attributes (`BoundingBox', `Confidence', `Landmarks',
%% `Pose', and `Quality'). The `BoundingBox' field only
%% applies to the detected face instance. The other facial attributes listed
%% in the
%% `Face' object of the following response syntax are not returned. For
%% more
%% information, see FaceDetail in the Amazon Rekognition Developer Guide.
%%
%% By default, the `Celebrities' array is sorted by time (milliseconds
%% from the start of the video).
%% You can also sort the array by celebrity by specifying the value `ID'
%% in the `SortBy' input parameter.
%%
%% The `CelebrityDetail' object includes the celebrity identifer and
%% additional information urls. If you don't store
%% the additional information urls, you can get them later by calling
%% `GetCelebrityInfo' with the celebrity identifer.
%%
%% No information is returned for faces not recognized as celebrities.
%%
%% Use MaxResults parameter to limit the number of labels returned. If there
%% are more results than
%% specified in `MaxResults', the value of `NextToken' in the
%% operation response contains a
%% pagination token for getting the next set of results. To get the next page
%% of results, call `GetCelebrityDetection'
%% and populate the `NextToken' request parameter with the token
%% value returned from the previous call to `GetCelebrityRecognition'.
-spec get_celebrity_recognition(aws_client:aws_client(), get_celebrity_recognition_request()) ->
    {ok, get_celebrity_recognition_response(), tuple()} |
    {error, any()} |
    {error, get_celebrity_recognition_errors(), tuple()}.
get_celebrity_recognition(Client, Input)
  when is_map(Client), is_map(Input) ->
    get_celebrity_recognition(Client, Input, []).

-spec get_celebrity_recognition(aws_client:aws_client(), get_celebrity_recognition_request(), proplists:proplist()) ->
    {ok, get_celebrity_recognition_response(), tuple()} |
    {error, any()} |
    {error, get_celebrity_recognition_errors(), tuple()}.
get_celebrity_recognition(Client, Input, Options)
  when is_map(Client), is_map(Input), is_list(Options) ->
    request(Client, <<"GetCelebrityRecognition">>, Input, Options).

%% @doc Gets the inappropriate, unwanted, or offensive content analysis
%% results for a Amazon Rekognition Video analysis started by
%% `StartContentModeration'.
%%
%% For a list of moderation labels in Amazon Rekognition, see
%% Using the image and video moderation APIs:
%% https://docs.aws.amazon.com/rekognition/latest/dg/moderation.html#moderation-api.
%%
%% Amazon Rekognition Video inappropriate or offensive content detection in a
%% stored video is an asynchronous operation. You start analysis by calling
%% `StartContentModeration' which returns a job identifier (`JobId').
%% When analysis finishes, Amazon Rekognition Video publishes a completion
%% status to the Amazon Simple Notification Service
%% topic registered in the initial call to `StartContentModeration'.
%% To get the results of the content analysis, first check that the status
%% value published to the Amazon SNS
%% topic is `SUCCEEDED'. If so, call `GetContentModeration' and pass
%% the job identifier
%% (`JobId') from the initial call to `StartContentModeration'.
%%
%% For more information, see Working with Stored Videos in the
%% Amazon Rekognition Devlopers Guide.
%%
%% `GetContentModeration' returns detected inappropriate, unwanted, or
%% offensive content moderation labels,
%% and the time they are detected, in an array, `ModerationLabels', of
%% `ContentModerationDetection' objects.
%%
%% By default, the moderated labels are returned sorted by time, in
%% milliseconds from the start of the
%% video. You can also sort them by moderated label by specifying `NAME'
%% for the `SortBy'
%% input parameter.
%%
%% Since video analysis can return a large number of results, use the
%% `MaxResults' parameter to limit
%% the number of labels returned in a single call to
%% `GetContentModeration'. If there are more results than
%% specified in `MaxResults', the value of `NextToken' in the
%% operation response contains a
%% pagination token for getting the next set of results. To get the next page
%% of results, call `GetContentModeration'
%% and populate the `NextToken' request parameter with the value of
%% `NextToken'
%% returned from the previous call to `GetContentModeration'.
%%
%% For more information, see moderating content in the Amazon Rekognition
%% Developer Guide.
-spec get_content_moderation(aws_client:aws_client(), get_content_moderation_request()) ->
    {ok, get_content_moderation_response(), tuple()} |
    {error, any()} |
    {error, get_content_moderation_errors(), tuple()}.
get_content_moderation(Client, Input)
  when is_map(Client), is_map(Input) ->
    get_content_moderation(Client, Input, []).

-spec get_content_moderation(aws_client:aws_client(), get_content_moderation_request(), proplists:proplist()) ->
    {ok, get_content_moderation_response(), tuple()} |
    {error, any()} |
    {error, get_content_moderation_errors(), tuple()}.
get_content_moderation(Client, Input, Options)
  when is_map(Client), is_map(Input), is_list(Options) ->
    request(Client, <<"GetContentModeration">>, Input, Options).

%% @doc Gets face detection results for a Amazon Rekognition Video analysis
%% started by `StartFaceDetection'.
%%
%% Face detection with Amazon Rekognition Video is an asynchronous operation.
%% You start face detection by calling `StartFaceDetection'
%% which returns a job identifier (`JobId'). When the face detection
%% operation finishes, Amazon Rekognition Video publishes a completion status
%% to
%% the Amazon Simple Notification Service topic registered in the initial
%% call to `StartFaceDetection'. To get the results
%% of the face detection operation, first check that the status value
%% published to the Amazon SNS topic is `SUCCEEDED'.
%% If so, call `GetFaceDetection' and pass the job identifier
%% (`JobId') from the initial call to `StartFaceDetection'.
%%
%% `GetFaceDetection' returns an array of detected faces (`Faces')
%% sorted by the time the faces were detected.
%%
%% Use MaxResults parameter to limit the number of labels returned. If there
%% are more results than
%% specified in `MaxResults', the value of `NextToken' in the
%% operation response contains a pagination token for getting the next set
%% of results. To get the next page of results, call `GetFaceDetection'
%% and populate the `NextToken' request parameter with the token
%% value returned from the previous call to `GetFaceDetection'.
%%
%% Note that for the `GetFaceDetection' operation, the returned values
%% for
%% `FaceOccluded' and `EyeDirection' will always be &quot;null&quot;.
-spec get_face_detection(aws_client:aws_client(), get_face_detection_request()) ->
    {ok, get_face_detection_response(), tuple()} |
    {error, any()} |
    {error, get_face_detection_errors(), tuple()}.
get_face_detection(Client, Input)
  when is_map(Client), is_map(Input) ->
    get_face_detection(Client, Input, []).

-spec get_face_detection(aws_client:aws_client(), get_face_detection_request(), proplists:proplist()) ->
    {ok, get_face_detection_response(), tuple()} |
    {error, any()} |
    {error, get_face_detection_errors(), tuple()}.
get_face_detection(Client, Input, Options)
  when is_map(Client), is_map(Input), is_list(Options) ->
    request(Client, <<"GetFaceDetection">>, Input, Options).

%% @doc Retrieves the results of a specific Face Liveness session.
%%
%% It requires the
%% `sessionId' as input, which was created using
%% `CreateFaceLivenessSession'. Returns the corresponding Face Liveness
%% confidence
%% score, a reference image that includes a face bounding box, and audit
%% images that also contain
%% face bounding boxes. The Face Liveness confidence score ranges from 0 to
%% 100.
%%
%% The number of audit images returned by `GetFaceLivenessSessionResults'
%% is
%% defined by the `AuditImagesLimit' paramater when calling
%% `CreateFaceLivenessSession'. Reference images are always returned when
%% possible.
-spec get_face_liveness_session_results(aws_client:aws_client(), get_face_liveness_session_results_request()) ->
    {ok, get_face_liveness_session_results_response(), tuple()} |
    {error, any()} |
    {error, get_face_liveness_session_results_errors(), tuple()}.
get_face_liveness_session_results(Client, Input)
  when is_map(Client), is_map(Input) ->
    get_face_liveness_session_results(Client, Input, []).

-spec get_face_liveness_session_results(aws_client:aws_client(), get_face_liveness_session_results_request(), proplists:proplist()) ->
    {ok, get_face_liveness_session_results_response(), tuple()} |
    {error, any()} |
    {error, get_face_liveness_session_results_errors(), tuple()}.
get_face_liveness_session_results(Client, Input, Options)
  when is_map(Client), is_map(Input), is_list(Options) ->
    request(Client, <<"GetFaceLivenessSessionResults">>, Input, Options).

%% @doc Gets the face search results for Amazon Rekognition Video face search
%% started by
%% `StartFaceSearch'.
%%
%% The search returns faces in a collection that match the faces
%% of persons detected in a video. It also includes the time(s) that faces
%% are matched in the video.
%%
%% Face search in a video is an asynchronous operation. You start face search
%% by calling
%% to `StartFaceSearch' which returns a job identifier (`JobId').
%% When the search operation finishes, Amazon Rekognition Video publishes a
%% completion status to the Amazon Simple Notification Service
%% topic registered in the initial call to `StartFaceSearch'.
%% To get the search results, first check that the status value published to
%% the Amazon SNS
%% topic is `SUCCEEDED'. If so, call `GetFaceSearch' and pass the job
%% identifier
%% (`JobId') from the initial call to `StartFaceSearch'.
%%
%% For more information, see Searching Faces in a Collection in the
%% Amazon Rekognition Developer Guide.
%%
%% The search results are retured in an array, `Persons', of
%% `PersonMatch' objects. Each`PersonMatch' element contains
%% details about the matching faces in the input collection, person
%% information (facial attributes,
%% bounding boxes, and person identifer)
%% for the matched person, and the time the person was matched in the video.
%%
%% `GetFaceSearch' only returns the default
%% facial attributes (`BoundingBox', `Confidence',
%% `Landmarks', `Pose', and `Quality'). The other facial
%% attributes listed
%% in the `Face' object of the following response syntax are not
%% returned. For more information,
%% see FaceDetail in the Amazon Rekognition Developer Guide.
%%
%% By default, the `Persons' array is sorted by the time, in milliseconds
%% from the
%% start of the video, persons are matched.
%% You can also sort by persons by specifying `INDEX' for the
%% `SORTBY' input
%% parameter.
-spec get_face_search(aws_client:aws_client(), get_face_search_request()) ->
    {ok, get_face_search_response(), tuple()} |
    {error, any()} |
    {error, get_face_search_errors(), tuple()}.
get_face_search(Client, Input)
  when is_map(Client), is_map(Input) ->
    get_face_search(Client, Input, []).

-spec get_face_search(aws_client:aws_client(), get_face_search_request(), proplists:proplist()) ->
    {ok, get_face_search_response(), tuple()} |
    {error, any()} |
    {error, get_face_search_errors(), tuple()}.
get_face_search(Client, Input, Options)
  when is_map(Client), is_map(Input), is_list(Options) ->
    request(Client, <<"GetFaceSearch">>, Input, Options).

%% @doc Gets the label detection results of a Amazon Rekognition Video
%% analysis started by `StartLabelDetection'.
%%
%% The label detection operation is started by a call to
%% `StartLabelDetection' which returns a job identifier (`JobId').
%% When
%% the label detection operation finishes, Amazon Rekognition publishes a
%% completion status to the
%% Amazon Simple Notification Service topic registered in the initial call to
%% `StartlabelDetection'.
%%
%% To get the results of the label detection operation, first check that the
%% status value
%% published to the Amazon SNS topic is `SUCCEEDED'. If so, call
%% `GetLabelDetection' and pass the job identifier (`JobId') from the
%% initial call to `StartLabelDetection'.
%%
%% `GetLabelDetection' returns an array of detected labels
%% (`Labels') sorted by the time the labels were detected. You can also
%% sort by the
%% label name by specifying `NAME' for the `SortBy' input parameter.
%% If
%% there is no `NAME' specified, the default sort is by
%% timestamp.
%%
%% You can select how results are aggregated by using the `AggregateBy'
%% input
%% parameter. The default aggregation method is `TIMESTAMPS'. You can
%% also aggregate
%% by `SEGMENTS', which aggregates all instances of labels detected in a
%% given
%% segment.
%%
%% The returned Labels array may include the following attributes:
%%
%% Name - The name of the detected label.
%%
%% Confidence - The level of confidence in the label assigned to a detected
%% object.
%%
%% Parents - The ancestor labels for a detected label. GetLabelDetection
%% returns a hierarchical
%% taxonomy of detected labels. For example, a detected car might be assigned
%% the label car.
%% The label car has two parent labels: Vehicle (its parent) and
%% Transportation (its
%% grandparent). The response includes the all ancestors for a label, where
%% every ancestor is
%% a unique label. In the previous example, Car, Vehicle, and Transportation
%% are returned as
%% unique labels in the response.
%%
%% Aliases - Possible Aliases for the label.
%%
%% Categories - The label categories that the detected label belongs to.
%%
%% BoundingBox  Bounding boxes are described for all instances of detected
%% common object labels,
%% returned in an array of Instance objects. An Instance object contains a
%% BoundingBox object, describing
%% the location of the label on the input image. It also includes the
%% confidence for the accuracy of the detected bounding box.
%%
%% Timestamp - Time, in milliseconds from the start of the video, that the
%% label was detected.
%% For aggregation by `SEGMENTS', the `StartTimestampMillis',
%% `EndTimestampMillis', and `DurationMillis' structures are what
%% define a segment. Although the Timestamp structure is still returned
%% with each label,
%% its value is set to be the same as `StartTimestampMillis'.
%%
%% Timestamp and Bounding box information are returned for detected
%% Instances, only if
%% aggregation is done by `TIMESTAMPS'. If aggregating by `SEGMENTS',
%% information about detected instances isnt returned.
%%
%% The version of the label model used for the detection is also returned.
%%
%% Note `DominantColors' isn't returned for `Instances',
%% although it is shown as part of the response in the sample seen below.
%%
%% Use `MaxResults' parameter to limit the number of labels returned. If
%% there are more results than specified in `MaxResults', the value of
%% `NextToken' in the operation response contains a pagination token for
%% getting the
%% next set of results. To get the next page of results, call
%% `GetlabelDetection' and
%% populate the `NextToken' request parameter with the token value
%% returned from the
%% previous call to `GetLabelDetection'.
%%
%% If you are retrieving results while using the Amazon Simple Notification
%% Service, note that you will receive an
%% &quot;ERROR&quot; notification if the job encounters an issue.
-spec get_label_detection(aws_client:aws_client(), get_label_detection_request()) ->
    {ok, get_label_detection_response(), tuple()} |
    {error, any()} |
    {error, get_label_detection_errors(), tuple()}.
get_label_detection(Client, Input)
  when is_map(Client), is_map(Input) ->
    get_label_detection(Client, Input, []).

-spec get_label_detection(aws_client:aws_client(), get_label_detection_request(), proplists:proplist()) ->
    {ok, get_label_detection_response(), tuple()} |
    {error, any()} |
    {error, get_label_detection_errors(), tuple()}.
get_label_detection(Client, Input, Options)
  when is_map(Client), is_map(Input), is_list(Options) ->
    request(Client, <<"GetLabelDetection">>, Input, Options).

%% @doc Retrieves the results for a given media analysis job.
%%
%% Takes a `JobId' returned by StartMediaAnalysisJob.
-spec get_media_analysis_job(aws_client:aws_client(), get_media_analysis_job_request()) ->
    {ok, get_media_analysis_job_response(), tuple()} |
    {error, any()} |
    {error, get_media_analysis_job_errors(), tuple()}.
get_media_analysis_job(Client, Input)
  when is_map(Client), is_map(Input) ->
    get_media_analysis_job(Client, Input, []).

-spec get_media_analysis_job(aws_client:aws_client(), get_media_analysis_job_request(), proplists:proplist()) ->
    {ok, get_media_analysis_job_response(), tuple()} |
    {error, any()} |
    {error, get_media_analysis_job_errors(), tuple()}.
get_media_analysis_job(Client, Input, Options)
  when is_map(Client), is_map(Input), is_list(Options) ->
    request(Client, <<"GetMediaAnalysisJob">>, Input, Options).

%% @doc
%%
%% End of support notice: On October 31, 2025, AWS will discontinue
%% support for Amazon Rekognition People Pathing.
%%
%% After October 31, 2025, you will no
%% longer be able to use the Rekognition People Pathing capability. For more
%% information,
%% visit this blog post:
%% https://aws.amazon.com/blogs/machine-learning/transitioning-from-amazon-rekognition-people-pathing-exploring-other-alternatives/.
%%
%% Gets the path tracking results of a Amazon Rekognition Video analysis
%% started by `StartPersonTracking'.
%%
%% The person path tracking operation is started by a call to
%% `StartPersonTracking'
%% which returns a job identifier (`JobId'). When the operation finishes,
%% Amazon Rekognition Video publishes a completion status to
%% the Amazon Simple Notification Service topic registered in the initial
%% call to `StartPersonTracking'.
%%
%% To get the results of the person path tracking operation, first check
%% that the status value published to the Amazon SNS topic is
%% `SUCCEEDED'.
%% If so, call `GetPersonTracking' and pass the job identifier
%% (`JobId') from the initial call to `StartPersonTracking'.
%%
%% `GetPersonTracking' returns an array, `Persons', of tracked
%% persons and the time(s) their
%% paths were tracked in the video.
%%
%% `GetPersonTracking' only returns the default
%% facial attributes (`BoundingBox', `Confidence',
%% `Landmarks', `Pose', and `Quality'). The other facial
%% attributes listed
%% in the `Face' object of the following response syntax are not
%% returned.
%%
%% For more information, see FaceDetail in the Amazon Rekognition Developer
%% Guide.
%%
%% By default, the array is sorted by the time(s) a person's path is
%% tracked in the video.
%% You can sort by tracked persons by specifying `INDEX' for the
%% `SortBy' input parameter.
%%
%% Use the `MaxResults' parameter to limit the number of items returned.
%% If there are more results than
%% specified in `MaxResults', the value of `NextToken' in the
%% operation response contains a pagination token for getting the next set
%% of results. To get the next page of results, call `GetPersonTracking'
%% and populate the `NextToken' request parameter with the token
%% value returned from the previous call to `GetPersonTracking'.
-spec get_person_tracking(aws_client:aws_client(), get_person_tracking_request()) ->
    {ok, get_person_tracking_response(), tuple()} |
    {error, any()} |
    {error, get_person_tracking_errors(), tuple()}.
get_person_tracking(Client, Input)
  when is_map(Client), is_map(Input) ->
    get_person_tracking(Client, Input, []).

-spec get_person_tracking(aws_client:aws_client(), get_person_tracking_request(), proplists:proplist()) ->
    {ok, get_person_tracking_response(), tuple()} |
    {error, any()} |
    {error, get_person_tracking_errors(), tuple()}.
get_person_tracking(Client, Input, Options)
  when is_map(Client), is_map(Input), is_list(Options) ->
    request(Client, <<"GetPersonTracking">>, Input, Options).

%% @doc Gets the segment detection results of a Amazon Rekognition Video
%% analysis started by `StartSegmentDetection'.
%%
%% Segment detection with Amazon Rekognition Video is an asynchronous
%% operation. You start segment detection by
%% calling `StartSegmentDetection' which returns a job identifier
%% (`JobId').
%% When the segment detection operation finishes, Amazon Rekognition
%% publishes a completion status to the Amazon Simple Notification Service
%% topic registered in the initial call to `StartSegmentDetection'. To
%% get the results
%% of the segment detection operation, first check that the status value
%% published to the Amazon SNS topic is `SUCCEEDED'.
%% if so, call `GetSegmentDetection' and pass the job identifier
%% (`JobId') from the initial call
%% of `StartSegmentDetection'.
%%
%% `GetSegmentDetection' returns detected segments in an array
%% (`Segments')
%% of `SegmentDetection' objects. `Segments' is sorted by the segment
%% types
%% specified in the `SegmentTypes' input parameter of
%% `StartSegmentDetection'.
%% Each element of the array includes the detected segment, the precentage
%% confidence in the acuracy
%% of the detected segment, the type of the segment, and the frame in which
%% the segment was detected.
%%
%% Use `SelectedSegmentTypes' to find out the type of segment detection
%% requested in the
%% call to `StartSegmentDetection'.
%%
%% Use the `MaxResults' parameter to limit the number of segment
%% detections returned. If there are more results than
%% specified in `MaxResults', the value of `NextToken' in the
%% operation response contains
%% a pagination token for getting the next set of results. To get the next
%% page of results, call `GetSegmentDetection'
%% and populate the `NextToken' request parameter with the token value
%% returned from the previous
%% call to `GetSegmentDetection'.
%%
%% For more information, see Detecting video segments in stored video in the
%% Amazon Rekognition Developer Guide.
-spec get_segment_detection(aws_client:aws_client(), get_segment_detection_request()) ->
    {ok, get_segment_detection_response(), tuple()} |
    {error, any()} |
    {error, get_segment_detection_errors(), tuple()}.
get_segment_detection(Client, Input)
  when is_map(Client), is_map(Input) ->
    get_segment_detection(Client, Input, []).

-spec get_segment_detection(aws_client:aws_client(), get_segment_detection_request(), proplists:proplist()) ->
    {ok, get_segment_detection_response(), tuple()} |
    {error, any()} |
    {error, get_segment_detection_errors(), tuple()}.
get_segment_detection(Client, Input, Options)
  when is_map(Client), is_map(Input), is_list(Options) ->
    request(Client, <<"GetSegmentDetection">>, Input, Options).

%% @doc Gets the text detection results of a Amazon Rekognition Video
%% analysis started by `StartTextDetection'.
%%
%% Text detection with Amazon Rekognition Video is an asynchronous operation.
%% You start text detection by
%% calling `StartTextDetection' which returns a job identifier
%% (`JobId')
%% When the text detection operation finishes, Amazon Rekognition publishes a
%% completion status to the Amazon Simple Notification Service
%% topic registered in the initial call to `StartTextDetection'. To get
%% the results
%% of the text detection operation, first check that the status value
%% published to the Amazon SNS topic is `SUCCEEDED'.
%% if so, call `GetTextDetection' and pass the job identifier
%% (`JobId') from the initial call
%% of `StartLabelDetection'.
%%
%% `GetTextDetection' returns an array of detected text
%% (`TextDetections') sorted by
%% the time the text was detected, up to 100 words per frame of video.
%%
%% Each element of the array includes the detected text, the precentage
%% confidence in the acuracy
%% of the detected text, the time the text was detected, bounding box
%% information for where the text
%% was located, and unique identifiers for words and their lines.
%%
%% Use MaxResults parameter to limit the number of text detections returned.
%% If there are more results than
%% specified in `MaxResults', the value of `NextToken' in the
%% operation response contains
%% a pagination token for getting the next set of results. To get the next
%% page of results, call `GetTextDetection'
%% and populate the `NextToken' request parameter with the token value
%% returned from the previous
%% call to `GetTextDetection'.
-spec get_text_detection(aws_client:aws_client(), get_text_detection_request()) ->
    {ok, get_text_detection_response(), tuple()} |
    {error, any()} |
    {error, get_text_detection_errors(), tuple()}.
get_text_detection(Client, Input)
  when is_map(Client), is_map(Input) ->
    get_text_detection(Client, Input, []).

-spec get_text_detection(aws_client:aws_client(), get_text_detection_request(), proplists:proplist()) ->
    {ok, get_text_detection_response(), tuple()} |
    {error, any()} |
    {error, get_text_detection_errors(), tuple()}.
get_text_detection(Client, Input, Options)
  when is_map(Client), is_map(Input), is_list(Options) ->
    request(Client, <<"GetTextDetection">>, Input, Options).

%% @doc Detects faces in the input image and adds them to the specified
%% collection.
%%
%% Amazon Rekognition doesn't save the actual faces that are detected.
%% Instead, the underlying
%% detection algorithm first detects the faces in the input image. For each
%% face, the algorithm
%% extracts facial features into a feature vector, and stores it in the
%% backend database.
%% Amazon Rekognition uses feature vectors when it performs face match and
%% search operations using the
%% `SearchFaces' and `SearchFacesByImage' operations.
%%
%% For more information, see Adding faces to a collection in the Amazon
%% Rekognition
%% Developer Guide.
%%
%% To get the number of faces in a collection, call `DescribeCollection'.
%%
%% If you're using version 1.0 of the face detection model,
%% `IndexFaces'
%% indexes the 15 largest faces in the input image. Later versions of the
%% face detection model
%% index the 100 largest faces in the input image.
%%
%% If you're using version 4 or later of the face model, image
%% orientation information is not
%% returned in the `OrientationCorrection' field.
%%
%% To determine which version of the model you're using, call
%% `DescribeCollection' and supply the collection ID. You can also get
%% the model
%% version from the value of `FaceModelVersion' in the response from
%% `IndexFaces'
%%
%% For more information, see Model Versioning in the Amazon Rekognition
%% Developer
%% Guide.
%%
%% If you provide the optional `ExternalImageId' for the input image you
%% provided, Amazon Rekognition associates this ID with all faces that it
%% detects. When you call the `ListFaces' operation, the response returns
%% the external ID. You can use this
%% external image ID to create a client-side index to associate the faces
%% with each image. You
%% can then use the index to find all faces in an image.
%%
%% You can specify the maximum number of faces to index with the
%% `MaxFaces' input
%% parameter. This is useful when you want to index the largest faces in an
%% image and don't want
%% to index smaller faces, such as those belonging to people standing in the
%% background.
%%
%% The `QualityFilter' input parameter allows you to filter out detected
%% faces
%% that dont meet a required quality bar. The quality bar is based on a
%% variety of common use
%% cases. By default, `IndexFaces' chooses the quality bar that's
%% used to filter
%% faces. You can also explicitly choose the quality bar. Use
%% `QualityFilter', to set
%% the quality bar by specifying `LOW', `MEDIUM', or `HIGH'. If
%% you do not want to filter detected faces, specify `NONE'.
%%
%% To use quality filtering, you need a collection associated with version 3
%% of the face
%% model or higher. To get the version of the face model associated with a
%% collection, call
%% `DescribeCollection'.
%%
%% Information about faces detected in an image, but not indexed, is returned
%% in an array of
%% `UnindexedFace' objects, `UnindexedFaces'. Faces aren't
%% indexed
%% for reasons such as:
%%
%% The number of faces detected exceeds the value of the `MaxFaces'
%% request
%% parameter.
%%
%% The face is too small compared to the image dimensions.
%%
%% The face is too blurry.
%%
%% The image is too dark.
%%
%% The face has an extreme pose.
%%
%% The face doesnt have enough detail to be suitable for face search.
%%
%% In response, the `IndexFaces' operation returns an array of metadata
%% for all
%% detected faces, `FaceRecords'. This includes:
%%
%% The bounding box, `BoundingBox', of the detected face.
%%
%% A confidence value, `Confidence', which indicates the confidence that
%% the
%% bounding box contains a face.
%%
%% A face ID, `FaceId', assigned by the service for each face that's
%% detected
%% and stored.
%%
%% An image ID, `ImageId', assigned by the service for the input image.
%%
%% If you request `ALL' or specific facial attributes (e.g.,
%% `FACE_OCCLUDED') by using the detectionAttributes parameter, Amazon
%% Rekognition
%% returns detailed facial attributes, such as facial landmarks (for example,
%% location of eye and
%% mouth), facial occlusion, and other facial attributes.
%%
%% If you provide the same image, specify the same collection, and use the
%% same external ID
%% in the `IndexFaces' operation, Amazon Rekognition doesn't save
%% duplicate face
%% metadata.
%%
%% The input image is passed either as base64-encoded image bytes, or as a
%% reference to an
%% image in an Amazon S3 bucket. If you use the AWS CLI to call Amazon
%% Rekognition operations,
%% passing image bytes isn't supported. The image must be formatted as a
%% PNG or JPEG file.
%%
%% This operation requires permissions to perform the
%% `rekognition:IndexFaces'
%% action.
-spec index_faces(aws_client:aws_client(), index_faces_request()) ->
    {ok, index_faces_response(), tuple()} |
    {error, any()} |
    {error, index_faces_errors(), tuple()}.
index_faces(Client, Input)
  when is_map(Client), is_map(Input) ->
    index_faces(Client, Input, []).

-spec index_faces(aws_client:aws_client(), index_faces_request(), proplists:proplist()) ->
    {ok, index_faces_response(), tuple()} |
    {error, any()} |
    {error, index_faces_errors(), tuple()}.
index_faces(Client, Input, Options)
  when is_map(Client), is_map(Input), is_list(Options) ->
    request(Client, <<"IndexFaces">>, Input, Options).

%% @doc Returns list of collection IDs in your account.
%%
%% If the result is truncated, the
%% response also provides a `NextToken' that you can use in the
%% subsequent request to
%% fetch the next set of collection IDs.
%%
%% For an example, see Listing collections in the Amazon Rekognition
%% Developer
%% Guide.
%%
%% This operation requires permissions to perform the
%% `rekognition:ListCollections' action.
-spec list_collections(aws_client:aws_client(), list_collections_request()) ->
    {ok, list_collections_response(), tuple()} |
    {error, any()} |
    {error, list_collections_errors(), tuple()}.
list_collections(Client, Input)
  when is_map(Client), is_map(Input) ->
    list_collections(Client, Input, []).

-spec list_collections(aws_client:aws_client(), list_collections_request(), proplists:proplist()) ->
    {ok, list_collections_response(), tuple()} |
    {error, any()} |
    {error, list_collections_errors(), tuple()}.
list_collections(Client, Input, Options)
  when is_map(Client), is_map(Input), is_list(Options) ->
    request(Client, <<"ListCollections">>, Input, Options).

%% @doc
%% This operation applies only to Amazon Rekognition Custom Labels.
%%
%% Lists the entries (images) within a dataset. An entry is a
%% JSON Line that contains the information for a single image, including
%% the image location, assigned labels, and object location bounding boxes.
%% For
%% more information, see Creating a manifest file:
%% https://docs.aws.amazon.com/rekognition/latest/customlabels-dg/md-manifest-files.html.
%%
%% JSON Lines in the response include information about non-terminal
%% errors found in the dataset.
%% Non terminal errors are reported in `errors' lists within each JSON
%% Line. The
%% same information is reported in the training and testing validation result
%% manifests that
%% Amazon Rekognition Custom Labels creates during model training.
%%
%% You can filter the response in variety of ways, such as choosing which
%% labels to return and returning JSON Lines created after a specific date.
%%
%% This operation requires permissions to perform the
%% `rekognition:ListDatasetEntries' action.
-spec list_dataset_entries(aws_client:aws_client(), list_dataset_entries_request()) ->
    {ok, list_dataset_entries_response(), tuple()} |
    {error, any()} |
    {error, list_dataset_entries_errors(), tuple()}.
list_dataset_entries(Client, Input)
  when is_map(Client), is_map(Input) ->
    list_dataset_entries(Client, Input, []).

-spec list_dataset_entries(aws_client:aws_client(), list_dataset_entries_request(), proplists:proplist()) ->
    {ok, list_dataset_entries_response(), tuple()} |
    {error, any()} |
    {error, list_dataset_entries_errors(), tuple()}.
list_dataset_entries(Client, Input, Options)
  when is_map(Client), is_map(Input), is_list(Options) ->
    request(Client, <<"ListDatasetEntries">>, Input, Options).

%% @doc
%% This operation applies only to Amazon Rekognition Custom Labels.
%%
%% Lists the labels in a dataset. Amazon Rekognition Custom Labels uses
%% labels to describe images. For more information, see
%% Labeling images:
%% https://docs.aws.amazon.com/rekognition/latest/customlabels-dg/md-labeling-images.html.
%%
%% Lists the labels in a dataset. Amazon Rekognition Custom Labels uses
%% labels to describe images. For more information, see Labeling images
%% in the Amazon Rekognition Custom Labels Developer Guide.
-spec list_dataset_labels(aws_client:aws_client(), list_dataset_labels_request()) ->
    {ok, list_dataset_labels_response(), tuple()} |
    {error, any()} |
    {error, list_dataset_labels_errors(), tuple()}.
list_dataset_labels(Client, Input)
  when is_map(Client), is_map(Input) ->
    list_dataset_labels(Client, Input, []).

-spec list_dataset_labels(aws_client:aws_client(), list_dataset_labels_request(), proplists:proplist()) ->
    {ok, list_dataset_labels_response(), tuple()} |
    {error, any()} |
    {error, list_dataset_labels_errors(), tuple()}.
list_dataset_labels(Client, Input, Options)
  when is_map(Client), is_map(Input), is_list(Options) ->
    request(Client, <<"ListDatasetLabels">>, Input, Options).

%% @doc Returns metadata for faces in the specified collection.
%%
%% This metadata
%% includes information such as the bounding box coordinates, the confidence
%% (that the bounding
%% box contains a face), and face ID. For an example, see Listing Faces in a
%% Collection in the
%% Amazon Rekognition Developer Guide.
%%
%% This operation requires permissions to perform the
%% `rekognition:ListFaces'
%% action.
-spec list_faces(aws_client:aws_client(), list_faces_request()) ->
    {ok, list_faces_response(), tuple()} |
    {error, any()} |
    {error, list_faces_errors(), tuple()}.
list_faces(Client, Input)
  when is_map(Client), is_map(Input) ->
    list_faces(Client, Input, []).

-spec list_faces(aws_client:aws_client(), list_faces_request(), proplists:proplist()) ->
    {ok, list_faces_response(), tuple()} |
    {error, any()} |
    {error, list_faces_errors(), tuple()}.
list_faces(Client, Input, Options)
  when is_map(Client), is_map(Input), is_list(Options) ->
    request(Client, <<"ListFaces">>, Input, Options).

%% @doc Returns a list of media analysis jobs.
%%
%% Results are sorted by `CreationTimestamp' in descending order.
-spec list_media_analysis_jobs(aws_client:aws_client(), list_media_analysis_jobs_request()) ->
    {ok, list_media_analysis_jobs_response(), tuple()} |
    {error, any()} |
    {error, list_media_analysis_jobs_errors(), tuple()}.
list_media_analysis_jobs(Client, Input)
  when is_map(Client), is_map(Input) ->
    list_media_analysis_jobs(Client, Input, []).

-spec list_media_analysis_jobs(aws_client:aws_client(), list_media_analysis_jobs_request(), proplists:proplist()) ->
    {ok, list_media_analysis_jobs_response(), tuple()} |
    {error, any()} |
    {error, list_media_analysis_jobs_errors(), tuple()}.
list_media_analysis_jobs(Client, Input, Options)
  when is_map(Client), is_map(Input), is_list(Options) ->
    request(Client, <<"ListMediaAnalysisJobs">>, Input, Options).

%% @doc
%% This operation applies only to Amazon Rekognition Custom Labels.
%%
%% Gets a list of the project policies attached to a project.
%%
%% To attach a project policy to a project, call `PutProjectPolicy'. To
%% remove a project policy from a project, call `DeleteProjectPolicy'.
%%
%% This operation requires permissions to perform the
%% `rekognition:ListProjectPolicies' action.
-spec list_project_policies(aws_client:aws_client(), list_project_policies_request()) ->
    {ok, list_project_policies_response(), tuple()} |
    {error, any()} |
    {error, list_project_policies_errors(), tuple()}.
list_project_policies(Client, Input)
  when is_map(Client), is_map(Input) ->
    list_project_policies(Client, Input, []).

-spec list_project_policies(aws_client:aws_client(), list_project_policies_request(), proplists:proplist()) ->
    {ok, list_project_policies_response(), tuple()} |
    {error, any()} |
    {error, list_project_policies_errors(), tuple()}.
list_project_policies(Client, Input, Options)
  when is_map(Client), is_map(Input), is_list(Options) ->
    request(Client, <<"ListProjectPolicies">>, Input, Options).

%% @doc Gets a list of stream processors that you have created with
%% `CreateStreamProcessor'.
-spec list_stream_processors(aws_client:aws_client(), list_stream_processors_request()) ->
    {ok, list_stream_processors_response(), tuple()} |
    {error, any()} |
    {error, list_stream_processors_errors(), tuple()}.
list_stream_processors(Client, Input)
  when is_map(Client), is_map(Input) ->
    list_stream_processors(Client, Input, []).

-spec list_stream_processors(aws_client:aws_client(), list_stream_processors_request(), proplists:proplist()) ->
    {ok, list_stream_processors_response(), tuple()} |
    {error, any()} |
    {error, list_stream_processors_errors(), tuple()}.
list_stream_processors(Client, Input, Options)
  when is_map(Client), is_map(Input), is_list(Options) ->
    request(Client, <<"ListStreamProcessors">>, Input, Options).

%% @doc Returns a list of tags in an Amazon Rekognition collection, stream
%% processor, or Custom Labels
%% model.
%%
%% This operation requires permissions to perform the
%% `rekognition:ListTagsForResource' action.
-spec list_tags_for_resource(aws_client:aws_client(), list_tags_for_resource_request()) ->
    {ok, list_tags_for_resource_response(), tuple()} |
    {error, any()} |
    {error, list_tags_for_resource_errors(), tuple()}.
list_tags_for_resource(Client, Input)
  when is_map(Client), is_map(Input) ->
    list_tags_for_resource(Client, Input, []).

-spec list_tags_for_resource(aws_client:aws_client(), list_tags_for_resource_request(), proplists:proplist()) ->
    {ok, list_tags_for_resource_response(), tuple()} |
    {error, any()} |
    {error, list_tags_for_resource_errors(), tuple()}.
list_tags_for_resource(Client, Input, Options)
  when is_map(Client), is_map(Input), is_list(Options) ->
    request(Client, <<"ListTagsForResource">>, Input, Options).

%% @doc Returns metadata of the User such as `UserID' in the specified
%% collection.
%%
%% Anonymous User (to reserve faces without any identity) is not returned as
%% part of this
%% request. The results are sorted by system generated primary key ID. If the
%% response is
%% truncated, `NextToken' is returned in the response that can be used in
%% the
%% subsequent request to retrieve the next set of identities.
-spec list_users(aws_client:aws_client(), list_users_request()) ->
    {ok, list_users_response(), tuple()} |
    {error, any()} |
    {error, list_users_errors(), tuple()}.
list_users(Client, Input)
  when is_map(Client), is_map(Input) ->
    list_users(Client, Input, []).

-spec list_users(aws_client:aws_client(), list_users_request(), proplists:proplist()) ->
    {ok, list_users_response(), tuple()} |
    {error, any()} |
    {error, list_users_errors(), tuple()}.
list_users(Client, Input, Options)
  when is_map(Client), is_map(Input), is_list(Options) ->
    request(Client, <<"ListUsers">>, Input, Options).

%% @doc
%% This operation applies only to Amazon Rekognition Custom Labels.
%%
%% Attaches a project policy to a Amazon Rekognition Custom Labels project in
%% a trusting AWS account. A
%% project policy specifies that a trusted AWS account can copy a model
%% version from a
%% trusting AWS account to a project in the trusted AWS account. To copy a
%% model version
%% you use the `CopyProjectVersion' operation. Only applies to Custom
%% Labels
%% projects.
%%
%% For more information about the format of a project policy document, see
%% Attaching a project policy (SDK)
%% in the Amazon Rekognition Custom Labels Developer Guide.
%%
%% The response from `PutProjectPolicy' is a revision ID for the project
%% policy.
%% You can attach multiple project policies to a project. You can also update
%% an existing
%% project policy by specifying the policy revision ID of the existing
%% policy.
%%
%% To remove a project policy from a project, call `DeleteProjectPolicy'.
%% To get a list of project policies attached to a project, call
%% `ListProjectPolicies'.
%%
%% You copy a model version by calling `CopyProjectVersion'.
%%
%% This operation requires permissions to perform the
%% `rekognition:PutProjectPolicy' action.
-spec put_project_policy(aws_client:aws_client(), put_project_policy_request()) ->
    {ok, put_project_policy_response(), tuple()} |
    {error, any()} |
    {error, put_project_policy_errors(), tuple()}.
put_project_policy(Client, Input)
  when is_map(Client), is_map(Input) ->
    put_project_policy(Client, Input, []).

-spec put_project_policy(aws_client:aws_client(), put_project_policy_request(), proplists:proplist()) ->
    {ok, put_project_policy_response(), tuple()} |
    {error, any()} |
    {error, put_project_policy_errors(), tuple()}.
put_project_policy(Client, Input, Options)
  when is_map(Client), is_map(Input), is_list(Options) ->
    request(Client, <<"PutProjectPolicy">>, Input, Options).

%% @doc Returns an array of celebrities recognized in the input image.
%%
%% For more
%% information, see Recognizing celebrities in the Amazon Rekognition
%% Developer Guide.
%%
%% `RecognizeCelebrities' returns the 64 largest faces in the image. It
%% lists
%% the recognized celebrities in the `CelebrityFaces' array and any
%% unrecognized faces
%% in the `UnrecognizedFaces' array. `RecognizeCelebrities'
%% doesn't return
%% celebrities whose faces aren't among the largest 64 faces in the
%% image.
%%
%% For each celebrity recognized, `RecognizeCelebrities' returns a
%% `Celebrity' object. The `Celebrity' object contains the celebrity
%% name, ID, URL links to additional information, match confidence, and a
%% `ComparedFace' object that you can use to locate the celebrity's
%% face on the
%% image.
%%
%% Amazon Rekognition doesn't retain information about which images a
%% celebrity has been recognized
%% in. Your application must store this information and use the
%% `Celebrity' ID
%% property as a unique identifier for the celebrity. If you don't store
%% the celebrity name or
%% additional information URLs returned by `RecognizeCelebrities', you
%% will need the
%% ID to identify the celebrity in a call to the `GetCelebrityInfo'
%% operation.
%%
%% You pass the input image either as base64-encoded image bytes or as a
%% reference to an
%% image in an Amazon S3 bucket. If you use the
%% AWS
%% CLI to call Amazon Rekognition operations, passing image bytes is not
%% supported. The image must be either a PNG or JPEG formatted file.
%%
%% For an example, see Recognizing celebrities in an image in the Amazon
%% Rekognition
%% Developer Guide.
%%
%% This operation requires permissions to perform the
%% `rekognition:RecognizeCelebrities' operation.
-spec recognize_celebrities(aws_client:aws_client(), recognize_celebrities_request()) ->
    {ok, recognize_celebrities_response(), tuple()} |
    {error, any()} |
    {error, recognize_celebrities_errors(), tuple()}.
recognize_celebrities(Client, Input)
  when is_map(Client), is_map(Input) ->
    recognize_celebrities(Client, Input, []).

-spec recognize_celebrities(aws_client:aws_client(), recognize_celebrities_request(), proplists:proplist()) ->
    {ok, recognize_celebrities_response(), tuple()} |
    {error, any()} |
    {error, recognize_celebrities_errors(), tuple()}.
recognize_celebrities(Client, Input, Options)
  when is_map(Client), is_map(Input), is_list(Options) ->
    request(Client, <<"RecognizeCelebrities">>, Input, Options).

%% @doc For a given input face ID, searches for matching faces in the
%% collection the face
%% belongs to.
%%
%% You get a face ID when you add a face to the collection using the
%% `IndexFaces' operation. The operation compares the features of the
%% input face with
%% faces in the specified collection.
%%
%% You can also search faces without indexing faces by using the
%% `SearchFacesByImage' operation.
%%
%% The operation response returns an array of faces that match, ordered by
%% similarity
%% score with the highest similarity first. More specifically, it is an array
%% of metadata for
%% each face match that is found. Along with the metadata, the response also
%% includes a
%% `confidence' value for each face match, indicating the confidence that
%% the
%% specific face matches the input face.
%%
%% For an example, see Searching for a face using its face ID in the Amazon
%% Rekognition
%% Developer Guide.
%%
%% This operation requires permissions to perform the
%% `rekognition:SearchFaces'
%% action.
-spec search_faces(aws_client:aws_client(), search_faces_request()) ->
    {ok, search_faces_response(), tuple()} |
    {error, any()} |
    {error, search_faces_errors(), tuple()}.
search_faces(Client, Input)
  when is_map(Client), is_map(Input) ->
    search_faces(Client, Input, []).

-spec search_faces(aws_client:aws_client(), search_faces_request(), proplists:proplist()) ->
    {ok, search_faces_response(), tuple()} |
    {error, any()} |
    {error, search_faces_errors(), tuple()}.
search_faces(Client, Input, Options)
  when is_map(Client), is_map(Input), is_list(Options) ->
    request(Client, <<"SearchFaces">>, Input, Options).

%% @doc For a given input image, first detects the largest face in the image,
%% and then searches
%% the specified collection for matching faces.
%%
%% The operation compares the features of the input
%% face with faces in the specified collection.
%%
%% To search for all faces in an input image, you might first call the
%% `IndexFaces' operation, and then use the face IDs returned in
%% subsequent calls
%% to the `SearchFaces' operation.
%%
%% You can also call the `DetectFaces' operation and use the bounding
%% boxes
%% in the response to make face crops, which then you can pass in to the
%% `SearchFacesByImage' operation.
%%
%% You pass the input image either as base64-encoded image bytes or as a
%% reference to an
%% image in an Amazon S3 bucket. If you use the
%% AWS
%% CLI to call Amazon Rekognition operations, passing image bytes is not
%% supported. The image must be either a PNG or JPEG formatted file.
%%
%% The response returns an array of faces that match, ordered by similarity
%% score with
%% the highest similarity first. More specifically, it is an array of
%% metadata for each face
%% match found. Along with the metadata, the response also includes a
%% `similarity'
%% indicating how similar the face is to the input face. In the response, the
%% operation also
%% returns the bounding box (and a confidence level that the bounding box
%% contains a face) of the
%% face that Amazon Rekognition used for the input image.
%%
%% If no faces are detected in the input image, `SearchFacesByImage'
%% returns an
%% `InvalidParameterException' error.
%%
%% For an example, Searching for a Face Using an Image in the Amazon
%% Rekognition
%% Developer Guide.
%%
%% The `QualityFilter' input parameter allows you to filter out detected
%% faces
%% that dont meet a required quality bar. The quality bar is based on a
%% variety of common use
%% cases. Use `QualityFilter' to set the quality bar for filtering by
%% specifying
%% `LOW', `MEDIUM', or `HIGH'. If you do not want to filter
%% detected faces, specify `NONE'. The default value is `NONE'.
%%
%% To use quality filtering, you need a collection associated with version 3
%% of the face
%% model or higher. To get the version of the face model associated with a
%% collection, call
%% `DescribeCollection'.
%%
%% This operation requires permissions to perform the
%% `rekognition:SearchFacesByImage' action.
-spec search_faces_by_image(aws_client:aws_client(), search_faces_by_image_request()) ->
    {ok, search_faces_by_image_response(), tuple()} |
    {error, any()} |
    {error, search_faces_by_image_errors(), tuple()}.
search_faces_by_image(Client, Input)
  when is_map(Client), is_map(Input) ->
    search_faces_by_image(Client, Input, []).

-spec search_faces_by_image(aws_client:aws_client(), search_faces_by_image_request(), proplists:proplist()) ->
    {ok, search_faces_by_image_response(), tuple()} |
    {error, any()} |
    {error, search_faces_by_image_errors(), tuple()}.
search_faces_by_image(Client, Input, Options)
  when is_map(Client), is_map(Input), is_list(Options) ->
    request(Client, <<"SearchFacesByImage">>, Input, Options).

%% @doc Searches for UserIDs within a collection based on a `FaceId' or
%% `UserId'.
%%
%% This API can be used to find the closest UserID (with a highest
%% similarity) to associate a face. The request must be provided with either
%% `FaceId'
%% or `UserId'. The operation returns an array of UserID that match the
%% `FaceId' or `UserId', ordered by similarity score with the highest
%% similarity first.
-spec search_users(aws_client:aws_client(), search_users_request()) ->
    {ok, search_users_response(), tuple()} |
    {error, any()} |
    {error, search_users_errors(), tuple()}.
search_users(Client, Input)
  when is_map(Client), is_map(Input) ->
    search_users(Client, Input, []).

-spec search_users(aws_client:aws_client(), search_users_request(), proplists:proplist()) ->
    {ok, search_users_response(), tuple()} |
    {error, any()} |
    {error, search_users_errors(), tuple()}.
search_users(Client, Input, Options)
  when is_map(Client), is_map(Input), is_list(Options) ->
    request(Client, <<"SearchUsers">>, Input, Options).

%% @doc Searches for UserIDs using a supplied image.
%%
%% It first detects the largest face in the
%% image, and then searches a specified collection for matching UserIDs.
%%
%% The operation returns an array of UserIDs that match the face in the
%% supplied image,
%% ordered by similarity score with the highest similarity first. It also
%% returns a bounding box
%% for the face found in the input image.
%%
%% Information about faces detected in the supplied image, but not used for
%% the search, is
%% returned in an array of `UnsearchedFace' objects. If no valid face is
%% detected in
%% the image, the response will contain an empty `UserMatches' list and
%% no
%% `SearchedFace' object.
-spec search_users_by_image(aws_client:aws_client(), search_users_by_image_request()) ->
    {ok, search_users_by_image_response(), tuple()} |
    {error, any()} |
    {error, search_users_by_image_errors(), tuple()}.
search_users_by_image(Client, Input)
  when is_map(Client), is_map(Input) ->
    search_users_by_image(Client, Input, []).

-spec search_users_by_image(aws_client:aws_client(), search_users_by_image_request(), proplists:proplist()) ->
    {ok, search_users_by_image_response(), tuple()} |
    {error, any()} |
    {error, search_users_by_image_errors(), tuple()}.
search_users_by_image(Client, Input, Options)
  when is_map(Client), is_map(Input), is_list(Options) ->
    request(Client, <<"SearchUsersByImage">>, Input, Options).

%% @doc Starts asynchronous recognition of celebrities in a stored video.
%%
%% Amazon Rekognition Video can detect celebrities in a video must be stored
%% in an Amazon S3 bucket. Use `Video' to specify the bucket name
%% and the filename of the video.
%% `StartCelebrityRecognition'
%% returns a job identifier (`JobId') which you use to get the results of
%% the analysis.
%% When celebrity recognition analysis is finished, Amazon Rekognition Video
%% publishes a completion status
%% to the Amazon Simple Notification Service topic that you specify in
%% `NotificationChannel'.
%% To get the results of the celebrity recognition analysis, first check that
%% the status value published to the Amazon SNS
%% topic is `SUCCEEDED'. If so, call `GetCelebrityRecognition' and
%% pass the job identifier
%% (`JobId') from the initial call to `StartCelebrityRecognition'.
%%
%% For more information, see Recognizing celebrities in the Amazon
%% Rekognition Developer Guide.
-spec start_celebrity_recognition(aws_client:aws_client(), start_celebrity_recognition_request()) ->
    {ok, start_celebrity_recognition_response(), tuple()} |
    {error, any()} |
    {error, start_celebrity_recognition_errors(), tuple()}.
start_celebrity_recognition(Client, Input)
  when is_map(Client), is_map(Input) ->
    start_celebrity_recognition(Client, Input, []).

-spec start_celebrity_recognition(aws_client:aws_client(), start_celebrity_recognition_request(), proplists:proplist()) ->
    {ok, start_celebrity_recognition_response(), tuple()} |
    {error, any()} |
    {error, start_celebrity_recognition_errors(), tuple()}.
start_celebrity_recognition(Client, Input, Options)
  when is_map(Client), is_map(Input), is_list(Options) ->
    request(Client, <<"StartCelebrityRecognition">>, Input, Options).

%% @doc Starts asynchronous detection of inappropriate, unwanted, or
%% offensive content in a stored video.
%%
%% For a list of moderation labels in Amazon Rekognition, see
%% Using the image and video moderation APIs:
%% https://docs.aws.amazon.com/rekognition/latest/dg/moderation.html#moderation-api.
%%
%% Amazon Rekognition Video can moderate content in a video stored in an
%% Amazon S3 bucket. Use `Video' to specify the bucket name
%% and the filename of the video. `StartContentModeration'
%% returns a job identifier (`JobId') which you use to get the results of
%% the analysis.
%% When content analysis is finished, Amazon Rekognition Video publishes a
%% completion status
%% to the Amazon Simple Notification Service topic that you specify in
%% `NotificationChannel'.
%%
%% To get the results of the content analysis, first check that the status
%% value published to the Amazon SNS
%% topic is `SUCCEEDED'. If so, call `GetContentModeration' and pass
%% the job identifier
%% (`JobId') from the initial call to `StartContentModeration'.
%%
%% For more information, see Moderating content in the Amazon Rekognition
%% Developer Guide.
-spec start_content_moderation(aws_client:aws_client(), start_content_moderation_request()) ->
    {ok, start_content_moderation_response(), tuple()} |
    {error, any()} |
    {error, start_content_moderation_errors(), tuple()}.
start_content_moderation(Client, Input)
  when is_map(Client), is_map(Input) ->
    start_content_moderation(Client, Input, []).

-spec start_content_moderation(aws_client:aws_client(), start_content_moderation_request(), proplists:proplist()) ->
    {ok, start_content_moderation_response(), tuple()} |
    {error, any()} |
    {error, start_content_moderation_errors(), tuple()}.
start_content_moderation(Client, Input, Options)
  when is_map(Client), is_map(Input), is_list(Options) ->
    request(Client, <<"StartContentModeration">>, Input, Options).

%% @doc Starts asynchronous detection of faces in a stored video.
%%
%% Amazon Rekognition Video can detect faces in a video stored in an Amazon
%% S3 bucket.
%% Use `Video' to specify the bucket name and the filename of the video.
%% `StartFaceDetection' returns a job identifier (`JobId') that you
%% use to get the results of the operation.
%% When face detection is finished, Amazon Rekognition Video publishes a
%% completion status
%% to the Amazon Simple Notification Service topic that you specify in
%% `NotificationChannel'.
%% To get the results of the face detection operation, first check that the
%% status value published to the Amazon SNS
%% topic is `SUCCEEDED'. If so, call `GetFaceDetection' and pass the
%% job identifier
%% (`JobId') from the initial call to `StartFaceDetection'.
%%
%% For more information, see Detecting faces in a stored video in the
%% Amazon Rekognition Developer Guide.
-spec start_face_detection(aws_client:aws_client(), start_face_detection_request()) ->
    {ok, start_face_detection_response(), tuple()} |
    {error, any()} |
    {error, start_face_detection_errors(), tuple()}.
start_face_detection(Client, Input)
  when is_map(Client), is_map(Input) ->
    start_face_detection(Client, Input, []).

-spec start_face_detection(aws_client:aws_client(), start_face_detection_request(), proplists:proplist()) ->
    {ok, start_face_detection_response(), tuple()} |
    {error, any()} |
    {error, start_face_detection_errors(), tuple()}.
start_face_detection(Client, Input, Options)
  when is_map(Client), is_map(Input), is_list(Options) ->
    request(Client, <<"StartFaceDetection">>, Input, Options).

%% @doc Starts the asynchronous search for faces in a collection that match
%% the faces of persons detected in a stored video.
%%
%% The video must be stored in an Amazon S3 bucket. Use `Video' to
%% specify the bucket name
%% and the filename of the video. `StartFaceSearch'
%% returns a job identifier (`JobId') which you use to get the search
%% results once the search has completed.
%% When searching is finished, Amazon Rekognition Video publishes a
%% completion status
%% to the Amazon Simple Notification Service topic that you specify in
%% `NotificationChannel'.
%% To get the search results, first check that the status value published to
%% the Amazon SNS
%% topic is `SUCCEEDED'. If so, call `GetFaceSearch' and pass the job
%% identifier
%% (`JobId') from the initial call to `StartFaceSearch'. For more
%% information, see
%% Searching stored videos for faces:
%% https://docs.aws.amazon.com/rekognition/latest/dg/procedure-person-search-videos.html.
-spec start_face_search(aws_client:aws_client(), start_face_search_request()) ->
    {ok, start_face_search_response(), tuple()} |
    {error, any()} |
    {error, start_face_search_errors(), tuple()}.
start_face_search(Client, Input)
  when is_map(Client), is_map(Input) ->
    start_face_search(Client, Input, []).

-spec start_face_search(aws_client:aws_client(), start_face_search_request(), proplists:proplist()) ->
    {ok, start_face_search_response(), tuple()} |
    {error, any()} |
    {error, start_face_search_errors(), tuple()}.
start_face_search(Client, Input, Options)
  when is_map(Client), is_map(Input), is_list(Options) ->
    request(Client, <<"StartFaceSearch">>, Input, Options).

%% @doc Starts asynchronous detection of labels in a stored video.
%%
%% Amazon Rekognition Video can detect labels in a video. Labels are
%% instances of real-world entities.
%% This includes objects like flower, tree, and table; events like
%% wedding, graduation, and birthday party; concepts like landscape, evening,
%% and nature; and activities
%% like a person getting out of a car or a person skiing.
%%
%% The video must be stored in an Amazon S3 bucket. Use `Video' to
%% specify the bucket name
%% and the filename of the video.
%% `StartLabelDetection' returns a job identifier (`JobId') which you
%% use to get the
%% results of the operation. When label detection is finished, Amazon
%% Rekognition Video publishes a completion status
%% to the Amazon Simple Notification Service topic that you specify in
%% `NotificationChannel'.
%%
%% To get the results of the label detection operation, first check that the
%% status value published to the Amazon SNS
%% topic is `SUCCEEDED'. If so, call `GetLabelDetection' and pass the
%% job identifier
%% (`JobId') from the initial call to `StartLabelDetection'.
%%
%% Optional Parameters
%%
%% `StartLabelDetection' has the `GENERAL_LABELS' Feature applied by
%% default. This feature allows you to provide filtering criteria to the
%% `Settings'
%% parameter. You can filter with sets of individual labels or with label
%% categories. You can
%% specify inclusive filters, exclusive filters, or a combination of
%% inclusive and exclusive
%% filters. For more information on filtering, see Detecting labels in a
%% video:
%% https://docs.aws.amazon.com/rekognition/latest/dg/labels-detecting-labels-video.html.
%%
%% You can specify `MinConfidence' to control the confidence threshold
%% for the
%% labels returned. The default is 50.
-spec start_label_detection(aws_client:aws_client(), start_label_detection_request()) ->
    {ok, start_label_detection_response(), tuple()} |
    {error, any()} |
    {error, start_label_detection_errors(), tuple()}.
start_label_detection(Client, Input)
  when is_map(Client), is_map(Input) ->
    start_label_detection(Client, Input, []).

-spec start_label_detection(aws_client:aws_client(), start_label_detection_request(), proplists:proplist()) ->
    {ok, start_label_detection_response(), tuple()} |
    {error, any()} |
    {error, start_label_detection_errors(), tuple()}.
start_label_detection(Client, Input, Options)
  when is_map(Client), is_map(Input), is_list(Options) ->
    request(Client, <<"StartLabelDetection">>, Input, Options).

%% @doc Initiates a new media analysis job.
%%
%% Accepts a manifest file in an Amazon S3 bucket. The
%% output is a manifest file and a summary of the manifest stored in the
%% Amazon S3 bucket.
-spec start_media_analysis_job(aws_client:aws_client(), start_media_analysis_job_request()) ->
    {ok, start_media_analysis_job_response(), tuple()} |
    {error, any()} |
    {error, start_media_analysis_job_errors(), tuple()}.
start_media_analysis_job(Client, Input)
  when is_map(Client), is_map(Input) ->
    start_media_analysis_job(Client, Input, []).

-spec start_media_analysis_job(aws_client:aws_client(), start_media_analysis_job_request(), proplists:proplist()) ->
    {ok, start_media_analysis_job_response(), tuple()} |
    {error, any()} |
    {error, start_media_analysis_job_errors(), tuple()}.
start_media_analysis_job(Client, Input, Options)
  when is_map(Client), is_map(Input), is_list(Options) ->
    request(Client, <<"StartMediaAnalysisJob">>, Input, Options).

%% @doc
%%
%% End of support notice: On October 31, 2025, AWS will discontinue
%% support for Amazon Rekognition People Pathing.
%%
%% After October 31, 2025, you will no
%% longer be able to use the Rekognition People Pathing capability. For more
%% information,
%% visit this blog post:
%% https://aws.amazon.com/blogs/machine-learning/transitioning-from-amazon-rekognition-people-pathing-exploring-other-alternatives/.
%%
%% Starts the asynchronous tracking of a person's path in a stored video.
%%
%% Amazon Rekognition Video can track the path of people in a video stored in
%% an Amazon S3 bucket. Use `Video' to specify the bucket name
%% and the filename of the video. `StartPersonTracking'
%% returns a job identifier (`JobId') which you use to get the results of
%% the operation.
%% When label detection is finished, Amazon Rekognition publishes a
%% completion status
%% to the Amazon Simple Notification Service topic that you specify in
%% `NotificationChannel'.
%%
%% To get the results of the person detection operation, first check that the
%% status value published to the Amazon SNS
%% topic is `SUCCEEDED'. If so, call `GetPersonTracking' and pass the
%% job identifier
%% (`JobId') from the initial call to `StartPersonTracking'.
-spec start_person_tracking(aws_client:aws_client(), start_person_tracking_request()) ->
    {ok, start_person_tracking_response(), tuple()} |
    {error, any()} |
    {error, start_person_tracking_errors(), tuple()}.
start_person_tracking(Client, Input)
  when is_map(Client), is_map(Input) ->
    start_person_tracking(Client, Input, []).

-spec start_person_tracking(aws_client:aws_client(), start_person_tracking_request(), proplists:proplist()) ->
    {ok, start_person_tracking_response(), tuple()} |
    {error, any()} |
    {error, start_person_tracking_errors(), tuple()}.
start_person_tracking(Client, Input, Options)
  when is_map(Client), is_map(Input), is_list(Options) ->
    request(Client, <<"StartPersonTracking">>, Input, Options).

%% @doc
%% This operation applies only to Amazon Rekognition Custom Labels.
%%
%% Starts the running of the version of a model. Starting a model takes a
%% while to
%% complete. To check the current state of the model, use
%% `DescribeProjectVersions'.
%%
%% Once the model is running, you can detect custom labels in new images by
%% calling
%% `DetectCustomLabels'.
%%
%% You are charged for the amount of time that the model is running. To stop
%% a running
%% model, call `StopProjectVersion'.
%%
%% This operation requires permissions to perform the
%% `rekognition:StartProjectVersion' action.
-spec start_project_version(aws_client:aws_client(), start_project_version_request()) ->
    {ok, start_project_version_response(), tuple()} |
    {error, any()} |
    {error, start_project_version_errors(), tuple()}.
start_project_version(Client, Input)
  when is_map(Client), is_map(Input) ->
    start_project_version(Client, Input, []).

-spec start_project_version(aws_client:aws_client(), start_project_version_request(), proplists:proplist()) ->
    {ok, start_project_version_response(), tuple()} |
    {error, any()} |
    {error, start_project_version_errors(), tuple()}.
start_project_version(Client, Input, Options)
  when is_map(Client), is_map(Input), is_list(Options) ->
    request(Client, <<"StartProjectVersion">>, Input, Options).

%% @doc Starts asynchronous detection of segment detection in a stored video.
%%
%% Amazon Rekognition Video can detect segments in a video stored in an
%% Amazon S3 bucket. Use `Video' to specify the bucket name and
%% the filename of the video. `StartSegmentDetection' returns a job
%% identifier (`JobId') which you use to get
%% the results of the operation. When segment detection is finished, Amazon
%% Rekognition Video publishes a completion status to the Amazon Simple
%% Notification Service topic
%% that you specify in `NotificationChannel'.
%%
%% You can use the `Filters' (`StartSegmentDetectionFilters')
%% input parameter to specify the minimum detection confidence returned in
%% the response.
%% Within `Filters', use `ShotFilter'
%% (`StartShotDetectionFilter')
%% to filter detected shots. Use `TechnicalCueFilter'
%% (`StartTechnicalCueDetectionFilter')
%% to filter technical cues.
%%
%% To get the results of the segment detection operation, first check that
%% the status value published to the Amazon SNS
%% topic is `SUCCEEDED'. if so, call `GetSegmentDetection' and pass
%% the job identifier (`JobId')
%% from the initial call to `StartSegmentDetection'.
%%
%% For more information, see Detecting video segments in stored video in the
%% Amazon Rekognition Developer Guide.
-spec start_segment_detection(aws_client:aws_client(), start_segment_detection_request()) ->
    {ok, start_segment_detection_response(), tuple()} |
    {error, any()} |
    {error, start_segment_detection_errors(), tuple()}.
start_segment_detection(Client, Input)
  when is_map(Client), is_map(Input) ->
    start_segment_detection(Client, Input, []).

-spec start_segment_detection(aws_client:aws_client(), start_segment_detection_request(), proplists:proplist()) ->
    {ok, start_segment_detection_response(), tuple()} |
    {error, any()} |
    {error, start_segment_detection_errors(), tuple()}.
start_segment_detection(Client, Input, Options)
  when is_map(Client), is_map(Input), is_list(Options) ->
    request(Client, <<"StartSegmentDetection">>, Input, Options).

%% @doc Starts processing a stream processor.
%%
%% You create a stream processor by calling `CreateStreamProcessor'.
%% To tell `StartStreamProcessor' which stream processor to start, use
%% the value of the `Name' field specified in the call to
%% `CreateStreamProcessor'.
%%
%% If you are using a label detection stream processor to detect labels, you
%% need to provide a `Start selector' and a `Stop selector' to
%% determine the length of the stream processing time.
-spec start_stream_processor(aws_client:aws_client(), start_stream_processor_request()) ->
    {ok, start_stream_processor_response(), tuple()} |
    {error, any()} |
    {error, start_stream_processor_errors(), tuple()}.
start_stream_processor(Client, Input)
  when is_map(Client), is_map(Input) ->
    start_stream_processor(Client, Input, []).

-spec start_stream_processor(aws_client:aws_client(), start_stream_processor_request(), proplists:proplist()) ->
    {ok, start_stream_processor_response(), tuple()} |
    {error, any()} |
    {error, start_stream_processor_errors(), tuple()}.
start_stream_processor(Client, Input, Options)
  when is_map(Client), is_map(Input), is_list(Options) ->
    request(Client, <<"StartStreamProcessor">>, Input, Options).

%% @doc Starts asynchronous detection of text in a stored video.
%%
%% Amazon Rekognition Video can detect text in a video stored in an Amazon S3
%% bucket. Use `Video' to specify the bucket name and
%% the filename of the video. `StartTextDetection' returns a job
%% identifier (`JobId') which you use to get
%% the results of the operation. When text detection is finished, Amazon
%% Rekognition Video publishes a completion status to the Amazon Simple
%% Notification Service topic
%% that you specify in `NotificationChannel'.
%%
%% To get the results of the text detection operation, first check that the
%% status value published to the Amazon SNS
%% topic is `SUCCEEDED'. if so, call `GetTextDetection' and pass the
%% job identifier (`JobId')
%% from the initial call to `StartTextDetection'.
-spec start_text_detection(aws_client:aws_client(), start_text_detection_request()) ->
    {ok, start_text_detection_response(), tuple()} |
    {error, any()} |
    {error, start_text_detection_errors(), tuple()}.
start_text_detection(Client, Input)
  when is_map(Client), is_map(Input) ->
    start_text_detection(Client, Input, []).

-spec start_text_detection(aws_client:aws_client(), start_text_detection_request(), proplists:proplist()) ->
    {ok, start_text_detection_response(), tuple()} |
    {error, any()} |
    {error, start_text_detection_errors(), tuple()}.
start_text_detection(Client, Input, Options)
  when is_map(Client), is_map(Input), is_list(Options) ->
    request(Client, <<"StartTextDetection">>, Input, Options).

%% @doc
%% This operation applies only to Amazon Rekognition Custom Labels.
%%
%% Stops a running model. The operation might take a while to complete. To
%% check the
%% current status, call `DescribeProjectVersions'. Only applies to Custom
%% Labels projects.
%%
%% This operation requires permissions to perform the
%% `rekognition:StopProjectVersion' action.
-spec stop_project_version(aws_client:aws_client(), stop_project_version_request()) ->
    {ok, stop_project_version_response(), tuple()} |
    {error, any()} |
    {error, stop_project_version_errors(), tuple()}.
stop_project_version(Client, Input)
  when is_map(Client), is_map(Input) ->
    stop_project_version(Client, Input, []).

-spec stop_project_version(aws_client:aws_client(), stop_project_version_request(), proplists:proplist()) ->
    {ok, stop_project_version_response(), tuple()} |
    {error, any()} |
    {error, stop_project_version_errors(), tuple()}.
stop_project_version(Client, Input, Options)
  when is_map(Client), is_map(Input), is_list(Options) ->
    request(Client, <<"StopProjectVersion">>, Input, Options).

%% @doc Stops a running stream processor that was created by
%% `CreateStreamProcessor'.
-spec stop_stream_processor(aws_client:aws_client(), stop_stream_processor_request()) ->
    {ok, stop_stream_processor_response(), tuple()} |
    {error, any()} |
    {error, stop_stream_processor_errors(), tuple()}.
stop_stream_processor(Client, Input)
  when is_map(Client), is_map(Input) ->
    stop_stream_processor(Client, Input, []).

-spec stop_stream_processor(aws_client:aws_client(), stop_stream_processor_request(), proplists:proplist()) ->
    {ok, stop_stream_processor_response(), tuple()} |
    {error, any()} |
    {error, stop_stream_processor_errors(), tuple()}.
stop_stream_processor(Client, Input, Options)
  when is_map(Client), is_map(Input), is_list(Options) ->
    request(Client, <<"StopStreamProcessor">>, Input, Options).

%% @doc Adds one or more key-value tags to an Amazon Rekognition collection,
%% stream processor, or Custom
%% Labels model.
%%
%% For more information, see Tagging AWS
%% Resources: https://docs.aws.amazon.com/general/latest/gr/aws_tagging.html.
%%
%% This operation requires permissions to perform the
%% `rekognition:TagResource'
%% action.
-spec tag_resource(aws_client:aws_client(), tag_resource_request()) ->
    {ok, tag_resource_response(), tuple()} |
    {error, any()} |
    {error, tag_resource_errors(), tuple()}.
tag_resource(Client, Input)
  when is_map(Client), is_map(Input) ->
    tag_resource(Client, Input, []).

-spec tag_resource(aws_client:aws_client(), tag_resource_request(), proplists:proplist()) ->
    {ok, tag_resource_response(), tuple()} |
    {error, any()} |
    {error, tag_resource_errors(), tuple()}.
tag_resource(Client, Input, Options)
  when is_map(Client), is_map(Input), is_list(Options) ->
    request(Client, <<"TagResource">>, Input, Options).

%% @doc Removes one or more tags from an Amazon Rekognition collection,
%% stream processor, or Custom Labels
%% model.
%%
%% This operation requires permissions to perform the
%% `rekognition:UntagResource' action.
-spec untag_resource(aws_client:aws_client(), untag_resource_request()) ->
    {ok, untag_resource_response(), tuple()} |
    {error, any()} |
    {error, untag_resource_errors(), tuple()}.
untag_resource(Client, Input)
  when is_map(Client), is_map(Input) ->
    untag_resource(Client, Input, []).

-spec untag_resource(aws_client:aws_client(), untag_resource_request(), proplists:proplist()) ->
    {ok, untag_resource_response(), tuple()} |
    {error, any()} |
    {error, untag_resource_errors(), tuple()}.
untag_resource(Client, Input, Options)
  when is_map(Client), is_map(Input), is_list(Options) ->
    request(Client, <<"UntagResource">>, Input, Options).

%% @doc
%% This operation applies only to Amazon Rekognition Custom Labels.
%%
%% Adds or updates one or more entries (images) in a dataset. An entry is a
%% JSON Line which contains the
%% information for a single image, including
%% the image location, assigned labels, and object location bounding boxes.
%% For more information,
%% see Image-Level labels in manifest files and Object localization in
%% manifest files in the Amazon Rekognition Custom Labels Developer Guide.
%%
%% If the `source-ref' field in the JSON line references an existing
%% image, the existing image in the dataset
%% is updated.
%% If `source-ref' field doesn't reference an existing image, the
%% image is added as a new image to the dataset.
%%
%% You specify the changes that you want to make in the `Changes' input
%% parameter.
%% There isn't a limit to the number JSON Lines that you can change, but
%% the size of `Changes' must be less
%% than 5MB.
%%
%% `UpdateDatasetEntries' returns immediatly, but the dataset update
%% might take a while to complete.
%% Use `DescribeDataset' to check the
%% current status. The dataset updated successfully if the value of
%% `Status' is
%% `UPDATE_COMPLETE'.
%%
%% To check if any non-terminal errors occured, call `ListDatasetEntries'
%% and check for the presence of `errors' lists in the JSON Lines.
%%
%% Dataset update fails if a terminal error occurs (`Status' =
%% `UPDATE_FAILED').
%% Currently, you can't access the terminal error information from the
%% Amazon Rekognition Custom Labels SDK.
%%
%% This operation requires permissions to perform the
%% `rekognition:UpdateDatasetEntries' action.
-spec update_dataset_entries(aws_client:aws_client(), update_dataset_entries_request()) ->
    {ok, update_dataset_entries_response(), tuple()} |
    {error, any()} |
    {error, update_dataset_entries_errors(), tuple()}.
update_dataset_entries(Client, Input)
  when is_map(Client), is_map(Input) ->
    update_dataset_entries(Client, Input, []).

-spec update_dataset_entries(aws_client:aws_client(), update_dataset_entries_request(), proplists:proplist()) ->
    {ok, update_dataset_entries_response(), tuple()} |
    {error, any()} |
    {error, update_dataset_entries_errors(), tuple()}.
update_dataset_entries(Client, Input, Options)
  when is_map(Client), is_map(Input), is_list(Options) ->
    request(Client, <<"UpdateDatasetEntries">>, Input, Options).

%% @doc
%% Allows you to update a stream processor.
%%
%% You can change some settings and regions of interest and delete certain
%% parameters.
-spec update_stream_processor(aws_client:aws_client(), update_stream_processor_request()) ->
    {ok, update_stream_processor_response(), tuple()} |
    {error, any()} |
    {error, update_stream_processor_errors(), tuple()}.
update_stream_processor(Client, Input)
  when is_map(Client), is_map(Input) ->
    update_stream_processor(Client, Input, []).

-spec update_stream_processor(aws_client:aws_client(), update_stream_processor_request(), proplists:proplist()) ->
    {ok, update_stream_processor_response(), tuple()} |
    {error, any()} |
    {error, update_stream_processor_errors(), tuple()}.
update_stream_processor(Client, Input, Options)
  when is_map(Client), is_map(Input), is_list(Options) ->
    request(Client, <<"UpdateStreamProcessor">>, Input, Options).

%%====================================================================
%% Internal functions
%%====================================================================

-spec request(aws_client:aws_client(), binary(), map(), list()) ->
    {ok, Result, {integer(), list(), hackney:client()}} |
    {error, Error, {integer(), list(), hackney:client()}} |
    {error, term()} when
    Result :: map() | undefined,
    Error :: map().
request(Client, Action, Input, Options) ->
    RequestFun = fun() -> do_request(Client, Action, Input, Options) end,
    aws_request:request(RequestFun, Options).

do_request(Client, Action, Input0, Options) ->
    Client1 = Client#{service => <<"rekognition">>},
    Host = build_host(<<"rekognition">>, Client1),
    URL = build_url(Host, Client1),
    Headers = [
        {<<"Host">>, Host},
        {<<"Content-Type">>, <<"application/x-amz-json-1.1">>},
        {<<"X-Amz-Target">>, <<"RekognitionService.", Action/binary>>}
    ],

    Input = Input0,

    Payload = jsx:encode(Input),
    SignedHeaders = aws_request:sign_request(Client1, <<"POST">>, URL, Headers, Payload),
    Response = hackney:request(post, URL, SignedHeaders, Payload, Options),
    handle_response(Response).

handle_response({ok, 200, ResponseHeaders, Client}) ->
    case hackney:body(Client) of
        {ok, <<>>} ->
            {ok, undefined, {200, ResponseHeaders, Client}};
        {ok, Body} ->
            Result = jsx:decode(Body),
            {ok, Result, {200, ResponseHeaders, Client}}
    end;
handle_response({ok, StatusCode, ResponseHeaders, Client}) ->
    {ok, Body} = hackney:body(Client),
    Error = jsx:decode(Body),
    {error, Error, {StatusCode, ResponseHeaders, Client}};
handle_response({error, Reason}) ->
    {error, Reason}.

build_host(_EndpointPrefix, #{region := <<"local">>, endpoint := Endpoint}) ->
    Endpoint;
build_host(_EndpointPrefix, #{region := <<"local">>}) ->
    <<"localhost">>;
build_host(EndpointPrefix, #{region := Region, endpoint := Endpoint}) ->
    aws_util:binary_join([EndpointPrefix, Region, Endpoint], <<".">>).

build_url(Host, Client) ->
    Proto = aws_client:proto(Client),
    Port = aws_client:port(Client),
    aws_util:binary_join([Proto, <<"://">>, Host, <<":">>, Port, <<"/">>], <<"">>).
